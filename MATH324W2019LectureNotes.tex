\documentclass[14pt,twoside,a4paper,fleqn]{article}
\usepackage{amsmath,mathtools,latexsym,semantic,amsthm,cancel,stackengine,dcolumn,pxfonts,graphicx,mathrsfs}
\theoremstyle{plain}
%theorems and lemmas with numbering
\newtheorem{theorem}{Theorem}[section]
\newtheorem{example}{Example}[section]
\newtheorem{lemma}{Lemma}[theorem]
\newtheorem{remark}{Remark}[section]
\newtheorem{definition}{Definition}[section]


%theorem and lemmas without numbering
\newtheorem*{theorem*}{Theorem}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{remark*}{Remark}
\newtheorem*{definition*}{Definition}
\newtheorem*{example*}{Example}

%commands:
\newcommand\myeq{\mathrel{\stackrel{\makebox[0pt]{\mbox{\normalfont\tiny def}}}{=}}}

\newcolumntype{d}[1]{D{.}{\cdot}{#1} }

%for distribution
\makeatletter
\newcommand{\distas}[1]{\mathbin{\overset{#1}{\kern\z@\sim}}}%

\usepackage{parskip}
\setlength{\parindent}{14pt}

\usepackage{setspace}
\doublespacing
% \onehalfspacing





\title{MATH324 (Statistics) -- Lecture Notes\\McGill University}
\date{Winter 2019}
\author{Masoud Asgharian}


\begin{document}

\maketitle

%newcommands:
%       newcommand{\COMMAND}{REPLACEDCOMMAND}
%\newcommand{\nexists}{\not\exists}
%	tab command
\newcommand\tab[1][1cm]{\hspace*{#1}}

\tableofcontents
\newpage
\section{Lecture 0}
\section{Lecture 1}

\newpage
%Lecture 3
\section{Lecture 2}
%Markov's Inequality
\subsection{Markov's Inequality}
Let $X$ be a random variable and $h$ be a \textbf{non-negative} function; ie:
	\begin{equation*}
	h:R->R^{+}\ \cup\ \{0\}\tab[0.5cm]=\tab[0.5cm] [0,\infty)
	\end{equation*}
Suppose $E(h(X))\ < \infty$ ,then for some $\lambda > 0$ , we have:
	\begin{equation}\label{markov}
	P(h(X) \geq \lambda)\ \leq\ \frac{E[h(X)]}{\lambda}
	\end{equation}
\begin{proof}
Suppose $X$ is a continuous random variable:
	\begin{equation*}
	\begin{split}
	E[h(x)] &= \int_{x} h(x)f_{_X}(x)dx \\
		&=\Big(\int_{x:h(x)\geq\lambda}h(x)f_{_X}(x)dx + \int_{x:h(x) < \lambda}h(x)f_{_X}(x)dx\Big) \\
		&\geq \int_{x:h(x)\geq\lambda}h(x)f_{_X}(x)dx\tab[4cm] \underline{since}\ h\geq 0\\
		&\geq\lambda \int_{x:h(x\geq\lambda}f_{_X}(x)dx = \lambda\ P(h(X)\geq \lambda)\\
		\implies P(h(X)\geq \lambda) \leq \frac{E(h(X))}{\lambda}
	\end{split}
	\end{equation*}
\emph{The proof for the discrete case is similar.}
\end{proof} 

%Tchebushev's Inequality
\subsection{Tchebyshev's Inequality}
\emph{Tchebyshev's Inequality} is a special case of Markov's Inequality. \mbox{Consider $h(x) = (x-\mu)^2$}, then:
	\begin{equation*}
	\begin{split}
	P(\vert X-\mu \vert \geq \lambda) &= P((X-\mu)^2 \geq \lambda^2)\\
		&\leq \frac{E[(X-\mu)^2]}{\lambda^2}\tab[2cm]if\ E[(X-\mu)^2]<\infty
	\end{split}
	\end{equation*}
Let $\mu = E(X)$, then $E[(X-\mu)^2] = Var(X)$ denoted by $\sigma_{_X}^2$. We therefore have:
	\begin{equation}\label{tchev_sigma}
		P(\vert X-\mu_{_X} \vert \geq \lambda) \leq \frac{\sigma_{_X}^2}{\lambda^2}\tab[2cm] where\ \mu_{_X} = E(X)
	\end{equation}
Now consider $\lambda= K\sigma_x$ where $K$ is a known number. Then:
	\begin{equation}\label{tchev}
		P(\vert X-\mu_{_X}\vert \geq K\sigma_{_X}) \geq \frac{\sigma_{_X}^2}{K^2\sigma_{_X}^2}=\frac{1}{K^2}
	\end{equation}

This is called \textbf{Tchbyshev's Inequality}. 
\begin{example}
Suppose $K=3$.\hfill\newline
$$P(\vert X-\mu_x\vert \geq 3\ \sigma_{_X}) \leq \frac{1}{9}$$
In other words, at least $88\%$ of the observations are within $3$ standard deviation from the population mean.
\end{example}

Going back to the our example:
$$X_i \sim (\mu, 1)\tab[0.5cm],\tab[0.5cm] \bar X_n = \frac{1}{n}\sum_{i=1}^{n} X_i$$
We want to study $P(\epsilon \geq \delta) = P(\vert \bar{X}_n - \mu\vert \geq \delta)$, first we note that: $$E(X_i)=\mu\ \ \ ,\ i=1,2,\ldots,n$$
Then:
	\begin{equation*}
	\begin{split}
		E(\bar{X}_n) &= E\big(\frac{1}{n}\sum_{i=1}^{n}X_i \big)=\frac{1}{n}\sum_{i=1}^{n}E(X_i)\\
		&= \frac{1}{n}\sum_{i=1}^{n} \mu =\frac{1}{n}(n\mu)= \frac{1}{\cancel{n}}.(\cancel{n}\mu)\\
		&= \mu\tab[13cm] (*)
	\end{split}
	\end{equation*}
Thus, using (\ref{tchev_sigma}) we have:
	$$P(\vert\bar{X}_n - \mu\vert \geq \delta ) \leq \frac{Var(\bar{X}_n)}{\delta^2}$$
Now:
	\begin{equation*}
	\begin{split}
	Var(\bar{X}_n) &= Var\big(\frac{1}{n} \sum_{i=1}^{n} X_i \big) = \frac{1}{n^2}Var\big(\sum_{i=1}^{n}X_i \big)\\
	&=\frac{1}{n^2}\Big[\sum_{i=1}^{n} Var(X_i) +\sum_{1\leq i<j\leq n}\ \sum_{1\leq i<j\leq n} Cov(X_i, X_j) \Big]\ \ \ using\ Thm\ 5.12(b)-page\ 271\\
	&=\frac{1}{n^2}\sum_{i=1}^{n}Var(X_i)\tab[2cm]since\ \coprod_{1}^{n}X_i\\
	&=\frac{1}{n^{\cancel{2}}} \cancel{n} Var(X) = \frac{Var(X)}{n} \tab since\ x_{i}s\ are\ identically\ distributed\\
	&=\frac{\delta_X^2}{n}\tab[12cm](**)
	\end{split}
	\end{equation*}
	In our case $X\sim N(\mu, 1)$ so $Var(X) = \delta_X^2 = 1$. Thus $Var(\bar{X}_n) = \frac{1}{n}$
	
\begin{remark*}
$X\coprod Y \implies Cov(X,Y)=0$. Note that:
$$X \coprod Y \implies E\big[g_1(X)g_2(Y)\big] = E[g_1(X)].E[g_2(Y)]$$
in particular:
$$X \coprod Y \implies E\big[XY\big] = E[X].E[Y]$$
on the other hand:
$$Cov(X,Y) = E[XY] - E(X)E(Y)$$
thus: 
$$X\coprod Y\implies Cov(X,Y)=0.$$
\end{remark*}
recall that $X\coprod Y$ means X and Y are independent, i.e. \mbox{$f_{_{X,Y}}(x,y)=f_{_X}(x)f_{_Y}(y)$} where $f_{_{X,Y}},f_{_X} and f_{_Y}$ represent respectively the %%missed words%%%/////TODO\\




We therefore have:
\begin{equation}\
P(\vert\bar{X}_n - \mu\vert \geq \delta) \leq \frac{1}{n\delta^2}
\end{equation}
Using $(4)$ and  the sample size, $n$, we can find an upper bound for the proportion of deviations which are greater than a given threshold $\delta$.\hfill\newline
We can also use $(4)$ for \underline{Sample Size Deterministic}:\\
Suppose $\delta$ is given and we want $P(\vert\bar{X}_n - \mu\vert \geq \delta) \leq \beta$ where $\beta$ is also given. Then setting $\frac{1}{n\delta^2} = \beta$, we can estimate $n\approx\frac{1}{\beta\delta^2}$.\hfill\newline
%Application to voting
\subsection{Application to Voting}
Define $X_i = \begin{cases}
		1 & \text{NDP}\\
		0 & \text{otherwise}
		\end{cases}$ . Associated to each eligible voter in Canada we have a binary variable X. Let $p=P(X=1)$. So $p$ represents the proportion of eligible voters who favor $NDP$. Of interest is often estimation of $p$. Suppose we have a sample of size $n$, $X_1,X_2,\ldots,X_n$.\\
		$\bar{X}_n = \frac{1}{n}\sum_{i=1}^{n}X_i$ is the sample proportion; The counterpart of $p$ which nat be denoted by $\hat{p}$. Note that:
		$$\mu_{_X} = E(X) = 1 \times P(X=1) + 0\times P(X=0) = 1-p + 0\times (1-p)=p$$
		and:
		$$E(X^2) = 1^2 \times P(X=1) + 0^2 \times P(X=0) = 1-p + 0\times (1-p) = p$$
From (*) and (**) we find that :
	$$E(\hat{p}_n) = E(\bar{X}_n) \mu_{_X} = p$$
and:
	$$Var(\hat{p}_n) = E(\bar{X}_n) = \frac{Var(X)}{n} = \frac{\sigma_X^2}{n} = \frac{p(1-p)}{n}$$
Thus using (\ref{tchev_sigma}), we have:
$$P(\vert\hat{p}_n - p\vert \geq \delta) \leq \frac{Var(\hat{p}_n)}{\delta^2} = \frac{p(1-p)}{n\delta^2}$$
Note that the above bound on the probability of derivation depends on $p$ which is \emph{unknown}. We however notice that $p(1-p) \leq \frac{1}{4}$ .\\
Define $\mathscr{C}(x) = x(1-x)\ for\ 0 < x < 1$. Then:
	\begin{equation*}
	\begin{split}
	&\mathscr{C}^{'}(x) = 1-2x \implies \mathscr{C}^{'}(x)= 0 \implies x = \frac{1}{2} \\
	&\mathscr{C}^{"}(\frac{1}{2}) = -2 \implies x = \frac{1}{2}\tab \text{which is a \textbf{maximizer}}\\
	&\mathscr{C}(\frac{1}{2}) = \frac{1}{2}(1-\frac{1}{2})=\frac{1}{4}
	\end{split}
	\end{equation*}
	(Note that $\mathscr{C}^{"}(x) = -2$ for all $0 < x < 1$)
	
	
	%The graph is missing
	
	
We therefore find:
	\begin{equation}\label{voter}
	P(\vert\hat{p}_n - p\vert \geq \delta) \leq \frac{1}{4n\delta^2}
	\end{equation}
Using (\ref{voter}) and a given sample size $n$ we can find an upper bound for the probability of derivation by $\delta$ and the amount for any given $\delta$.\newline
We can also use (\ref{voter}) for \underline{sample size deterministic} for a size bound $
\beta$ and derivative $\delta$ as follows:
$$\frac{1}{4n\delta^2}= \beta\tab \implies \tab n \geq \frac{1}{4\beta\delta^2}$$
This is of course conservative since $p(1-p)\leq \frac{1}{4}$.



\newpage
\section{Lecture 3}
\subsection{MSE} 
\textbf{MSE}: To study estimation error we started by studying $P(\vert \hat{\Theta}_n - \Theta\vert > \delta)$ , deviation above a given threshold $\delta$, by bounding this probability. One may take a different approach by studying average Euclidean distance, i.e. $E[\ \vert\hat{\Theta}_n - \Theta\vert ^2]$, which denoted by \textbf{MSE($\hat{\Theta}_n$)}.\newline
We note that if $\Theta = E(\hat{\Theta}_n)$, i.e. $\hat{\Theta}_n$ is an unbiased estimation of $\Theta$, then: 
$$MSE(\hat{\Theta}_n) = E[\vert\hat{\Theta}_n - \Theta\vert^2] = E[(\hat{\Theta}_n - \mu _{n_{\Theta _n}})^2] = Var(\hat{\Theta}_n)$$
Now recall that $Var(X) = 0\  \implies \ P(X = \text{constant}) = 1$ which essentially means random variable X is a constant.\\
The same comment applies to MSE($\hat{\Theta}_n$). We want to find the closest estimator $\hat{\Theta}_n$ to $\Theta$ which means that we want to minimize $E[(\hat{\Theta}_n - \Theta)^2]$ over all possible estimators, ideally at least the above comment tells us that in real applications we cannot expect to find an estimator whose MSE is equal to zero. Let's try to understand the MSE a bit more:
\begin{equation*}
\begin{split}
\text{MSE}(\hat{\Theta}_n) &= E[(\hat{\Theta}_n - \Theta)^2]\\
	&= E\Big[\Big(\big(\hat{\Theta}_n - E(\hat{\Theta}_n)\big) + \big(E(\hat{\Theta}_n) - \Theta\big) \Big)^2 \Big]\\
	&= E\big[\big(\hat{\Theta}_n - E(\hat{\Theta}_n)\big)^2\big] + \big(E(\hat{\Theta}_n) - \Theta\big)^2 + 2 \cdot E\big[\big(\hat{\Theta}_n - E(\hat{\Theta}_n)\big)\big]\cdot \big(E(\hat{\Theta}_n) - \Theta\big)\\
	&= E\big[(\hat{\Theta}_n - E(\hat{\Theta}_n))^2\big] + E\big[\overbrace{(E(\hat{\Theta}_n) - \Theta)^2}^{\text{not a r.v.}}\big]
		+2\cdot E\Big[\overbrace{\big(E(\hat{\Theta}_n) - \Theta)}^{\text{not a r.v.}}\big)\cdot \big(\hat{\Theta}_n - E(\hat{\Theta}_n)\big) \Big]\\
		&= Var(\hat{\Theta}_n) + \big[\overbrace{E(\hat{\Theta}_n) - \Theta}^{\text{Bias$(\hat{\Theta}_n)$}}\big]^2 + 2\cdot Bias(\hat{\Theta}_n)\cdot \overbrace{E[(\hat{\Theta}_n - E(\hat{\Theta}_n))]}^{E(\hat{\Theta}_n) - E(\hat{\Theta}_n) = 0}\\
		&= Var(\hat{\Theta}_n) + Bias^2(\hat{\Theta}_n)
\end{split}
\end{equation*}
Roughly speaking, \textbf{bias} measures how far off the target we hit on the average while \textbf{variance} measures how much fluctuation our estimator may show from one sample to another.

\subsection{Unbiased Estimators}
In almost all real applications, the class of possible estimators for an \textbf{ESTIMANAL} is huge and the best estimator, i.e. the one that minimizes MSE no matter what the value of the \textbf{ESTIMANAL} is, almost never exists. Thus we try to reduce the class of potential estimators by improving a plausible restriction, for example Bias$(\hat{\Theta}_n) = 0$.

\begin{definition*}An estimator  $\hat{\Theta}_n$ of an \textbf{ESTIMANAL} $\Theta$ is said to be \textbf{unbiased} if $E(\hat{\Theta}_n) = \Theta$ , for all possible values of $\Theta$.
\end{definition*}
\begin{example}
$X_i\distas{iid}  N(\mu, \sigma^2)\tab i=1,2,\ldots,n$\\
Suppose both $\mu$ and $\sigma^2$ are unknown. Consider $\bar{X}_n = \frac{1}{n}\sum_{i=1}^{n} X_i$.
$$E(\bar{X_n}) = E(\frac{1}{n}\sum_{i=1}^{n} X_i) = \frac{1}{n}\sum_{i=1}^n \overbrace{E(X_i)}^{\mu} = \frac{1}{\cancel{n}}\cdot \cancel{n}\mu = \mu$$
\end{example}
Thus $\bar{X_n}$ is an unbiased estimator of $\mu$. As for the MSE$(\bar{X_n})$, we need to find Var$(\bar{X_n})$.
\begin{equation*}
\begin{split}
Var(\bar{X_n})&= Var\big(\frac{1}{n}\sum_{i=1}^{n} X_i\big) = \frac{1}{n^2} Var(\sum_{i=1}^n X_i)\\
	&= \frac{1}{n^2}\Big[\sum_{i=1}^n Var(X_i) + 2\cdot\sum\sum_{1\leq i < j \leq n} \overbrace{Cov(X_i,X_j)}^{0} \Big]\tab[2cm] \text{Theorem 5.12(b) - page 271}\\
	&= \frac{1}{n^2}\sum_{i=1}^n Var(X_i)\tab[8cm] \coprod_{i=1}^n X_i\\
	& = \frac{1}{n^2}\sum_{i=1}^{n} \sigma^2 = \frac{1}{n^{\cancel{2}}}\cdot\cancel{n}\sigma^2 = \frac{\sigma^2}{n}\tab[5cm]\ \ \text{identically distributed}\\
&\implies MSE(\bar{X_n}) = Var(\bar{X_n}) + \overbrace{Bias^2(\bar{X_n})}^{0}=Var(\bar{X_n}) = \frac{\sigma^2}{n}
\end{split}
\end{equation*}
An inspection of the above calculation shows that for unbiased $\mu$ we only require a common mean $\mu$ while for calculating the variance we would only require a common variance $\sigma^2$ and orthogonality, i.e:
$$Cov(X_i,X_j) = 0\tab \text{where}\ i\neq j$$
Suppose $X_1,\ldots,X_n$ have the same mean value $\mu$. Then:
$$E(\bar{X_n}) = E(\frac{1}{n}\sum_{i=1}^n X_i) = \frac{1}{n}\sum_{i=1}^n E(X_i) = \frac{1}{\cancel{n}}\cancel{n}\mu=\mu$$
Suppose further that $ X_1,\ldots ,X_n$ have the same variance $\sigma^2$ and \mbox{$Cov(X_i,X_j) = 0,\ i\neq j$}. Then:
\begin{equation*}
\begin{split}
Var(\bar{X_n}) &= Var(\frac{1}{n}\sum_{i=1}^n X_i) = \frac{1}{n^2} Var(\sum_{i=1}^n X_i)\\
	&= \frac{1}{n^2}\Big[\ \sum_{i=1}^nVar(X_i) + 2\sum\sum_{1\leq i < j \leq n} Cov(X_i, X_j)\ \Big]\tab[3cm] \text{Theorem 5.12(b) - Page 271}\\
	&= \frac{1}{n^2}\sum_{i=1}^n Var(X_i)\tab[5cm] \text{Orthogonality:\ i.e. $Cov(X_i,X_j) = 0\ \ if\ i\neq j$}\\
	&=\frac{1}{n^2}\sum_{i=1}^n \sigma^2 = \frac{1}{n^{\cancel{2}}}\cancel{n}\sigma^2 = \frac{\sigma^2}{n} \tab[4cm] \text{having the same variance}\\
	&\implies MSE(\bar{X_n}) = Var(\bar{X_n}) = \frac{\sigma^2}{n}
\end{split}
\end{equation*}
If $X_1,\ldots,X_n$ have the same mean value and variance and they are orthogonal.
\subsection{Stein's Paradox}
We will learn later that if $X_i\distas{iid} N(\mu,\sigma^2)$ then \mbox{$\bar{X_n} = \frac{1}{n}\sum_{i=1}^n X_i$} has many optimal properties. A paradox due to Charles Stein, however, shows that such a nice optimal properties are not preserved in higher dimensions. In fact if:
$$X_i\distas{iid}N(\mu_{_X},1),\ \ Y_i\distas{iid}N(\mu_{_Y},1)\ \text{and  }Z_i\distas{iid}(\mu_{_Z},1)$$
then, we can find the biased estimators of 
$\begin{pmatrix} \mu_{_X}\\\mu_{_Y}\\ \mu_{_Z} \end{pmatrix}$
which are closer to 
$\begin{pmatrix} \mu_{_X}\\\mu_{_Y}\\ \mu_{_Z} \end{pmatrix}$ 
than 
$\begin{pmatrix}\bar{X_n}\\\bar{Y_n}\\\bar{Z_n} \end{pmatrix}$ 
for any 
$\begin{pmatrix} \mu_{_X}\\\mu_{_Y}\\ \mu_{_Z} \end{pmatrix}$.
We may then say that 
$\begin{pmatrix}\bar{X_n}\\\bar{Y_n}\\\bar{Z_n} \end{pmatrix}$
 is an \mbox{\textbf{\underline{inadmissible estimator}} of 
 $\begin{pmatrix} \mu_{_X}\\\mu_{_Y}\\ \mu_{_Z} \end{pmatrix}$.}
 \subsection{Admissibility}
 An estimator $\hat{\Theta}$ is called admissible if there is no estimator $\tilde{\Theta}$ such that:
 $$MSE(\tilde{\Theta}) \leq MSE(\hat{\Theta})\tab[3cm]\text{for all possible values of $\Theta$}$$
and this inequality is strict for some values of $\Theta$.\newline
What this example tells us is that by allowing a bit of bias we may be able to reduce variance considerably and hence find an estimator which is closer to the target than the most natural unbiased estimator. Note that this phenomena happens only when the dimension is at least 3.





\newpage
\section{Lecture 4}
We now want to restrict the class of estimators even further. Suppose $X_1,\ldots,X_n$ have the same mean $\mu$ and variance $\sigma^2$ and they are orthogonal; i.e. \mbox{$Cov(X_i,X_j) = 0\ ,\ i\neq j$}. Consider $\tilde{X}_{n,\underset{\sim}{C}} = \sum_{i=1}^n C_i\ X_i$ and
$$\mathscr{C} = \Big\{\tilde{X}_{n,\underset{\sim}{C}}\ \colon \underset{\sim}{C} = 
	(C_1,\ldots,C_n)\in \mathbf{R}^n ,\ \sum_{i=1}^n C_i = 1 \Big\} $$
Note that
\begin{equation*}
\begin{split}
E(\tilde{X}_{n,\underset{\sim}{C}}) &= E(\sum_{i=1}^n C_i\ X_i) = \sum_{i=1}^n C_i\ E(X_i)\\
	&= \sum_{i=1}^n C_i\ \mu = \mu\overbrace{\sum_{i=1}^n C_i}^1 = 1\cdot\mu\\
	&=\mu
\end{split}
\end{equation*}
Thus $\tilde{X}_{n,\underset{\sim}{C}}$ is an unbiased estimator of $\mu$ fir any $\underset{\sim}{C}\in\mathbf{R}^n$ as long as $\sum_{i=1}^n C_i = 1$.
Then $\mathscr{C}$ is the class of all unbiased linear estimators of $\mu$. We want to find the best estimator with $\mathscr{C}$; i.e.:
$$\underset{\underset{\sim}{C}\in\mathbf{R}^n}{\text{Min}}\ MSE(\tilde{X}_{n,\underset{\sim}{C}})\tab[0.5cm] s.t\ \sum_{i=1}^n C_i = 1\tab[2cm] (*)$$
First we note that $MSE(\tilde{X}_{n,\underset{\sim}{C}}) = Var(\tilde{X}_{n,\underset{\sim}{C}})$ since $\tilde{X}_{n,\underset{\sim}{C}}$ is an unbiased estimator of $\mu$ when $\sum_{i=1}^n C_i = 1$. On the other hand:
\begin{equation*}
\begin{split}
Var(\tilde{X}_{n,\underset{\sim}{C}}) &= Var(\sum_{i=1}^n C_i\ X_i)\\
	&= \sum_{i=1}^n {C_i}^2\ Var(X_i) + 2\sum\sum_{1\leq i<j\leq n} Cov(C_iX_i,C_j,X_j)\tab \text{Theorem 5.12 page 271}\\
	&= \sum_{i=1}^n {C_i}^2 \sigma^2 + 2 \cancel{\sum\sum_{1\leq i<j\leq n} C_iC_j\overbrace{Cov(X_i,X_j)}^0}\\
	&= \sigma^2\sum_{i=1}^n {C_i}^2 \\
\text{Thus (*) is equivalent to}:\\
	&\underset{\underset{\sim}{C}\in\mathbf{R}^n}{\text{Min}}\ \sigma^2\sum_{i=1}^n {C_i}^2\tab[2cm] (**)
\end{split}
\end{equation*}
Using the \emph{Lagrange Theorem}, (**) is equivalent to:
$$
\underset{\sim}{C} = \overset{Min}{(C_1,...,C_n)} \in\mathbb{R}^n\ \ \big\{\overbrace{\sigma^2\sum_{i=1}^n C_i + \lambda(\sum_{i=1}^n C_i -1)}^{\mathscr{C_{_\lambda} (\underset{\sim}{\text{C}})}}\big\}\ .
$$
Note that: 
$
\tab\frac{\partial\ \mathscr{C}_{\lambda}(\underset{\sim}{C})}{\partial C_i}
	= 2\ \sigma^2\ C_i + \lambda\tab[0.5cm] ,\ i=1,2,3,\ldots\\
$\\
$\frac{\partial}{\partial \lambda} \mathscr{C}_{\lambda}(\underset{\sim}{C})
	= \sum_{i=1}^n C_i - 1$\newline\\
$\begin{cases}
		\frac{\partial}{\partial C_i}\mathscr{C}_{\lambda}(\underset{\sim}{C})
			= 2\ \sigma^2\ C_i + \lambda = 0\ \ ,\ \ i=1,2,3,\ldots\\
		\frac{\partial}{\partial \lambda} \mathscr{C}_{\lambda} = 0\ \ \implies \ \ \sum_{i=1}^n C_i =1
		\end{cases}$\hfill\newline\\
Thus $C_i = -\frac{\lambda}{2\ \sigma^2}\ \ , \ \ i=1,2,3,\ldots,n\ $  and using the last equation:
$$
\sum_{i=1}^n -\frac{\lambda}{2\ \sigma^2} = 1 \implies \lambda = -\frac{2\ \sigma^2}{n}
$$
and therefore:
$$
C_i = -\frac{\lambda}{2\ \sigma^2} = -\frac{-\frac{2\ \sigma^2}{n}}{2\ \sigma^2} = \frac{1}{n}\tab ,\ \ i=1,2,3,\ldots,n
$$
We can further find:
$$\ \mathcal{H} = [\frac{\partial^2}{\partial C_i \partial C_j} \mathscr{C}_{\lambda}(\underset{\sim}{C})]\tab,\ \ i,j=1,2,\ldots,n$$
and show that:
\begin{equation*}
\begin{split}
\underset{\sim}{x}^T\ \mathcal{H}\underset{\sim}{x}\ &\geq\ 0\tab \forall \underset{\sim}{x}\in\mathbb{R}^n\\
&=0\tab \text{if and only if \ } \underset{\sim}{x} = 0
\end{split}
\end{equation*}
This then guarantees that ${\underset{\sim}{C}}^{*} = (\frac{1}{n},\frac{1}{n},\ldots,\frac{1}{n})$ is indeed a minimizer; in fact, the \mbox{\emph{unique minimizer}}. To summarize:
$$
\tilde{X}_{n,{\underset{\sim}{C}}^{*}} = \sum{i=1}^n \frac{1}{n} X_i = \frac{1}{n}\sum_{i=1}^n X_i = \bar{X_n}
$$
Thus $\bar{X_n}$ is the best unbiased linear estimator.
\subsection{Estimating Variance}
So far we confirmed ourselves to estimation of th population mean.\newline
Now suppose we are interested in estimating variance from $X_1,\ldots,X_n$ where $X_is$ have the same mean value $\mu$\ , the same variance $\sigma^2$ and they are orthogonal, i.e. \mbox{$Cov(X_i,X_j)=0\ ,\ i\neq j$}, then a \mbox{\emph{natural estimator}} of:
$$
\sigma^2 = Var(X) = \mathbb{E}[(x-\mu)^2]
$$
is \underline{its sample counterpart}, i.e.
$$
S_{n,*}^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X_n})^2
$$
Now the first question is if $S_{n,*}^2$ is an unbiased estimator of $\sigma^2$ , i.e. $\mathbb{E}(S_{n,*}^2) = \sigma^2$


\begin{equation*}
\begin{split}
(X_i - \mu)^2 &= \big[(X_i - \bar{X_n})+(\bar{X_n}-\mu)\big]^2\\
	&= (X_i - \bar{X_n})^2 + (\bar{X_n}-\mu)^2 + 2\cdot (X_i - \bar{X_n}) (\bar{X_n}-\mu)\\
\sum_{i=1}^n(X_i -\mu)^2 &= \sum_{i=1}^n (X_i - \bar{X_n})^2 + n(\bar{X_n}-\mu)^2 + 2\cdot (\bar{X_n} - \mu)\overbrace{\sum_{i=1}^{n}(X_i-\bar{X_n})}^{0}\\
	&= \sum_{i=1}^n (X_i - \bar{X_n})^2 + n(\bar{X_n}-\mu)^2\tab[5cm] (I)
\end{split}
\end{equation*}

Taking estimation we find:
\begin{equation*}
\begin{split}
\mathbb{E}\big[\sum_{i=1}^n(X_i - \mu)^2\big] &= \mathbb{E}[n\cdot S_{n,*}^2]+\mathbb{E}[n(\bar{X_n}-\mu)^2]\tab[5cm] (II)\\
	RHS &= \sum_{i=1}^n\overbrace{\mathbb{E}(X_i - \mu)^2}^{\sigma^2} = n\cdot\sigma^2
\end{split}
\end{equation*}
Note that $\mathbb{E}(\bar{X_n}-\mu)=0$ ,\ i.e. $\mathbb{E}(\bar{X_n})=\mu$. Thus:
$$
	\mathbb{E}[n(\bar{X_n}-\mu)^2] = n\ \mathbb{E}[(\bar{X_n}-\mu)^2] = n\ Var(\bar{X_n})\ .
$$
On the other hand $Var(\bar{X_n})=\frac{\sigma^2}{n}$. We therefore have:
$$
	\mathbb{E}[n(\bar{X_n}-\mu)^2] = n\cdot Var(\bar{X_n}) = n\cdot \frac{\sigma^2}{n} = \sigma^2
$$
and hence from $(II)$:
$$
	n\sigma^2 = \mathbb{E}(n\ S_{n,*}^2) + \sigma^2
$$
which implies:
$$
	\implies \mathbb{E}(S_{n,*}^2 = (\frac{n-1}{n})\sigma^2 = (1-\frac{1}{n})\sigma^2
$$
meaning that $S_{n,*}^2$ is \textbf{NOT} an unbiased estimator of $\sigma^2$.\\
Multiplying both sides of the last equation by the reciprocal of $(1-\frac{1}{n})$ we find \mbox{$\mathbb{E}(\frac{n}{n-1}S_{n,*}^2) = \sigma^2$} . Note however that:
$$
\frac{n}{n-1}S_{n,*}^2 = \frac{\cancel{n}}{n-1}\cdot\frac{1}{\cancel{n}}\sum_{i=1}^n (X_i - \bar{X_n})^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X_n})^2
$$
Thus $\boxed{S_n^2 = \frac{1}{n-1}\sum_{i=1}^n(X_i - \bar{X_n})^2}$ is an unbiased estimator.\\ \\
\textbf{Question: Why $(n-1)$?}\\
$"n-1"$ is the dimension of $span\overbrace{\{X_i - \bar{X_n} \colon i=1,2,\ldots,n\}}^{\mathbb{V}}$\ .\newline $n-1 = dim(span\ V)$.\ \ Note however $\tab[0.3cm] dim(span\ W)=n$ where \mbox{$W={X_i-\mu\ ,\ i=1,2,\ldots,n}$}.\newline
We discuss these issues further in Chapter $11$ where we learn about the \mbox{regression}.\par
So far we only considered sampling from one population. We may have samples from two or more populations and may want to make inference about differences between the populations. 
\begin{example}
\end{example}
Suppose we want to study the differences between the average salaries of men and women:

\begin{tabular}{l r c d{1} }
Men&Women\\
$X_1$&$Y_1$\\
$\vdots$&$\vdots$\\
$X_m$&$Y_n$\\
\end{tabular}




where $X_is$ have the common mean $\mu_{_X}$ and $Y_js$ have the command mean $\mu_{_\mu}$. We want to estimate $\mu_{_X} - \mu_{_Y}$. The natural estimator is $\bar{X_m} = \bar{Y_n}$. Show that:
$$\mathbb{E}[\bar{X_m}-\bar{Y_n}] = \mu_{_X} - \mu_{_Y}$$
Hence $\bar{X_m}-\bar{Y_n}$ is an unbiased estimator of $\mu_{_X} - \mu_{_Y}$.\newline
Assume further that $X$s and $Y$s are independent and $X$s have common variance $\sigma_{_X}^2$ and $Y$s have common variance $\sigma_{_Y}^2$ and \mbox{$Cov(X_i,X_j)=0\ ,\ \ \ i\neq j$} and \mbox{$Cov(Y_i,Y_j)=0\ ,\ \ \ i\neq j$}.\newline
Find $Var(\bar{X_m} - \bar{Y_n})$. Hint: use $Thm\ 5.12$.\\
The difference between two proportions can be treated similarly. Note that proportions are essentially means of binary variables.


\newpage
\section{Lecture 5}
\section{Lecture 6}
\section{Lecture 7}
\section{Lecture 8}
\section{Lecture 9}
\section{Lecture 10}
\section{Lecture 11}
\section{Lecture 12}



\end{document}
