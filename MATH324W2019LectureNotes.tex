\documentclass[14pt,twoside,a4paper,fleqn]{article}
\usepackage{amsmath,mathtools,latexsym,forest,semantic,amsthm,cancel,stackengine,dcolumn,pxfonts,graphicx,mathrsfs}
\usepackage{enumerate,tikz}
\usetikzlibrary{trees}
\theoremstyle{plain}
%theorems and lemmas with numbering
\newtheorem{theorem}{Theorem}[section]
\newtheorem{example}{Example}[section]
\newtheorem{lemma}{Lemma}[theorem]
\newtheorem{remark}{Remark}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[section]


%theorem and lemmas without numbering
\newtheorem*{theorem*}{Theorem}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{corollary*}{Corollary}
\newtheorem*{remark*}{Remark}
\newtheorem*{definition*}{Definition}
\newtheorem*{example*}{Example}
\newtheorem*{deff}{Definitin}
\newtheorem*{thm}{Theorem}
\newtheorem*{eg}{Example}
%commands:
\newcommand\myeq{\mathrel{\stackrel{\makebox[0pt]{\mbox{\normalfont\tiny def}}}{=}}}

\newcolumntype{d}[1]{D{.}{\cdot}{#1} }

%for distribution
\makeatletter
\newcommand{\distas}[1]{\mathbin{\overset{#1}{\kern\z@\sim}}}%

\usepackage{parskip}
\setlength{\parindent}{14pt}

\usepackage{setspace}
\doublespacing
% \onehalfspacing


%for the tree:
\forestset{
  L1/.style={draw=black,},
  L2/.style={,edge={,line width=0.8pt}},
}

%	\begin{tikzpicture}
%	\node {root}[edge from parent fork right,grow=east]
%	child {node {left}}
%	child {node {right}
%	child {node {child}}
%	child {node {child}}
%	};
%	\end{tikzpicture}



\title{MATH324 (Statistics) -- Lecture Notes\\McGill University\\Prof. Masoud Asgharian}
\date{Winter 2019}
\author{Sam K.H.Targhi-Dunn\\sam.targhi@mail.mcgill.ca}


\begin{document}

\maketitle

%newcommands:
%       newcommand{\COMMAND}{REPLACEDCOMMAND}
%\newcommand{\nexists}{\not\exists}
%	tab command
\newcommand\tab[1][1cm]{\hspace*{#1}}

\tableofcontents
\newpage
\section{Lecture 1}
\textbf{Overview of statistics}
\subsection*{Point estimation: } Statistic and estimator + examples
\subsection*{Bias and Mean Square Error}
	Unbiasedness , Bias , $MSE(\hat{\theta})$ , Decomposition of MSE ($\#\ 8.8\ (\hat{\theta}_1,\hat{\theta}_5) \ , \ page\ 3294\ ,\ \#\ 8.6\ ,\ ,\ page\ 394$)
\subsection*{Common Unbiased Estimators}
$\mu ,\ p\ , \ \mu_1-\mu_2\ , \ p_1-p_2\ ,\ \And \sigma^2\ (S^{2}_{n-1}\to S^2\ \text{\ in the textbook})$
\textbf{Confidence Interval}
\subsection*{Pivotal Quantities}
$$
	\text{\#MISSING Graph - lecture 1 - p0}
$$
\textbf{Pivotal} 
\begin{equation*}
\begin{split}
\ X_i\sim F_{\theta}(x) &\implies Y_i = F^{-1}_{\theta}(X_i)\ \sim\ Unif(0,1) \ \implies\ -\log{Y_i}\ \sim\ Exp(1)\\
	&\implies \sum\limits_{i=1}^n -\log{F_{\theta}^{-1}(X_i)} = \sum\limits_{i=1}^n Y_i \sim G(n,1)\\
	&\tab Example:\ \ X_i\sim Exp(\lambda)
\end{split}
\end{equation*}
\subsection*{Large n: } $\tab\frac{\hat{\theta}_n - \theta}{\sigma_{_{\hat{\theta}}}} \ \sim \ N(0,1)\ \ \ for\ large\ n$
\tab Example : $X_i \ \sim \ N(\mu,1)$
\subsection*{Sample Size Determination: }\ Use the notes for 203.
\subsection{Overview} \#MISSING GRAPH LECTURE 1 PAGE 1

In MATH324 we cover statistical Inference (Theory of Point Estimator (TPE) , Testting Statistical Hypothesis (TSH) , Confidence Interval (C.I) and Data Generating Process (DGP).
\subsection{Parametric Models} Model is known up to finitely many unknown parameters.\\
E.g. $X_i\ \ \underset{i=1,2,\ldots,n}{\distas{iid}}\ \ \ N(\mu,\sigma^2)\ $ where $\ \mu$ and $\sigma^2$ are unknown.\\
\textbf{Note:}\tab[0.2cm]"$iid$"\ means \textbf{I}ndependent \textbf{i}dentically \textbf{d}istributed\\
\tab "$\sim$"\ means \textbf{Distributed according to}
\subsection{Nonparametric Models}$\ X_i\distas{iid} F_{_X}(x)\ $ where the cdf $F$ is completely unknown, but we may assume that $F$ is smooth, for instance continuous or differentiable.\\
In the non-parametric setting $F_{_X}(x)$ should be estimated for every $x$. Thus for a random variable $X$ that can assume infinitely many values, we need to estimate $F(x)$
 at infinitely many values of $x$. This is, particularly , the case when $X$ is a continuous random variable. Recall that \mbox{$F_{_X}(x) = P(X\leq x)$}. Then the sample counterpart of $F_{_X}(x)$ is $\frac{\# X_i\leq x}{n}$ for a sample $X_1,\ldots,X_n$ . Define:
 $$
 	\mathcal{E}(t) =\left\{
 	\begin{array}{lr}
 	1 &\tab\text{if } t\geq 0\\
 	0 &\tab\text{otherwise}
	\end{array}\right.
 $$
 Then\ \ $\ \frac{\# X_i \leq x}{n} = \frac{1}{n}\sum\limits{i=1}^n \mathcal{E}(x-X_i)\ $ \ and  
 $\ \hat{F}_n(x)=\frac{1}{n}\sum\limits_{i=1}^n(x-X_i)\ $\ is the \underline{E}mpricial \underline{C}umulative \underline{D}istribution \underline{F}unction\ (ECDF) .

\subsection{Point Estimator} Suppose $\ X_1,\ldots,X_n \distas{iid} N(\mu, 1)\ $ where:
$$
	N(\mu,1): f_{_X}(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2}}\ \ , \ \ x\in\mathbb{R} \ , \ \mu\in\mathbb{R}
$$
We want to have an estimate of $\mu$ ; i.e. a scientific guess, based on the observations , $X_1,\ldots,X_n$ . Recall that $\ \mathbb{E}(X_i) = \mu \ \ , \ \mu=1,2,\ldots,n \ , \text{$\mu$ is the population mean.}$\\
\textbf{What is an "estimate"?}\\
\textbf{Statistic:\ } A function of observations that does not depend on any unknown parameter.\\
\textbf{Estimator:\ } An estimator is a statistic that aims at estimating a function of the population unknown parameters.
\begin{example*}
$\ X_i\distas{iid}N(\mu,1)$
\end{example*}
$\bar{X}_n = \frac{1}{n}\sum\limits_{i=1}^n X_i\ $ is a \underline{statistic} and as an \underline{estimator} of $\mu$\\
$(\bar{X}_n - \mu)$ is \underline{NOT} a \underline{statistic} since it depends on $\mu$ , which is an unknown parameter.\\
$S^2=\frac{1}{n-1}\sum\limits_{i=1}^n (X_i - \bar{X}_n)^2\ $ is a \underline{statistic} , but not an estimator of $\mu$ . Note that \mbox{$dim(S^2) = (dim\ \mu)^2$} . For instance, if $X_i$s are \#MISSING-lec1-p3 of a fund and measured in dollars $(\$)$ , then $dim$ of $\mu$ is $\$$ while the $dim$ of $S^2$ is $\$^2$ . Besides, $\mu$ can be negative while $S^2$ is \emph{\underline{always positive}}.

\subsection{Estimation Error} Going back to the example above \mbox{$(X_i\distas{iid}N(\mu,1) \ , \ i=1,2,\ldots,n)$} and choosing $\bar{X}_n$ as the estimator of $\mu$ . We often want to study \mbox{$\mathcal{E} = \vert \bar{X}_n - \mu\vert$} or a function of $\mathcal{E}$. Starting with $\mathcal{E}$ itself, the first thing that comes to mind is \mbox{$P(\mathcal{E}\geq \delta)$}\ for a prespecified $\delta$ or perhaps $\mathbb{E}(\mathcal{E})$ . A well known tool for studying the latter is \emph{Tchbyshev's Inequality} .\\















\newpage
%Lecture 3
\section{Lecture 2}
%Markov's Inequality
\subsection{Markov's Inequality}
Let $X$ be a random variable and $h$ be a \textbf{non-negative} function; ie:
	\begin{equation*}
	h:R->R^{+}\ \cup\ \{0\}\tab[0.5cm]=\tab[0.5cm] [0,\infty)
	\end{equation*}
Suppose $E(h(X))\ < \infty$ ,then for some $\lambda > 0$ , we have:
	\begin{equation}\label{markov}
	P(h(X) \geq \lambda)\ \leq\ \frac{E[h(X)]}{\lambda}
	\end{equation}
\begin{proof}
Suppose $X$ is a continuous random variable:
	\begin{equation*}
	\begin{split}
	E[h(x)] &= \int_{x} h(x)f_{_X}(x)dx \\
		&=\Big(\int_{x:h(x)\geq\lambda}h(x)f_{_X}(x)dx + \int_{x:h(x) < \lambda}h(x)f_{_X}(x)dx\Big) \\
		&\geq \int_{x:h(x)\geq\lambda}h(x)f_{_X}(x)dx\tab[4cm] \underline{since}\ h\geq 0\\
		&\geq\lambda \int_{x:h(x\geq\lambda}f_{_X}(x)dx = \lambda\ P(h(X)\geq \lambda)\\
		\implies P(h(X)\geq \lambda) \leq \frac{E(h(X))}{\lambda}
	\end{split}
	\end{equation*}
\emph{The proof for the discrete case is similar.}
\end{proof} 

%Tchebushev's Inequality
\subsection{Tchebyshev's Inequality}
\emph{Tchebyshev's Inequality} is a special case of Markov's Inequality. \mbox{Consider $h(x) = (x-\mu)^2$}, then:
	\begin{equation*}
	\begin{split}
	P(\vert X-\mu \vert \geq \lambda) &= P((X-\mu)^2 \geq \lambda^2)\\
		&\leq \frac{E[(X-\mu)^2]}{\lambda^2}\tab[2cm]if\ E[(X-\mu)^2]<\infty
	\end{split}
	\end{equation*}
Let $\mu = E(X)$, then $E[(X-\mu)^2] = Var(X)$ denoted by $\sigma_{_X}^2$. We therefore have:
	\begin{equation}\label{tchev_sigma}
		P(\vert X-\mu_{_X} \vert \geq \lambda) \leq \frac{\sigma_{_X}^2}{\lambda^2}\tab[2cm] where\ \mu_{_X} = E(X)
	\end{equation}
Now consider $\lambda= K\sigma_x$ where $K$ is a known number. Then:
	\begin{equation}\label{tchev}
		P(\vert X-\mu_{_X}\vert \geq K\sigma_{_X}) \geq \frac{\sigma_{_X}^2}{K^2\sigma_{_X}^2}=\frac{1}{K^2}
	\end{equation}

This is called \textbf{Tchbyshev's Inequality}. 
\begin{example}
Suppose $K=3$.\hfill\newline
$$P(\vert X-\mu_x\vert \geq 3\ \sigma_{_X}) \leq \frac{1}{9}$$
In other words, at least $88\%$ of the observations are within $3$ standard deviation from the population mean.
\end{example}

Going back to the our example:
$$X_i \sim (\mu, 1)\tab[0.5cm],\tab[0.5cm] \bar X_n = \frac{1}{n}\sum_{i=1}^{n} X_i$$
We want to study $P(\epsilon \geq \delta) = P(\vert \bar{X}_n - \mu\vert \geq \delta)$, first we note that: $$E(X_i)=\mu\ \ \ ,\ i=1,2,\ldots,n$$
Then:
	\begin{equation*}
	\begin{split}
		E(\bar{X}_n) &= E\big(\frac{1}{n}\sum_{i=1}^{n}X_i \big)=\frac{1}{n}\sum_{i=1}^{n}E(X_i)\\
		&= \frac{1}{n}\sum_{i=1}^{n} \mu =\frac{1}{n}(n\mu)= \frac{1}{\cancel{n}}.(\cancel{n}\mu)\\
		&= \mu\tab[13cm] (*)
	\end{split}
	\end{equation*}
Thus, using (\ref{tchev_sigma}) we have:
	$$P(\vert\bar{X}_n - \mu\vert \geq \delta ) \leq \frac{Var(\bar{X}_n)}{\delta^2}$$
Now:
	\begin{equation*}
	\begin{split}
	Var(\bar{X}_n) &= Var\big(\frac{1}{n} \sum_{i=1}^{n} X_i \big) = \frac{1}{n^2}Var\big(\sum_{i=1}^{n}X_i \big)\\
	&=\frac{1}{n^2}\Big[\sum_{i=1}^{n} Var(X_i) +\sum_{1\leq i<j\leq n}\ \sum_{1\leq i<j\leq n} Cov(X_i, X_j) \Big]\ \ \ using\ Thm\ 5.12(b)-page\ 271\\
	&=\frac{1}{n^2}\sum_{i=1}^{n}Var(X_i)\tab[2cm]since\ \coprod_{1}^{n}X_i\\
	&=\frac{1}{n^{\cancel{2}}} \cancel{n} Var(X) = \frac{Var(X)}{n} \tab since\ x_{i}s\ are\ identically\ distributed\\
	&=\frac{\delta_X^2}{n}\tab[12cm](**)
	\end{split}
	\end{equation*}
	In our case $X\sim N(\mu, 1)$ so $Var(X) = \delta_X^2 = 1$. Thus $Var(\bar{X}_n) = \frac{1}{n}$
	
\begin{remark*}
$X\coprod Y \implies Cov(X,Y)=0$. Note that:
$$X \coprod Y \implies E\big[g_1(X)g_2(Y)\big] = E[g_1(X)].E[g_2(Y)]$$
in particular:
$$X \coprod Y \implies E\big[XY\big] = E[X].E[Y]$$
on the other hand:
$$Cov(X,Y) = E[XY] - E(X)E(Y)$$
thus: 
$$X\coprod Y\implies Cov(X,Y)=0.$$
\end{remark*}
recall that $X\coprod Y$ means X and Y are independent, i.e. \mbox{$f_{_{X,Y}}(x,y)=f_{_X}(x)f_{_Y}(y)$} where $f_{_{X,Y}},f_{_X} and f_{_Y}$ represent respectively the %%missed words%%%/////TODO\\




We therefore have:
\begin{equation}\
P(\vert\bar{X}_n - \mu\vert \geq \delta) \leq \frac{1}{n\delta^2}
\end{equation}
Using $(4)$ and  the sample size, $n$, we can find an upper bound for the proportion of deviations which are greater than a given threshold $\delta$.\hfill\newline
We can also use $(4)$ for \underline{Sample Size Deterministic}:\\
Suppose $\delta$ is given and we want $P(\vert\bar{X}_n - \mu\vert \geq \delta) \leq \beta$ where $\beta$ is also given. Then setting $\frac{1}{n\delta^2} = \beta$, we can estimate $n\approx\frac{1}{\beta\delta^2}$.\hfill\newline
%Application to voting
\subsection{Application to Voting}
Define $X_i = \begin{cases}
		1 & \text{NDP}\\
		0 & \text{otherwise}
		\end{cases}$ . Associated to each eligible voter in Canada we have a binary variable X. Let $p=P(X=1)$. So $p$ represents the proportion of eligible voters who favor $NDP$. Of interest is often estimation of $p$. Suppose we have a sample of size $n$, $X_1,X_2,\ldots,X_n$.\\
		$\bar{X}_n = \frac{1}{n}\sum_{i=1}^{n}X_i$ is the sample proportion; The counterpart of $p$ which nat be denoted by $\hat{p}$. Note that:
		$$\mu_{_X} = E(X) = 1 \times P(X=1) + 0\times P(X=0) = 1-p + 0\times (1-p)=p$$
		and:
		$$E(X^2) = 1^2 \times P(X=1) + 0^2 \times P(X=0) = 1-p + 0\times (1-p) = p$$
From (*) and (**) we find that :
	$$E(\hat{p}_n) = E(\bar{X}_n) \mu_{_X} = p$$
and:
	$$Var(\hat{p}_n) = E(\bar{X}_n) = \frac{Var(X)}{n} = \frac{\sigma_X^2}{n} = \frac{p(1-p)}{n}$$
Thus using (\ref{tchev_sigma}), we have:
$$P(\vert\hat{p}_n - p\vert \geq \delta) \leq \frac{Var(\hat{p}_n)}{\delta^2} = \frac{p(1-p)}{n\delta^2}$$
Note that the above bound on the probability of derivation depends on $p$ which is \emph{unknown}. We however notice that $p(1-p) \leq \frac{1}{4}$ .\\
Define $\mathscr{C}(x) = x(1-x)\ for\ 0 < x < 1$. Then:
	\begin{equation*}
	\begin{split}
	&\mathscr{C}^{'}(x) = 1-2x \implies \mathscr{C}^{'}(x)= 0 \implies x = \frac{1}{2} \\
	&\mathscr{C}^{"}(\frac{1}{2}) = -2 \implies x = \frac{1}{2}\tab \text{which is a \textbf{maximizer}}\\
	&\mathscr{C}(\frac{1}{2}) = \frac{1}{2}(1-\frac{1}{2})=\frac{1}{4}
	\end{split}
	\end{equation*}
	(Note that $\mathscr{C}^{"}(x) = -2$ for all $0 < x < 1$)
	
	
	%The graph is missing
	
	
We therefore find:
	\begin{equation}\label{voter}
	P(\vert\hat{p}_n - p\vert \geq \delta) \leq \frac{1}{4n\delta^2}
	\end{equation}
Using (\ref{voter}) and a given sample size $n$ we can find an upper bound for the probability of derivation by $\delta$ and the amount for any given $\delta$.\newline
We can also use (\ref{voter}) for \underline{sample size deterministic} for a size bound $
\beta$ and derivative $\delta$ as follows:
$$\frac{1}{4n\delta^2}= \beta\tab \implies \tab n \geq \frac{1}{4\beta\delta^2}$$
This is of course conservative since $p(1-p)\leq \frac{1}{4}$.



\newpage
\section{Lecture 3}
\subsection{MSE} 
\textbf{MSE}: To study estimation error we started by studying $P(\vert \hat{\Theta}_n - \Theta\vert > \delta)$ , deviation above a given threshold $\delta$, by bounding this probability. One may take a different approach by studying average Euclidean distance, i.e. $E[\ \vert\hat{\Theta}_n - \Theta\vert ^2]$, which denoted by \textbf{MSE($\hat{\Theta}_n$)}.\newline
We note that if $\Theta = E(\hat{\Theta}_n)$, i.e. $\hat{\Theta}_n$ is an unbiased estimation of $\Theta$, then: 
$$MSE(\hat{\Theta}_n) = E[\vert\hat{\Theta}_n - \Theta\vert^2] = E[(\hat{\Theta}_n - \mu _{n_{\Theta _n}})^2] = Var(\hat{\Theta}_n)$$
Now recall that $Var(X) = 0\  \implies \ P(X = \text{constant}) = 1$ which essentially means random variable X is a constant.\\
The same comment applies to MSE($\hat{\Theta}_n$). We want to find the closest estimator $\hat{\Theta}_n$ to $\Theta$ which means that we want to minimize $E[(\hat{\Theta}_n - \Theta)^2]$ over all possible estimators, ideally at least the above comment tells us that in real applications we cannot expect to find an estimator whose MSE is equal to zero. Let's try to understand the MSE a bit more:
\begin{equation*}
\begin{split}
\text{MSE}(\hat{\Theta}_n) &= E[(\hat{\Theta}_n - \Theta)^2]\\
	&= E\Big[\Big(\big(\hat{\Theta}_n - E(\hat{\Theta}_n)\big) + \big(E(\hat{\Theta}_n) - \Theta\big) \Big)^2 \Big]\\
	&= E\big[\big(\hat{\Theta}_n - E(\hat{\Theta}_n)\big)^2\big] + \big(E(\hat{\Theta}_n) - \Theta\big)^2 + 2 \cdot E\big[\big(\hat{\Theta}_n - E(\hat{\Theta}_n)\big)\big]\cdot \big(E(\hat{\Theta}_n) - \Theta\big)\\
	&= E\big[(\hat{\Theta}_n - E(\hat{\Theta}_n))^2\big] + E\big[\overbrace{(E(\hat{\Theta}_n) - \Theta)^2}^{\text{not a r.v.}}\big]
		+2\cdot E\Big[\overbrace{\big(E(\hat{\Theta}_n) - \Theta)}^{\text{not a r.v.}}\big)\cdot \big(\hat{\Theta}_n - E(\hat{\Theta}_n)\big) \Big]\\
		&= Var(\hat{\Theta}_n) + \big[\overbrace{E(\hat{\Theta}_n) - \Theta}^{\text{Bias$(\hat{\Theta}_n)$}}\big]^2 + 2\cdot Bias(\hat{\Theta}_n)\cdot \overbrace{E[(\hat{\Theta}_n - E(\hat{\Theta}_n))]}^{E(\hat{\Theta}_n) - E(\hat{\Theta}_n) = 0}\\
		&= Var(\hat{\Theta}_n) + Bias^2(\hat{\Theta}_n)
\end{split}
\end{equation*}
Roughly speaking, \textbf{bias} measures how far off the target we hit on the average while \textbf{variance} measures how much fluctuation our estimator may show from one sample to another.

\subsection{Unbiased Estimators}
In almost all real applications, the class of possible estimators for an \textbf{ESTIMANAL} is huge and the best estimator, i.e. the one that minimizes MSE no matter what the value of the \textbf{ESTIMANAL} is, almost never exists. Thus we try to reduce the class of potential estimators by improving a plausible restriction, for example Bias$(\hat{\Theta}_n) = 0$.

\begin{definition*}An estimator  $\hat{\Theta}_n$ of an \textbf{ESTIMANAL} $\Theta$ is said to be \textbf{unbiased} if $E(\hat{\Theta}_n) = \Theta$ , for all possible values of $\Theta$.
\end{definition*}
\begin{example}
$X_i\distas{iid}  N(\mu, \sigma^2)\tab i=1,2,\ldots,n$\\
Suppose both $\mu$ and $\sigma^2$ are unknown. Consider $\bar{X}_n = \frac{1}{n}\sum_{i=1}^{n} X_i$.
$$E(\bar{X_n}) = E(\frac{1}{n}\sum_{i=1}^{n} X_i) = \frac{1}{n}\sum_{i=1}^n \overbrace{E(X_i)}^{\mu} = \frac{1}{\cancel{n}}\cdot \cancel{n}\mu = \mu$$
\end{example}
Thus $\bar{X_n}$ is an unbiased estimator of $\mu$. As for the MSE$(\bar{X_n})$, we need to find Var$(\bar{X_n})$.
\begin{equation*}
\begin{split}
Var(\bar{X_n})&= Var\big(\frac{1}{n}\sum_{i=1}^{n} X_i\big) = \frac{1}{n^2} Var(\sum_{i=1}^n X_i)\\
	&= \frac{1}{n^2}\Big[\sum_{i=1}^n Var(X_i) + 2\cdot\sum\sum_{1\leq i < j \leq n} \overbrace{Cov(X_i,X_j)}^{0} \Big]\tab[2cm] \text{Theorem 5.12(b) - page 271}\\
	&= \frac{1}{n^2}\sum_{i=1}^n Var(X_i)\tab[8cm] \coprod_{i=1}^n X_i\\
	& = \frac{1}{n^2}\sum_{i=1}^{n} \sigma^2 = \frac{1}{n^{\cancel{2}}}\cdot\cancel{n}\sigma^2 = \frac{\sigma^2}{n}\tab[5cm]\ \ \text{identically distributed}\\
&\implies MSE(\bar{X_n}) = Var(\bar{X_n}) + \overbrace{Bias^2(\bar{X_n})}^{0}=Var(\bar{X_n}) = \frac{\sigma^2}{n}
\end{split}
\end{equation*}
An inspection of the above calculation shows that for unbiased $\mu$ we only require a common mean $\mu$ while for calculating the variance we would only require a common variance $\sigma^2$ and orthogonality, i.e:
$$Cov(X_i,X_j) = 0\tab \text{where}\ i\neq j$$
Suppose $X_1,\ldots,X_n$ have the same mean value $\mu$. Then:
$$E(\bar{X_n}) = E(\frac{1}{n}\sum_{i=1}^n X_i) = \frac{1}{n}\sum_{i=1}^n E(X_i) = \frac{1}{\cancel{n}}\cancel{n}\mu=\mu$$
Suppose further that $ X_1,\ldots ,X_n$ have the same variance $\sigma^2$ and \mbox{$Cov(X_i,X_j) = 0,\ i\neq j$}. Then:
\begin{equation*}
\begin{split}
Var(\bar{X_n}) &= Var(\frac{1}{n}\sum_{i=1}^n X_i) = \frac{1}{n^2} Var(\sum_{i=1}^n X_i)\\
	&= \frac{1}{n^2}\Big[\ \sum_{i=1}^nVar(X_i) + 2\sum\sum_{1\leq i < j \leq n} Cov(X_i, X_j)\ \Big]\tab[3cm] \text{Theorem 5.12(b) - Page 271}\\
	&= \frac{1}{n^2}\sum_{i=1}^n Var(X_i)\tab[5cm] \text{Orthogonality:\ i.e. $Cov(X_i,X_j) = 0\ \ if\ i\neq j$}\\
	&=\frac{1}{n^2}\sum_{i=1}^n \sigma^2 = \frac{1}{n^{\cancel{2}}}\cancel{n}\sigma^2 = \frac{\sigma^2}{n} \tab[4cm] \text{having the same variance}\\
	&\implies MSE(\bar{X_n}) = Var(\bar{X_n}) = \frac{\sigma^2}{n}
\end{split}
\end{equation*}
If $X_1,\ldots,X_n$ have the same mean value and variance and they are orthogonal.
\subsection{Stein's Paradox}
We will learn later that if $X_i\distas{iid} N(\mu,\sigma^2)$ then \mbox{$\bar{X_n} = \frac{1}{n}\sum_{i=1}^n X_i$} has many optimal properties. A paradox due to Charles Stein, however, shows that such a nice optimal properties are not preserved in higher dimensions. In fact if:
$$X_i\distas{iid}N(\mu_{_X},1),\ \ Y_i\distas{iid}N(\mu_{_Y},1)\ \text{and  }Z_i\distas{iid}(\mu_{_Z},1)$$
then, we can find the biased estimators of 
$\begin{pmatrix} \mu_{_X}\\\mu_{_Y}\\ \mu_{_Z} \end{pmatrix}$
which are closer to 
$\begin{pmatrix} \mu_{_X}\\\mu_{_Y}\\ \mu_{_Z} \end{pmatrix}$ 
than 
$\begin{pmatrix}\bar{X_n}\\\bar{Y_n}\\\bar{Z_n} \end{pmatrix}$ 
for any 
$\begin{pmatrix} \mu_{_X}\\\mu_{_Y}\\ \mu_{_Z} \end{pmatrix}$.
We may then say that 
$\begin{pmatrix}\bar{X_n}\\\bar{Y_n}\\\bar{Z_n} \end{pmatrix}$
 is an \mbox{\textbf{\underline{inadmissible estimator}} of 
 $\begin{pmatrix} \mu_{_X}\\\mu_{_Y}\\ \mu_{_Z} \end{pmatrix}$.}
 \subsection{Admissibility}
 An estimator $\hat{\Theta}$ is called admissible if there is no estimator $\tilde{\Theta}$ such that:
 $$MSE(\tilde{\Theta}) \leq MSE(\hat{\Theta})\tab[3cm]\text{for all possible values of $\Theta$}$$
and this inequality is strict for some values of $\Theta$.\newline
What this example tells us is that by allowing a bit of bias we may be able to reduce variance considerably and hence find an estimator which is closer to the target than the most natural unbiased estimator. Note that this phenomena happens only when the dimension is at least 3.





\newpage
\section{Lecture 4}
We now want to restrict the class of estimators even further. Suppose $X_1,\ldots,X_n$ have the same mean $\mu$ and variance $\sigma^2$ and they are orthogonal; i.e. \mbox{$Cov(X_i,X_j) = 0\ ,\ i\neq j$}. Consider $\tilde{X}_{n,\underset{\sim}{C}} = \sum_{i=1}^n C_i\ X_i$ and
$$\mathscr{C} = \Big\{\tilde{X}_{n,\underset{\sim}{C}}\ \colon \underset{\sim}{C} = 
	(C_1,\ldots,C_n)\in \mathbf{R}^n ,\ \sum_{i=1}^n C_i = 1 \Big\} $$
Note that
\begin{equation*}
\begin{split}
E(\tilde{X}_{n,\underset{\sim}{C}}) &= E(\sum_{i=1}^n C_i\ X_i) = \sum_{i=1}^n C_i\ E(X_i)\\
	&= \sum_{i=1}^n C_i\ \mu = \mu\overbrace{\sum_{i=1}^n C_i}^1 = 1\cdot\mu\\
	&=\mu
\end{split}
\end{equation*}
Thus $\tilde{X}_{n,\underset{\sim}{C}}$ is an unbiased estimator of $\mu$ fir any $\underset{\sim}{C}\in\mathbf{R}^n$ as long as $\sum_{i=1}^n C_i = 1$.
Then $\mathscr{C}$ is the class of all unbiased linear estimators of $\mu$. We want to find the best estimator with $\mathscr{C}$; i.e.:
$$\underset{\underset{\sim}{C}\in\mathbf{R}^n}{\text{Min}}\ MSE(\tilde{X}_{n,\underset{\sim}{C}})\tab[0.5cm] s.t\ \sum_{i=1}^n C_i = 1\tab[2cm] (*)$$
First we note that $MSE(\tilde{X}_{n,\underset{\sim}{C}}) = Var(\tilde{X}_{n,\underset{\sim}{C}})$ since $\tilde{X}_{n,\underset{\sim}{C}}$ is an unbiased estimator of $\mu$ when $\sum_{i=1}^n C_i = 1$. On the other hand:
\begin{equation*}
\begin{split}
Var(\tilde{X}_{n,\underset{\sim}{C}}) &= Var(\sum_{i=1}^n C_i\ X_i)\\
	&= \sum_{i=1}^n {C_i}^2\ Var(X_i) + 2\sum\sum_{1\leq i<j\leq n} Cov(C_iX_i,C_j,X_j)\tab \text{Theorem 5.12 page 271}\\
	&= \sum_{i=1}^n {C_i}^2 \sigma^2 + 2 \cancel{\sum\sum_{1\leq i<j\leq n} C_iC_j\overbrace{Cov(X_i,X_j)}^0}\\
	&= \sigma^2\sum_{i=1}^n {C_i}^2 \\
\text{Thus (*) is equivalent to}:\\
	&\underset{\underset{\sim}{C}\in\mathbf{R}^n}{\text{Min}}\ \sigma^2\sum_{i=1}^n {C_i}^2\tab[2cm] (**)
\end{split}
\end{equation*}
Using the \emph{Lagrange Theorem}, (**) is equivalent to:
$$
\underset{\sim}{C} = \overset{Min}{(C_1,...,C_n)} \in\mathbb{R}^n\ \ \big\{\overbrace{\sigma^2\sum_{i=1}^n C_i + \lambda(\sum_{i=1}^n C_i -1)}^{\mathscr{C_{_\lambda} (\underset{\sim}{\text{C}})}}\big\}\ .
$$
Note that: 
$
\tab\frac{\partial\ \mathscr{C}_{\lambda}(\underset{\sim}{C})}{\partial C_i}
	= 2\ \sigma^2\ C_i + \lambda\tab[0.5cm] ,\ i=1,2,3,\ldots\\
$\\
$\frac{\partial}{\partial \lambda} \mathscr{C}_{\lambda}(\underset{\sim}{C})
	= \sum_{i=1}^n C_i - 1$\newline\\
$\begin{cases}
		\frac{\partial}{\partial C_i}\mathscr{C}_{\lambda}(\underset{\sim}{C})
			= 2\ \sigma^2\ C_i + \lambda = 0\ \ ,\ \ i=1,2,3,\ldots\\
		\frac{\partial}{\partial \lambda} \mathscr{C}_{\lambda} = 0\ \ \implies \ \ \sum_{i=1}^n C_i =1
		\end{cases}$\hfill\newline\\
Thus $C_i = -\frac{\lambda}{2\ \sigma^2}\ \ , \ \ i=1,2,3,\ldots,n\ $  and using the last equation:
$$
\sum_{i=1}^n -\frac{\lambda}{2\ \sigma^2} = 1 \implies \lambda = -\frac{2\ \sigma^2}{n}
$$
and therefore:
$$
C_i = -\frac{\lambda}{2\ \sigma^2} = -\frac{-\frac{2\ \sigma^2}{n}}{2\ \sigma^2} = \frac{1}{n}\tab ,\ \ i=1,2,3,\ldots,n
$$
We can further find:
$$\ \mathcal{H} = [\frac{\partial^2}{\partial C_i \partial C_j} \mathscr{C}_{\lambda}(\underset{\sim}{C})]\tab,\ \ i,j=1,2,\ldots,n$$
and show that:
\begin{equation*}
\begin{split}
\underset{\sim}{x}^T\ \mathcal{H}\underset{\sim}{x}\ &\geq\ 0\tab \forall \underset{\sim}{x}\in\mathbb{R}^n\\
&=0\tab \text{if and only if \ } \underset{\sim}{x} = 0
\end{split}
\end{equation*}
This then guarantees that ${\underset{\sim}{C}}^{*} = (\frac{1}{n},\frac{1}{n},\ldots,\frac{1}{n})$ is indeed a minimizer; in fact, the \mbox{\emph{unique minimizer}}. To summarize:
$$
\tilde{X}_{n,{\underset{\sim}{C}}^{*}} = \sum{i=1}^n \frac{1}{n} X_i = \frac{1}{n}\sum_{i=1}^n X_i = \bar{X_n}
$$
Thus $\bar{X_n}$ is the best unbiased linear estimator.
\subsection{Estimating Variance}
So far we confirmed ourselves to estimation of th population mean.\newline
Now suppose we are interested in estimating variance from $X_1,\ldots,X_n$ where $X_is$ have the same mean value $\mu$\ , the same variance $\sigma^2$ and they are orthogonal, i.e. \mbox{$Cov(X_i,X_j)=0\ ,\ i\neq j$}, then a \mbox{\emph{natural estimator}} of:
$$
\sigma^2 = Var(X) = \mathbb{E}[(x-\mu)^2]
$$
is \underline{its sample counterpart}, i.e.
$$
S_{n,*}^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X_n})^2
$$
Now the first question is if $S_{n,*}^2$ is an unbiased estimator of $\sigma^2$ , i.e. $\mathbb{E}(S_{n,*}^2) = \sigma^2$


\begin{equation*}
\begin{split}
(X_i - \mu)^2 &= \big[(X_i - \bar{X_n})+(\bar{X_n}-\mu)\big]^2\\
	&= (X_i - \bar{X_n})^2 + (\bar{X_n}-\mu)^2 + 2\cdot (X_i - \bar{X_n}) (\bar{X_n}-\mu)\\
\sum_{i=1}^n(X_i -\mu)^2 &= \sum_{i=1}^n (X_i - \bar{X_n})^2 + n(\bar{X_n}-\mu)^2 + 2\cdot (\bar{X_n} - \mu)\overbrace{\sum_{i=1}^{n}(X_i-\bar{X_n})}^{0}\\
	&= \sum_{i=1}^n (X_i - \bar{X_n})^2 + n(\bar{X_n}-\mu)^2\tab[5cm] (I)
\end{split}
\end{equation*}

Taking estimation we find:
\begin{equation*}
\begin{split}
\mathbb{E}\big[\sum_{i=1}^n(X_i - \mu)^2\big] &= \mathbb{E}[n\cdot S_{n,*}^2]+\mathbb{E}[n(\bar{X_n}-\mu)^2]\tab[5cm] (II)\\
	RHS &= \sum_{i=1}^n\overbrace{\mathbb{E}(X_i - \mu)^2}^{\sigma^2} = n\cdot\sigma^2
\end{split}
\end{equation*}
Note that $\mathbb{E}(\bar{X_n}-\mu)=0$ ,\ i.e. $\mathbb{E}(\bar{X_n})=\mu$. Thus:
$$
	\mathbb{E}[n(\bar{X_n}-\mu)^2] = n\ \mathbb{E}[(\bar{X_n}-\mu)^2] = n\ Var(\bar{X_n})\ .
$$
On the other hand $Var(\bar{X_n})=\frac{\sigma^2}{n}$. We therefore have:
$$
	\mathbb{E}[n(\bar{X_n}-\mu)^2] = n\cdot Var(\bar{X_n}) = n\cdot \frac{\sigma^2}{n} = \sigma^2
$$
and hence from $(II)$:
$$
	n\sigma^2 = \mathbb{E}(n\ S_{n,*}^2) + \sigma^2
$$
which implies:
$$
	\implies \mathbb{E}(S_{n,*}^2 = (\frac{n-1}{n})\sigma^2 = (1-\frac{1}{n})\sigma^2
$$
meaning that $S_{n,*}^2$ is \textbf{NOT} an unbiased estimator of $\sigma^2$.\\
Multiplying both sides of the last equation by the reciprocal of $(1-\frac{1}{n})$ we find \mbox{$\mathbb{E}(\frac{n}{n-1}S_{n,*}^2) = \sigma^2$} . Note however that:
$$
\frac{n}{n-1}S_{n,*}^2 = \frac{\cancel{n}}{n-1}\cdot\frac{1}{\cancel{n}}\sum_{i=1}^n (X_i - \bar{X_n})^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X_n})^2
$$
Thus $\boxed{S_n^2 = \frac{1}{n-1}\sum_{i=1}^n(X_i - \bar{X_n})^2}$ is an unbiased estimator.\\ \\
\textbf{Question: Why $(n-1)$?}\\
$"n-1"$ is the dimension of $span\overbrace{\{X_i - \bar{X_n} \colon i=1,2,\ldots,n\}}^{\mathbb{V}}$\ .\newline $n-1 = dim(span\ V)$.\ \ Note however $\tab[0.3cm] dim(span\ W)=n$ where \mbox{$W={X_i-\mu\ ,\ i=1,2,\ldots,n}$}.\newline
We discuss these issues further in Chapter $11$ where we learn about the \mbox{regression}.\par
So far we only considered sampling from one population. We may have samples from two or more populations and may want to make inference about differences between the populations. 
\begin{example}
\end{example}
Suppose we want to study the differences between the average salaries of men and women:

\begin{tabular}{l r c d{1} }
Men&Women\\
$X_1$&$Y_1$\\
$\vdots$&$\vdots$\\
$X_m$&$Y_n$\\
\end{tabular}

where $X_is$ have the common mean $\mu_{_X}$ and $Y_js$ have the command mean $\mu_{_\mu}$. We want to estimate $\mu_{_X} - \mu_{_Y}$. The natural estimator is $\bar{X_m} = \bar{Y_n}$. Show that:
$$\mathbb{E}[\bar{X_m}-\bar{Y_n}] = \mu_{_X} - \mu_{_Y}$$
Hence $\bar{X_m}-\bar{Y_n}$ is an unbiased estimator of $\mu_{_X} - \mu_{_Y}$.\newline
Assume further that $X$s and $Y$s are independent and $X$s have common variance $\sigma_{_X}^2$ and $Y$s have common variance $\sigma_{_Y}^2$ and \mbox{$Cov(X_i,X_j)=0\ ,\ \ \ i\neq j$} and \mbox{$Cov(Y_i,Y_j)=0\ ,\ \ \ i\neq j$}.\newline
Find $Var(\bar{X_m} - \bar{Y_n})$. Hint: use $Thm\ 5.12$.\\
The difference between two proportions can be treated similarly. Note that proportions are essentially means of binary variables.

\newpage
\section{Lecture 5 : Confidence Intervals}
\begin{definition*}{Random Interval}
An interval whose endpoint(s) are random variables is called a \mbox{\textbf{Random Interval}}.
\end{definition*}
\subsection{Confidence Intervals}
A $100(1-\alpha)\%\ $ confidence interval for a parameter $\theta$ is a \emph{random interval} \mbox{$\big(\hat{\Theta}_L(X),\hat{\Theta}_V(X)\big)$} such that:
$$
	P(\hat{\Theta}_L(X) < \Theta < \hat{\Theta}_V(X))
$$
\textbf{Pivotal Quantity} : A function of the observation $X_1,\ldots,X_n$ and some unknown parameters, ideally just the parameter(s) of interest, whose distribution DOES NOT depend on any unknown parameter is called \textbf{Pivotal Quantity}.\\
Pivotal Quantities play a central role in theory confidence intervals.
\begin{example*}\emph{
Let $X_i \distas{iid} N(\mu, \sigma^2)\ ,\ i=1,2,\ldots,n\ $ where $\sigma^2$ is \underline{known}, but $\mu\ $ is \underline{unknown}. We show that 
$$
\bar{X}_n \sim N(\mu,\frac{\sigma^2}{n})\tab where\tab \bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i
$$}
\end{example*}
Recall that there are three  methods for finding the distribution of a function of random variables:
\begin{enumerate}
\item\textbf{Method of Transformation:} This si essentially theorem of change of variables in calculus.

\item\textbf{Method of Distribution:} On this method we connect the \emph{cdf} of the new variable to the \emph{cdf} of the original variables.
	\begin{example*}
	Suppose $X_i\distas{iid} f\ ,\ i=1,2,\ldots,n\ $ are continuous random variables with \emph{pdf} $\ f$ and \emph{cdf} F. Define $X_{(n)} = \underset{1 \leq i \leq n}{max}$ .
	\end{example*}
	\begin{equation*}
	\begin{split}
	F_{x(n)}(t)= P(X_{(n)} \leq t) &= P(X_1 \leq t, X_2\leq t,\ldots,X_n\leq t)\\
	&=\prod_{i=1}^nP(X_i\leq t)	\tab[2cm] (\text{by }\coprod_{i=1}^n X_i)\\
	&=\prod_{i=1}^n F_{X_i}(t)\\
	&=\prod_{i=1}^n F(t) = F^n(t)\tab[2cm] \text{identically distributed} \\
	\text{Thus}\\
	f_{X(n)}(t) &= \frac{d}{dt} F_{X(n)}(t) = \frac{d}{dt}F^n(t)\\
		&=n\ f(t)\ F^{n-1}(t)
	\end{split}
	\end{equation*}

\item\textbf{Method of Moment Generating Function(mgf): } This method is essentially based on the \emph{mgf} of the new variable of the \emph{mgf} if the original variables.
	\begin{example*}
		Suppose $X_i\sim N(\mu_{i}, \sigma_i^2)\ ,\ i=1,2,\ldots,n$ and $X_is$ are independent. Define $S=\sigma_{i=1}^n X_i$.
	\end{example*}
	\begin{equation*}
	\begin{split}	
		m_s(t) &=\mathbb{E}[e^{tS}] = \mathbb{E}[e^{t \sum_{i=1}^n X_i}]\\
		&= \mathbb{E}\Big[\prod_{i=1}^n e^{t X_i}\Big]\tab[3cm]\text{using independence: }(\coprod)\\
		&=\prod_{i=1}^n m_{X_i}(t)\\
		&=\prod_{i=1}^n e^{\mu_i t + \frac{\sigma_i^2 t^2}{2}}			\\
		&=exp\Big\{t\sum_{i=1}^n \mu_i + \frac{t^2}{2}\sum_{i=1}^n \sigma_i^2 \Big\}\\
\implies &S\sim N(\sum_{i=1}^n \mu_i, \sum_{i=1}^n \sigma_i^2)
	\end{split}
	\end{equation*}
	If we further assume that $X_is$ are identically distributed, then: $$\mu_i = \mu \tab\text{and}\tab\sigma_i^2 = \sigma\tab\forall i=1,2,\ldots,n$$
	Therefore we have:
	$$
	m_S(t) = exp\Big\{n\mu t + \frac{n\sigma^2 t^2}{2}  \Big\}
	$$
	and hence:
	\boxed{$$S\sim N(n\mu, n\sigma^2)$$}
	Then:
	\begin{equation*}
	\begin{split}	
		m_{\bar{X}_n}(t) &= \mathbb{E}[e^{t\bar{X}_n}] = \mathbb{E}[e^{t \frac{1}{n}\sum_{i=1}^n X_i}]\\
		\text{by }t^{*}=\frac{t}{n}\implies\tab&=E[e^{t^{*}S}]\\
		&= m_S(t^{*}) = e^{n\mu t^{*} + \frac{n\sigma^2 {t^{*}}^2}{2}}\\
		&=exp\Big\{n\mu t^{*} + \frac{n\sigma^2 {t^{*}}^2}{2}\Big\}\\
		\implies \text{\boxed{$$\bar{X}_n \sim N(\mu,\frac{\sigma^2}{n})$$}}\tab[3cm](1)
	\end{split}
	\end{equation*}
\end{enumerate}
Note further that if $X\sim N(\mu,\sigma^2)$ , then $Z=\frac{X-\mu}{\sigma} \sim N(0,1)$ . We prove a general form of this. Let $X\sim N(\mu,\sigma^2)$ ; then:
$$
	aX+b \sim N(a\mu +b\ ,\ a^2\sigma^2)\tab \text{for any constant $a,b$}
$$
	Let $V=ax+b$ , then:
	\begin{equation*}
	\begin{split}
	m_v(t) &= \mathbb{E}[e^{tV}] = \mathbb{E}[e^{t(ax+b)}]\\
		&= \mathbb{E}[e^{taX+tb}] = \mathbb{E}[\underbrace{e^{tb}}_{constant}\cdot e^{\overbrace{taX}^{t^{*}}}]\\
		&=e^{tb}\cdot \mathbb{E}[e^{t^{*}X}]\\
		&=e^{tb}\cdot e^{\mu t a+\frac{\sigma^2 t^2 a^2}{2}}\\
		&=exp\ \Big\{t(a\mu+b) + \frac{t^2(a^2\sigma^2)}{2}	\Big\}\\
	Thus:\\
	(ax&+b)\sim N(a\mu +b\ ,\ a^2\sigma^2)\\
	Now:\\
	Z&=\frac{X-\mu}{\sigma} = \frac{X}{\sigma} - \frac{\mu}{\sigma}\\
	&=\frac{1}{\sigma}X - \frac{\mu}{\sigma}\\
	&=aX+b\tab[3cm]\text{where $\ a=\frac{1}{\sigma}\ $ and $\ b=-\frac{\mu}{\sigma}$}\\
Hence:\\
	Z\sim&\ N(\overbrace{\frac{1}{\sigma}\mu +(-\frac{\mu}{\sigma})}^0 \ ,\ \overbrace{(\frac{1}{\sigma})^2\sigma^2}^1)\\
Thus:\\ 
	&\text{\boxed{$$Z\sim N(0,1)$$}}\tab[3cm](2)\\
\text{Using (1) and (2) :}\\
	\frac{\bar{X}_n - \mu}{\sqrt{\frac{\sigma^2}{n}}} &= \frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}} \sim N(0,1).\\
	\text{This means that :}\\
	&\text{\boxed{$$\frac{\bar{X}_n - \mu}{\sqrt{h}}$$\text{ is a Pivotal Quantity}}}
	\end{split}
	\end{equation*}
To summarize:
$$
	X_i\distas{iid} N(\mu, \sigma^2)\tab\implies\tab \frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}}\tab\text{is a \textbf{pivotal quantitity}}
$$
Notice that using the table for the normal distribution:
$$
	P\Big(\big\vert \frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}}\big\vert \leq 1.96	\Big) = 0.95
$$
Equivalently:
$$
	P\Big(\bar{X}_n - 1.96\frac{\sigma}{\sqrt{n}}\ \ \leq\ \mu\ \leq \bar{X}_n + 1.96\frac{\sigma}{\sqrt{n}} \Big) = 0.95
$$
This means that:
$$
	(\bar{X}_n - 1.96\frac{\sigma}{\sqrt{n}}\ ,\ \bar{X}_n + 1.96\frac{\sigma}{\sqrt{n}})
$$
covers the true $\mu$ with $\ 95 \%\ $ probability.\\
Thus a $\ 100\ (1-\alpha)\%$ confidence interval for $\mu$ where $X_i\distas{iid} N(\mu,\sigma^2)$ and $\sigma^2$ is known:\\
$$
	\bar{X}_n \pm \zeta_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}}
$$
where:
$$
	P(Z > \zeta_{\frac{\alpha}{2}}) = \frac{\alpha}{2}\tab,\tab Z\sim N(0,1)
$$
\#MISSING GRAPH - (LECTURE 5 - P5)


\begin{remark*}
\emph{
In real applications we compute $\bar{X}_n$ and obtain an interval, say \mbox{$(125, 135)$} . Now either this interval covers the true $\mu$ or it does not. Then the question is what do we mean by a $95\%$ \#MISSING ?\\
Note that the $\ 100(1-\alpha)\%\ $ confidence is the property of the procedure. It means that out of the all possible intervals of the form \mbox{$(\bar{X}_n \pm 1.96 \frac{\sigma}{\sqrt{n}})$} that we can make by taking samples of size $n$ from $N(\mu,\sigma^2)$, $\ 95\%$ of them cover the true $\mu$ . Now this is a real application when we make one of the such intervals by taking a random sample of size $n$ from $N(\mu,\sigma^2)$ , it is like taking one of those intervals randomly. Since that $\ 95\%$ of them cover $\mu$ , my chance of selecting an interval that covers $\mu$ is $\ 95\%\ $. Thus I can take a bet $19$ to $1$ that the interval I select covers $\mu$. 
}
\end{remark*}

\subsection{Large Sample Confidence Interval}
The derivation of the pivotal quantity in the above example totally hinges over the normality assumption, i.e. \mbox{$X_i \sim N(\mu,\sigma^2)$}.\\
What happens if we do not know the parametric for the population distribution?
\begin{theorem*}[\textbf{General Limit Theorem - GLT} (baby version)]
\emph{
Suppose $X_1,\ldots,X_n$ are independent random variables with common $\mu$ and variance $\sigma^2$ . Then:
$$\frac{\bar{X}_n - \mu}{(\frac{\sigma}{\sqrt{n}})}\ \ \distas{app}\ \ N(0,1)\tab\text{when \ $n$\ \ is large enough}$$
}
\end{theorem*}
This is a powerful theorem that implies that $\frac{\bar{X}_n - \mu}{(\frac{\sigma}{\sqrt{n}})}$ is approximately a pivotal quantity distributed according to $N(0,1)$ for large enough $n$ regardless of population distribution provided that the condition of the \textbf{GLT} are met.\\

\begin{forest}
    for tree={
        grow=0,reversed, % tree direction
        parent anchor=east,child anchor=west, % edge anchors
        edge={line cap=round},outer sep=+1pt, % edge/node connection
        rounded corners,minimum width=15mm,minimum height=8mm, % node shape
        l sep=10mm % level distance
    }
  [$\sigma^2$,L1
    [$\sigma^2$ is known:
    	$\implies\ (\bar{X}_n \pm \zeta_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}})$ is a $\ 100(1-\alpha)\%\ $ \#MISSING for $\mu$
    ,L2]
    [$\sigma^2$ is unknown:
    	$\implies\ (\bar{X}_n \pm \zeta_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}})$ is not \underline{not useful}
    ,L2]
  ]
\end{forest}



Note: if $\sigma^2$ is unknown, $(\bar{X}_n \pm \zeta_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}})$ is still a $\ 100(1-\alpha)\%\ $ \#MISSING for $\mu$ but not useful.\\
We need to somehow get rid of the \#MISSING parameter $\sigma$ . We can replace $\sigma$ by $S_n$ where:
$$
	S_n^2 = \frac{1}{n-1} \sum_{i=1}^{n}(X_i - \bar{X}_n)^2
$$ 

\begin{forest}
    for tree={
        grow=0,reversed, % tree direction
        parent anchor=east,child anchor=west, % edge anchors
        edge={line cap=round},outer sep=+1pt, % edge/node connection
        rounded corners,minimum width=15mm,minimum height=8mm, % node shape
        l sep=10mm % level distance
    }
  [Justification,L1
    [Intuitive, L2]
    [Formal, L2]
  ]
\end{forest}
\begin{enumerate}
\item[\textbf{Intuitive}] : 
	$S_n^2$ is the sample counterpart, almost, for $\sigma^2$ . Thus as $n$ increases, greater portion of the population and hence our sample sets closer to the population.
\item[\textbf{Formal}] : 
	The formal proof comprises three steps:
	\begin{enumerate}[1.]
	\item \textbf{GLT} of $$\frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}}$$
	\item Consistency of $S_n^2$ for $\sigma^2$, i.e. $S_n^2 \overset{P}{\rightarrow} \sigma^2$ which we learn in \emph{Chapter 9}. We then use a theorem called \textbf{Continuous Mapping Theorem} which says that if \mbox{$S_n^2 \overset{P}{\rightarrow} \sigma^2$}, then:
	$$
	g(S_n^2) \overset{P}{\rightarrow} g(\sigma^2)\tab[2cm]\text{for any continuous function}
	$$
	Considering $g(x) = \sqrt{X}$ , we obtain $S_n\overset{P}{\rightarrow}\sigma$ and hence $\frac{\sigma}{S}\overset{P}{\rightarrow} 1$.
	\item \textbf{Cramer's Theorem :}
			
	 This result says that if $V_n\overset{D}{\rightarrow} X$ and $Y_n\overset{P}{\rightarrow} 1$ , then $Y_n\cdot V_n \overset{D}{\rightarrow} X$ :\\
		$$\frac{\bar{X}_n - \mu}{\frac{S}{\sqrt{n}}} = \underbrace{\frac{\sigma}{S_n}}_{Y_n} \cdot \underbrace{\frac{\bar{X_n} - \mu}{\frac{\sigma}{\sqrt{n}}}}_{V_n}$$
	\end{enumerate}
\end{enumerate}
Note that GLT implies that $V_n\overset{D}{\rightarrow}2$ , i.e:
$$
\overbrace{F_{V_n}(t)}^{\text{cdf of $V_n$}} \rightarrow F_{_Z}(t) =\overbrace{\int_{-\infty}^t\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx}^{\Phi(t)\text{ cdf of }N(0,1)}
$$
Using step (2), $Y_n = \frac{\sigma}{S_n}\overset{P}{\rightarrow} 1$ and application of Cramer's Theorem computes the proof.
To summarize:

\begin{forest}
    for tree={
        grow=0,reversed, % tree direction
        parent anchor=east,child anchor=west, % edge anchors
        edge={line cap=round},outer sep=+1pt, % edge/node connection
        rounded corners,minimum width=15mm,minimum height=8mm, % node shape
        l sep=10mm % level distance
    }
  [$\sigma^2$,L1
    [$\sigma^2$ is known $\implies\ \ \ (\bar{X}_n \pm \zeta_{\frac{\alpha}{2}})$ is a $100(1-\alpha)\%$
    , L2]
    [$\sigma^2$ is unknown $\implies\ \ \ (\bar{X}_n \pm \zeta_{\frac{\alpha}{2}})$ is a $100(1-\alpha)\%$
    , L2]
  ]
\end{forest}
To be more precise, these confidence intervals are \underline{approximate} $\ 100(1-\alpha)\%\ $ confidence intervals for $\mu$ when $n$ is large enough.\\
So far we focused on $C.I$ for population mean. How can we make $C.I$ for other estimates?\\
A common, perhaps the most common, method of estimation that we will learn about in Chapter 9 is \underline{the method of maximum likelihood}. Suppose $\Theta$ is a parameter of interest. Suppose $\hat{\Theta}_n = \hat{\Theta}(X_1,\ldots,X_n)$ is the maximum likelihood estimate $(MLE)$ of $\Theta$ based on $X_1,\ldots,X_n$. Then relatively several condition we have:
$$
	\frac{\hat{\Theta}_n - \Theta}{\sqrt{Var(\hat{\Theta}_n)}} \distas{app} N(0,1)\ \ \text{when $n$ is large enough
}\tab(*)
$$
We therefore have a several recipe for confidence interval when the sample size $n$ is large enough, namely:
$$
	\hat{\Theta}_n \pm \zeta_{\frac{\alpha}{2}}\sqrt{Var(\hat{\Theta}_n)}\tab[2cm] (\dagger)
$$ 
that is a $\ 100(1-\alpha)\%\ C.I$ for $Q$.
\begin{example}
$X_i \distas{iid} N(\mu, \overbrace{\sigma^2}^{known})\ ,\ i=1,2\ldots,n$ .
\end{example}
We show in chapter 9 that $\bar{X}_n$ in the $MLE$ of $\mu$. Note that \mbox{$Var(\bar{X}_n) = \frac{\sigma^2}{n}$}. Then using $(\dagger)$ :
$$
	\bar{X}_n \pm \zeta_{\frac{\alpha}{2}} \sqrt{\frac{\sigma^2}{n}}\ \ \ \ \text{is a $100(1-\alpha)\%$ C.I for $\mu$}
$$

\begin{example}
$X_i \distas{iid} Bernoulli(p)\ \ \forall\ i=1,2,\ldots,n$ , i.e: 

$X_i = \left\{
  \begin{array}{lr}
    1 & \tab p\\
    0 & \tab 1-p
  \end{array}
\right.$
\end{example}
Then $\hat{p}_n = \frac{1}{n}\sum_{i=1}^n x_i$ is the $MLE$ of $p$. Thus using $(\dagger)$ :
$$
	\hat{p}_n \pm \zeta_{\frac{\alpha}{2}\sqrt{Var(\hat{p}_n)}}\ \ \text{ is a $100(1-\alpha)\%\ C.I$ for $p$ .}
$$

Note that $Var(\hat{p}_n) = \frac{p(1-p)}{n}$ . We have two choices:

\begin{enumerate}
\item replace $p$ by $\hat{p}_n$ in $Var(\hat{p}_n)$ :
	$$\hat{p}_n \zeta_{\frac{\alpha}{2}} \frac{\sqrt{\hat{p}_n(1-\hat{p}_n)}}{\sqrt{n}}$$
\item replace $p(1-p)$ in $Var(\hat{p}_n)$ by $\frac{1}{4}$ to find a conservatively large $C.I$ for $p$ :
	$$	\hat{p}_n \pm \zeta_{\frac{\alpha}{2}} \frac{1}{2\sqrt{n}}	$$
\end{enumerate}

\begin{example}
Suppose $X_i\distas{iid} Ber(p)\ ,\ i=1,2\ldots,n$ and we are interested in $\Theta = p(1-p)$ , the variance.
\end{example}
An interesting property of $MLE$ is the invariance , i.e. if $\hat{\Theta}_n$ if the $MLE$ of $\Theta$ , then $h(\hat{\Theta}_n)$ is the $MLE$ of $h(\Theta)$. The invariance property then implies that: \mbox{$\hat{\Theta}_n = \hat{p}_n(1-\hat{p}_n)$} is the $MLE$ of $p(1-p)=\Theta$ .\\
The $ 100(1-\alpha)\%\ $ $C.I\ $ for $\Theta=p(1-p)$\ \ \ \ is \ \ 
\mbox{$\ \hat{\Theta}_n \pm \zeta_{\frac{\alpha}{2}} \sqrt{Var(\hat{\Theta}_n)}$}
\subsection{Small Sample Confidence Intervals}
Unlike the large sample case, there is no general recipe like $({*})$ using which we can find an approximate pivotal quantity. In fact, there is on the paper, but only gives \#MISSING in special cases.\\
To summarize , small sample probabilities are solved mostly case by case. A case of particular importance is the \emph{normal case} . We will learn about the importance of this case when we discuss regression and ANOVA (Analysis of Variance) .\\ \\
\textbf{Normal Case:} \\
Suppose $X_i \distas N(\overbrace{\mu}^{\text{of interest}}, \underbrace{\sigma^2}_{nuisance})\ ,\ i=1,2,\ldots,n$ where n, the sample size is \emph{NOT} large.\\
We learned that when $X_i \distas{iid} N(\mu,\sigma^2)\ ,\ i=1,2,\ldots,n$ :
$$\frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}} \distas{Exact} N(0,1)\tab[2cm] (\ddagger)$$ 
This by itself is not useful since $\sigma$ is \underline{not} known. We discussed in previous section at length why we can replace $\sigma$ by $S$ when n is large enough. The formal justification is \textbf{not} applicable now since $n$ is small, the intuitive justification still stands though.\\
Replacing $\sigma$ with $(\ddagger)$ changes the picture a bit. Given that $S$ has the same spirit as $\sigma$ , though in a small \#MISSING the distribution of \mbox{$T = \frac{\bar{X}_n - \mu}{\frac{s}{\sqrt{n}}}$} still has a bell curve shape. The tails of the distribution, however, die out much more slowly than those of normal distribution. Heavier tails mean much more \underline{variability} and this should perhaps be expected since by replacing $\sigma$ by $S$ which can be crude estimate when $n$ is small, can add suite a hit to the variability. This is, of course, a intuitive argument. Following we present the sketch of a formal argument:
\begin{enumerate}
\item[step 1)] $X_i \distas{iid} N(\mu, \sigma^2) \implies \bar{X}_n\distas{} N(\mu,\frac{\sigma^2}{n})$\\
	$\implies \frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}}\distas{} 		N(0,1)$
\item[step 2)]
	$X_i \distas{iid} N(\mu,\sigma^2) \implies \frac{(n-1)S^2}{\sigma^2} \sim \mathcal{X}^2_{(n-1)}$\\
	\textbf{proof}
	\begin{equation*}
	\begin{split}
	\sum_{i=1}^n(X_i - \mu)^2 &= \sum_{i=1}^n \big[(X_i - \bar{X}_n)+(\bar{X}_n - \mu)\big]^2\\
		&= \sum_{i=1}^n (X_i - \bar{X}_n)^2 + n(\bar{X}_n - \mu)^2 + 2(\bar{X}_n - \mu)\overbrace{\sum_{i=1}^n (X_i - \bar{X}_n)}^0 \\
		& = (n-1)S^2 + n(\bar{X}_n - \mu)^2 \\
		\text{by dividing both sides by $\sigma^2$ we obtain:}\\
		\underbrace{\sum_{i=1}^n (\frac{X_i - \mu}{\sigma})^2}_W &= \underbrace{\frac{(n-1)S^2}{\sigma^2}}_{U} + \underbrace{(\frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}})^2}_{V}\\
		Now note that: \\
		X_i \distas{iid} N(\mu,\sigma^2) &\implies \frac{X_i - \mu}{\sigma} \sim N(0,1)\\
		&\implies (\frac{X_i - \mu}{\sigma})^2 \sim \mathcal{X}^2_1\\
		&\implies \sum_{i=1}^n (\frac{X_i - \mu}{\sigma})^2 \sim \mathcal{X}^2_n\\
		\text{(Exercise: Theorem 7.2 , page 356)}
	\end{split}
	\end{equation*}
	Thus $W \sim \mathcal{X}^2_n$ . On the other hand, using step 1:
	$$(\frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}})^2 \sim \mathcal{X}^2_1$$
\item[step 3)]
	\tab[0.5cm]If \tab[0.5cm]$X_i \distas{iid} N(\mu, \sigma^2)$			\tab[0.5cm]then\tab[0.5cm]$\bar{X}_n \coprod S^2$
\item[step 4)]
	\begin{equation*}
	\begin{split}
		m_{_W}(t) & = \mathbb{E}{e^{tW}}\\
			&= \mathbb{E}[e^{t(U+V)}]\\
			&= \mathbb{E}[e^{tU}\cdot e^{tV}]\\
			&= \mathbb{E}[e^{tU}]\cdot \mathbb{E}[e^{tV}]\\
			&= m_{_U}(t) + m_{_V}(t)\tab[2cm] U\coprod V\ \text{ using step 3}\\
			\text{Thus\tab  $m_{_U}(t)$} &= \frac{m_{_W}(t)}{m_{_V}(t)}\\
			&=\frac{(1-2t)^{-\frac{n}{2}}}{(1-2t)^{-\frac{1}{2}}}\\
			&=(1-2t)^{-\frac{n-1}{2}}\\
	\text{which implies that\tab$U\sim\mathcal{X}^2_{(n-1)}$}
	\end{split}
	\end{equation*}
\item[step 5)]
	If $Z\sim N(0,1)\ ,\ U\sim\mathcal{X}^2_{V}$ and $Z\coprod U$ \, then:
	$$	\frac{Z}{\sqrt{\frac{U}{V}}} \sim T_{n-1} \tab[4cm] \text{(Exercise 7.30 , page 367)}	$$
\item[step 6)]
	$$
	T_{n-1} = \frac{\bar{X}_n - \mu}{\frac{S}{\sqrt{n}}} = \frac{(\frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}})}{\sqrt{\frac{(\frac{(n-1)S^2}{\sigma^2})}{(n-1)}}}	= \frac{Z}{\sqrt{\frac{U}{V}}}
	$$
	The pdf of $T_{v}$ is :
	\begin{equation*}
	\begin{split}
		f_{_{T_v}}(t) = \frac{\Gamma{\frac{v+1}{2}}}{\Gamma(\frac{v}{2})\sqrt{v\pi}}(1+\frac{t^2}{v})^{-\frac{v+1}{2}}\tab -\infty<t<+\infty 
	\end{split}
	\end{equation*}
	\#MISSING GRAPH Lecture 5 - page 13\\
	
	$\mathbb{E}[T_{v}^r] = \left\{
  \begin{array}{lr}
    0 & \tab \text{if $r<v$ and $r$ is odd}\\
    v^{\frac{r}{2}}\cdot\frac{\Gamma(\frac{1+r}{2})\Gamma(\frac{v-r}{2})}{\Gamma(\frac{1}{2})\Gamma(\frac{v}{2})} & \tab \text{if $r<v$ and $r$ is even}
  \end{array}
\right.$\\ \\
	Thus, if $X_i\distas{iid}N(\mu,\sigma^2)\ ,\ i=1,2,\ldots,n\ $ and $\mu$ and $\sigma^2$ are both \underline{unknown} :
	$$\bar{X}_n \pm t_{(n-1),\frac{\alpha}{2}} \frac{S}{\sqrt{n}}$$ 
	provides a $ 100(1-\alpha)\%\ $ $C.I$ for $\mu$ where $P(T_{(n-1)} > t_{(n-1), \frac{\alpha}{2}}) = \frac{\alpha}{2}$ 
	
\subsection{Pivotal Quantity and Probability Integral Transform}
Suppose $X$ is a continuous random variable with $p.d.f$ $f$ and $cdf$ $F$. Then $F(X)\sim Uniform(0,1) \tab \text{(Exercise)}$\\
This result is referred to as the \textbf{Probability Integral Transform}. Now suppose $X_i \distas{iid} F$ . Then :
\begin{equation*}
\begin{split}
	F(X_i) \sim Unif(0,1) &\implies -2\ln{F(X_i) \sim \mathcal{X}^2_2}\\
	&\implies -2\sum_{i=1}^n \ln{F(X_i)}\sim \mathcal{X}^2_{2n}\\
	&\implies -2\sum_{i=1}^n \ln{[1-F(X_i)]} \sim \mathcal{X}^2_{2n}
\end{split}
\end{equation*}

There is hence a general recipe for finding a pivotal quantity when we have samples from continuous random variables. The usefulness of this pivotal quantity \emph{depends} on the form of $F$ , the $cdf$ of $X$ .\\
Suppose $X_i \distas{iid}exp(\lambda)\ ,\ i=1,2,\ldots,n\ ,\ $ i.e:

$$
	f_{_X}(x) = \left\{
  	\begin{array}{lr}
   	 	\lambda e^{-\lambda x} & \tab x>0\\
   	 	0 & \tab o/w
  	\end{array}
	\right.
$$

Then:
$$
	F(x) = \int_{0}^x f(t)dt = 1-e^{-\lambda x}\tab[0.5cm] ,\tab[0.5cm] x>0
$$
and:
$$
	F(x) = \left\{
  	\begin{array}{lr}
   	 	1-e^{-\lambda x} & \tab x>0\\
   	 	0 & \tab o/w
  	\end{array}
	\right.
$$
Using the above discussion:
$$
	-2\sum_{i=1}^n \ln{F(X_i)} \sim \mathcal{X}^2_{2n}\tab\text{and}\tab -2\sum_{i=1}^n \ln{[1-F(X_i)]} \sim \mathcal{X}^2_{2n}
$$
for this example it is easier to work with the latter, i.e. :
\begin{equation*}
\begin{split}
\-2\sum_{i=1}^n \ln{[1-F(X_i)]} &= -2\sum_{i=1}^n \ln{(e^{-\lambda X_i})}\\
	&= 2\lambda\sum_{i=1}^n X_i = 2 n \lambda \bar{X}_n \\
	so \implies &2 n \lambda \bar{X}_n \sim \mathcal{X}^2_{2n}
\end{split}
\end{equation*}
Using the $\mathcal{X}^2$ table (Application 3, page 850-851) , we can find \mbox{$\mathcal{X}^2_{(2n),0.025}$} and \mbox{$\mathcal{X}^2_{(2n),0.975}$} such that:
$$
P(\mathcal{X}^2_{(2n),0.975} < 2 n \lambda \bar{X}_n < \mathcal{X}^2_{(2n),0.025}) = 0.95
$$
Thus:
$$
	\big(\frac{\mathcal{X}^2_{(2n),0.975}}{2 n \bar{X}_n} , \frac{\mathcal{X}^2_{(2n),0.025}}{2 n \bar{X}_n}	\big)
$$
provides that a $95\%$ $C.I$ for $\lambda$ . Note that $\mathcal{X}^2_{(2n),\alpha}$ is such that \mbox{$P(\mathcal{X}^2_{(2n)} > \mathcal{X}^2_{(2n),\alpha}) = \alpha$}

\#MISSING GRAPH LECTURE 5 - PAGE 15
		
\end{enumerate} 






\newpage
\section{Lecture 6}
\subsection{Small Sample Confidence Interval(general case):}
We learned in the last lecture how to find $C.I.$ fir th population mean when the population distribution is normal. The two main pivotal quantities are:
$$
	(a)\ \ \ \ \frac{(n-1)S^2_n}{\sigma^2}\sim \chi^2_{(n-1)}\tab \&\tab (b)\ \ \ \ \frac{\bar{X}_n = \mu}{\frac{S}{\sqrt{n}}}\sim T_{(n-1)}
$$ 
when $X_i\distas{iid} N(\mu, \sigma^2)$ , $\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$ and $S^2_n = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X}_n)^2$ . \\
The first result can be used to make a $C.I$ for $\sigma^2$ and $\sigma$ which the latter is used for making a $C.I$ for $\mu$ .\\
We now consider the general case. 
\subsection{Probability Integral Transform(PIT)}Suppose $X_i \distas{iid} F_{_X}$ and $f$ is the pdf of $X_i$s:
\begin{equation*}
\begin{split}
X\sim F_X\ , Y = F_X(X) \\
F_Y(t) &= P(Y\leq t) = P(F_X(x)\leq t)\\
&=P(X\leq F_X^{-1}(t))\\
&=F_X(F_X^{-1}(t)) = t\tab \text{for } 0\leq 1\leq 1\\
\text{Thus \ \ } F_Y(t)= &\left\{
  	\begin{array}{lr}
   	 	0 & \tab \text{if } t<0 \\
   	 	t & \tab \text{if } 0\leq t <1 \\
   	 	1 & \tab \text{if } 1\leq t
  	\end{array}
	\right.
\end{split}
\end{equation*}
and hence $Y \sim Unif(0.1)$. This is called \textbf{Probability Integral Transform(PIT)}.
\begin{example}
$X\sim Exp(\lambda)$ , 
$f_X(x) = \left\{ \begin{array}{lr} \lambda e^{- \lambda x} &\tab x > 0\\ 0 &\tab x\leq 0
\end{array}
\right.$ , $\lambda > 0$.
\end{example}
\begin{equation*}
\begin{split}
F_{_X}(x) &= P(X\leq x) = \int_{-\infty}^x f_{_X}(t) dt = \int_0^x \lambda e^{-\lambda t} dt\\
	&= -e^{-\lambda t}\Big|_0^x\\
	&= 1-e^{-\lambda x}\tab[4cm] (1)\\
\text{Now consider \ \ } Y &= F_{_X}(x) = 1-e^{-\lambda x} :\\
F_{_Y}(t) &= P( Y \leq t) = P(1-e^{-\lambda x} \leq t)\\
	&=P(e^{-\lambda x} \geq 1-t) = P(X \leq -\frac{ln(1-t)}{\lambda})\\
	&=F_{_X}(-\frac{ln(1-t)}{\lambda})\\
	&=1-e^{-\lambda(-\frac{ln(1-t)}{\lambda})} \tab[3cm] \text{using } (1)\\
	&=1-e^{ln(1-t)} = 1-(1-t)\\
	&=t
\end{split}
\end{equation*}
Thus $Y \sim Unif(0,1)$.\\
\begin{remark*}\emph{
Using $PIT$ we can essentially generate random numbers from any continuous distributions. In fact, suppose we want samples from cdf F. Then:\\
\tab \textbf{Step 1:} Generate $U_i \distas{iid} \text{Unif}(0,1)\ \ ,\ \ i=1,2,\ldots,n$.\\
\tab \textbf{Step 2:} $X_i = F^{-1}(U_i)\distas{iid} F \ \ ,\ \ i=1,2,\ldots,n$
}
\end{remark*}
This algorithm then works as long as we can generate uniform random numbers and $F^{-1}$ can be explicitly found or well approximated.\\
	\begin{example*}
		$f_{_X}(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \ \ ,\ \ -\infty < x < +\infty \ \ \ (X\sim N(0,1))$ 
	\end{example*}
$$
\text{Then: \ \ } F_{_X}(x) = \int_{-\infty}^x f_{_X}(t)dt = \int_{-\infty}^x \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}dt\ \ \ .
$$
In this case, $F^{-1}$ does not have an explicit nice form, but it can be well approximated.\\
\textbf{Remark. } A simple and useful transformation:\\
$$
	X\sim F \implies \overbrace{Y = F(X) \sim Unif(0,1)}^{P.I.T} \implies -2\cdot log(Y) \sim \chi^2_2
$$
\subsection{Pivotal Quantity}
Suppose $X_i \distas{iid} F\ \ , \ \ i=1,2,\ldots,n$ . Define:
$$
Y_i = F(X_i) \distas{iid} Unif(0,1) \ \ ,\ \ i=1,2,\ldots,n
$$
Now consider:
$$
	V_i = -2\cdot log(Y_i) \distas{iid} \chi^2_{2n} \ \ , \ \ i=1,2,\ldots,n
$$
Then:
$$
	\sum_{i=1}^n V_i \sim \chi^2_{2n} \tab .
$$
Having established the first two results, i.e. :\\
\tab \textbf{Step 1: } \mbox{$X_i \sim F \implies Y_i = F(X_i) \sim Unif(0,1)\ \ (PIT)$}\\
\tab \textbf{Step 2: } $V_i = -2\cdot log(Y_i) \sim chi^2_2\ \ \ \text{(method of transformation)}$\\
The last result can be established using the method of moments:\\
\begin{equation*}
\begin{split}
m_{_{\sum_{i=1}^n V_i}}(t) &= \mathbb{E}[e^{-t\sum_{i=1}^n V_i}] = \mathbb{E}\Big[\prod_{i=1}^n e^{-t V_i}\Big]\\
\coprod_{i=1}^n V_i \implies\tab &= \prod_{i=1}^n \mathbb{E}[e^{-t V_i}] = \prod_{i=1}^n m_{_{V_i}}(t)\\
\text{identically distributed} \implies\tab &=[m_{_Y}(t)]^n = [(1-2t)^{-\frac{2}{2}}]^n\\
&=(1-2t)^{-\frac{2n}{2}} \implies \sum_{i=1}^n V_i \sim \chi^2_{2n}
\end{split}
\end{equation*}
Then a pivotal quantity based on $X_i \distas{iid} F_{\theta}\ \ , \ \ i=1,2,\ldots,n \ $\ is:
$$
	-2\sum_{i=1}^n log(F_{\theta}(X_i)) \sim chi^2_{2n}\tab (1)
$$
\begin{example*}
 $X_i \distas{iid} Exp(\lambda)\ \ \ , \ \  f_{_X}(x) = \left\{
  	\begin{array}{lr}
   	 	\lambda e^{-\lambda x} & \tab x\geq 0\\
   	 	0 & \tab x<0
  	\end{array}
	\right.
 	$
\end{example*}
$$F_{_X}(x)= \int_{-\infty}^x f_{_X}(t)dt = \int_0^x \lambda e^{-\lambda t} dt = 1 -e^{-\lambda x}$$
Now we notice that $-2\cdot log(F) = -2 \cdot log(1-e^{-\lambda x})$ does not provide an useful form for the purpose of making a $C.I$ for $\lambda$. There is a dual to $(1)$ that is useful in this case, however:
$$
	\sum_{i=1}^n W_i = \sum_{i=1}^n -2 log(1-F(X_i)) \sim \chi^2_{2n}\tab (2)
$$
This quickly follows from the fact that:
$$
	U \sim Unif(0,1) \implies 1-U \sim Unif(0,1)\tab .
$$
Using $(2)$ we have:
\begin{equation*}
\begin{split}
\sum_{i=1}^n -2 log(1-F(X_i)) &= \sum_{i=1}^n -2 log(e^{-\lambda X_i})\\
	= 2\lambda \sum_{i=1}^n X_i &= 2\lambda n \bar{X_n} \sim \chi^2_{2n}\\
	\text{Using the $\chi^2$-table (App.3 m page 850-851) , we find \mbox{$\chi^2_{2n, 0.025}$} and \mbox{$\chi^2_{2n,0.975}$} such that:}\\
	P(\chi^2_{2n,0.975} < 2\lambda n\bar{X_n} < \chi^2_{2n,0.025} ) = 0.95\\
	\text{and hence:}\tab[6cm]\\
	P(\frac{\chi^2_{2n, 0.975}}{2n\bar{X_n}} < \lambda < \frac{\chi^2_{2n,0.025}}{2 n \bar{X_n}}) = 0.95
\end{split}
\end{equation*}
Thus:
$$(\frac{\chi^2_{2n , 0.975}}{2n\bar{X_n}} , \frac{\chi^2_{2n,0.025}}{2n\bar{X_n}})\tab \text{is a \underline{$95\%$} confidence interval for $\lambda$}
$$
\#MISSING Graph Lecture 6 - page 36
\subsection{Small Size Determination}
Suppose we want to estimate the proportion of Canadian voters who are in favor of NDP and want our estimate to be one-percentage point from the actual population with $95\%$ confidence. Define:
$$
X = \left\{
	\begin{array}{lr}
	1 & \tab \text{NDP} \\
	0 & \tab \text{other parties}
	\end{array}
\tab\text{associated to each potential voter.}
\right.$$
We learned that to estimate the proportion of interest $p=P(X=1)$ , we can use \mbox{$\hat{P}_n = \frac{1}{n}\sum_{i=1}^n X_i$} from a random sample of size $n$. We further learned that if the sample size $n$ is large enough, then:
$$
	\hat{p}_n \pm 1.96 \sqrt{\frac{p(1-p)}{n}}
$$
is a $95\%$ confidence interval for $p$. Thus the margin of error is $\beta = 1.96\sqrt{\frac{p(1-p)}{n}}$ which is controlled by $n$ , the sample size. We should therefore choose $n$ such that:
$$
	0.01 = 1.96 \sqrt{\frac{p(1-p)}{n}}
$$
Given that $p$ is unknown, we can either replace $p$ by $\hat{p}_n$ or take a conservative approach and replace $p$ by $\frac{1}{2}$ which maximizes $p(1-p)$ . Thus we find:
$$
n = \frac{p(1-p)\zeta^2_{\frac{\alpha}{2}}}{\beta^2} = \left\{
	\begin{array}{lr}
	\frac{\hat{p}_n(1-\hat{p}_n)\zeta^2_{\frac{\alpha}{2}}}{\beta^2} &\tab \text{replacing $p$ by $\hat{p}_n$} \\
	\frac{\zeta^2_{\frac{\alpha}{2}}}{4\beta^2} & \tab\text{replacing $p$ by $\frac{1}{2}$}
	\end{array}
	\right.
$$
Taking the conservative approach, we have:
$$
	n = \frac{\zeta^2_{\frac{\alpha}{2}}}{4\beta^2}  = \frac{(1.96)^2}{4(0.01)^2} = 9604
$$
Likewise we can find the sample size formula for estimating the population mean with a given confidence $1-\alpha$ and margin of error $\beta$ , we should in fact solve the following equation for $n$: 
$$
	\beta = \zeta^2_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}\tab\text{where $\sigma^2$ is the population variance.}
$$
We then find $\boxed{n = \frac{\zeta^2_{\frac{\alpha}{2}}\sigma^2}{\beta^2}}$ where $\sigma^2$ should be estimated from a prior sample.\\
\subsection{Sample Size Determination For Other Parameters}
So far we only considered the population mean. Now consider a parameter $\theta$ . In chapter $9$ we learn about different methods of estimation, among them there is a method called the method of maximum likelihood (ML). Suppose $\hat{\theta}_n$ is the maximum likelihood estimate (MLE) of $\theta$. Then under some reasonable conditions for a considerably large class of parametric distributions, we have:
$$
	\frac{\hat{\theta}_n - \theta}{\sqrt{Var(\hat{\theta}_n)}} \distas{app} N(0,1) \tab ,\ \text{for large n}
$$
Thus:
$$
	\hat{\theta}\pm \underbrace{\zeta_{\frac{\alpha}{2}}\sqrt{Var(\hat{\theta}_n)}}_\beta \tab\text{ is a $100(1-\alpha)\%\ C.I$ for $\theta$}
$$
Let $\beta = \zeta_{\frac{\alpha}{2}}\sqrt{Var(\hat{\theta}_n)}$ . In many interesting cases, $Var(\hat{\theta}_n)$ is an explicit function of $n$ and $\sigma^2$ , the variance in the target population , say $h(\sigma^2,n)$ . Then the sample size can be determined by the solution of the following equation:
$$
	h(\sigma^2,n) = \frac{\beta^2}{\zeta^2_{\frac{\alpha}{2}}}
$$
Recall that for Bernoulli case, i.e. $X_i = \left\{ \begin{array}{lr}
	1 \\ 0 
\end{array}
\right.  \ \ \ ,\tab i=1,2,\ldots,n$\ :
$$
	h(\sigma^2, n) = Var(\hat{\theta}_n) = Var(\hat{p}_n) = \frac{\overbrace{p(1-p)}^{\sigma^2}}{n}
$$
while for estimating the population mean:
$$
	h(\sigma^2,n) = Var(\hat{\theta}_n) = Var(\bar{X}_n) = \frac{\sigma^2}{n} \ \ .
$$
\subsubsection{Sample Size Determination (Small Sample)}
\begin{enumerate}
\item[•]Normal Case:
	We learned that if $X_i \distas{iid} N(\mu, \sigma^2)\ , \ i=1,2,\ldots,n$ :\\
	$$
		\frac{\bar{X}_n - \mu}{\frac{S}{\sqrt{n}}} \sim T_{n-1} \implies \bar{X}_n \pm t_{\frac{\alpha}{2},(n-1)} \frac{S}{\sqrt{n}}\tab \text{is a $100(1-\alpha)\% \ C.I$ for $\mu$}
	$$
	We can therefore find sample size from the following equation:
	$$
		\beta = t_{\frac{\alpha}{2},(n-1)} \frac{S}{\sqrt{n}} \implies \boxed{n = \frac{S^2 t^2_{\frac{\alpha}{2},(n-1)}}{\beta^2}}
	$$
	Now note that the sample size determination based on large sample and small sample in the normal case had to $N(0,1)$ and $T_{(n-1)}$ respectively. These distributions are both symmetric. As such in the sample size determination we only deal with the half length of the confidence intervals when the sample size is small and the population from which the samples are taken is not normal, the pivotal quantities do not necessarily have asymmetric distribution and hence the confidence interval do not have the form of \mbox{$\boxed{\hat{\theta}_n \pm \beta}$}.\\
	In such cases we try to control the total length of the confidence interval.
	\begin{example*}
		$X_i \distas{iid} Exp(\lambda) \ \ , \ \ i=1,2,\ldots,n$
	\end{example*}
	We found that $(\frac{\chi^2_{2n,0.975}}{2n\bar{X}_n} , \frac{\chi^2_{2n,0.025}}{2n\bar{X}_n})$ ; let $C$ be the desired length for $C.I$ for $\lambda$ . Then:
	$$
		C = \frac{\chi^2_{2n , \frac{\alpha}{x}} - \chi^2_{2n, 1-\frac{\alpha}{2}}}{2n\bar{X}_n}
	$$
	represents the length of a $C.I$ for $\lambda$ based on a sample of size $n$ with \mbox{$100(1-\alpha)\%$} confidence.	
\end{enumerate}
\subsubsection{Sample Size Determination(Two Sample Case)}
So far we just confirmed ourselves to one population. We might, however, have two samples, \mbox{$X_1,X_2,\ldots,X_m$} from the population of men with population mean $\mu_{_M}$ , and \mbox{$Y_1,Y_2,\ldots,Y_n$} from the population of women with population mean $\mu_{_W}$ . Suppose the parameter of interest is \mbox{$\theta = \mu_{_M} - \mu_{_W}$} . Then the natural estimate of $\theta$ is \mbox{$\hat{\theta} = \bar{X}_m - \bar{Y}_n$} and using the central limit theorem:
$$
	\frac{(\bar{X}_m - \bar{Y}_n) - (\mu_{_M} - \mu_{_W})}{\sqrt{Var(\bar{X}_m - \bar{Y}_n)}} \distas{app} N(0,1)\tab\text{for large $m \And n$}
$$
Now:
$$
	Var(\bar{X}_m - \bar{Y}_n) = Var(\bar{X}_m) + Var(\bar{Y}_n) -2 Cov(\bar{X}_m , \bar{Y}_n)
$$
Assuming that $X$s and $Y$s are independent \tab(i.e $Cov(\bar{X}_m , \bar{Y}_n) = 0$)
:
$$
Var(\bar{X}_m - \bar{Y}_n) = Var(\bar{X}_m) + Var(\bar{Y}_n) = \frac{\sigma^2_{_M}}{m} + \frac{\sigma^2_{_W}}{n}
$$
Therefore:
$$
	(\bar{X}_m - \bar{Y}_n) \pm \zeta_{\frac{\alpha}{2}}\sqrt{\frac{\sigma^2_{_M}}{m} + \frac{\sigma^2_{_W}}{n}}\tab\text{is a $100(1-\alpha)\%\ \ C.I$ for $\mu_{_M} - \mu_{_W}$ .}
$$
To find the sample size we should solve:
$$
	\beta = \zeta_{\frac{\alpha}{2}} \sqrt{\frac{\sigma^2_{_M}}{m} + \frac{\sigma^2_{_W}}{n}}
$$
We should assume that $\sigma^2_{_M} \And \sigma^2_{_W}$ are known or estimated from prior samples, we will have one equation with two unknowns, $m \And n$. In order to have a unique solution we need another equation. We often consider $n = K\cdot m$ , where $K$ is a known value as the second equation. Suppose $C_{_M}\And C_{_W}$ represent respectively, the cost of taking a sample from population of men and women. Then \mbox{$K \propto(\frac{C_{_W}}{C_{_M}})^{-1}$}. In case that $C_{_W} = C_{_M}$ , we choose $K=1$. Now:
$$
\left\{\begin{array}{lr}
\beta = \zeta_{\frac{\alpha}{2}} \sqrt{\frac{\sigma^2_{_M}}{m} + \frac{\sigma^2_{_W}}{n}} \\
n = Km
\end{array}
\right.
$$
Solving the above system we find:
$$
\boxed{m = (\frac{\zeta_{\frac{\alpha}{2}}}{\beta})^2 \cdot (\sigma^2_{_M} + \frac{\sigma^2_{_W}}{K})}
$$
For proportions: \tab$\sigma^2_{_M} = p_{_M} (1-p_{_M}) \tab\And\tab \sigma^2_{_W}= p_{_W}(1-p_{_W})$
Taking the conservative approach and replacing both $p_{_M} \And p_{_W}$ by $\frac{1}{2}$ we find:
$$
\boxed{m = \big(\frac{\zeta_{\frac{\alpha}{2}}}{2\beta}\big)^2 (1+\frac{1}{K})}
$$





\newpage
\section{Lecture 7}
\subsection{Chapter 9 - Relative Efficiency}
\begin{definition*}\emph{
The relative efficiency of two unbiased estimators, $\hat{\theta}_1$ and $\hat{\theta}_2$ , is defined to be:
$$
	eff(\hat{\theta}_1 , \hat{\theta}_2) = \frac{Var(\hat{\theta}_1)}{Var(\hat{\theta}_2)}
$$
}\end{definition*}
We learned that between the two unbiased estimators the one with smaller variance is closer to the target on the average, i.e. has smaller $MSE$.\\
We also learned that the length of confidence intervals for large sample size is controlled by the variance of the estimator; so, the smaller the variance, the shorter the confidence interval using that estimator is. \\
We now want to quantify the gain in using the estimator with smaller variance.
\begin{example*}
Suppose $X_i \distas{iid} f\ \ , \ \ i=1,2,\ldots,n\ $ where $f$ is a symmetric $pdf$.
\end{example*}
The mean and median of  $f$ are the same, say $\mu$ . Given that $f$ is symmetric, we can use: \mbox{$\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i\ $} (the sample average) , or:
$$
	M_n = \left\{
		\begin{array}{lr}
		X_{_\frac{n+1}{2}} & \tab \text{if n is odd} \\
		\frac{1}{2}[X_{\frac{n}{2}} + X_{\frac{n}{2} + 1}] & \tab \text{if n is even} 
		\end{array}
		\right.\tab \text{where } X_{(1)} < X_{(2)} < \cdots < X_{(n)}\ \text{are the order statistics.}
$$
We learned that $Var(\hat{X}_n) = \frac{\sigma^2}{n}$ where $\sigma^2$ is the population variance, i.e. :
$$
	\sigma^2 = \int_{-\infty}^{+\infty} (x-\mu)^2 f(x)dx
$$
It can be shown (beyond the scope of this course), that:
$$
	Var(M_n) \approx \frac{1}{4\cdot[f(\mu)]^2 n}\tab \text{for large }n
$$
For instance if $f$ is Normal, i.e. \mbox{$X_i \distas{iid}N(\mu,\sigma^2)$} , then \mbox{$f(\mu)=\frac{1}{\sigma\sqrt{2\pi}}$} and hence:
$$
	Var(M_n) = \frac{2\pi}{4}\cdot \frac{\sigma^2}{n}
$$
Thus:
$$
	eff(\bar{X}_n,M_n) = \frac{Var(M_n)}{Var(\bar{X}_n)} = \frac{\frac{2\pi}{4}\cdot \frac{\sigma^2}{n}}{\frac{\sigma^2}{n}} = \frac{2\pi}{4} = 1.57
$$
This then essentially means that if you can make a confidence interval of a given length using $\bar{X}_n$ with $100$ observations, to make a confidence interval of the same length for $\mu$ using $M_n$ , you need $100\times 1.57 = 157$ observations.
\begin{example*}[9.1 , page 446]
	$Y_i \distas{iid} Unif(0,\theta)$\tab, $i=1,2,\ldots,n$ \ ,\ $\theta >0\ $ and $\theta$ is unknown.
\end{example*}
Consider $\ \hat{\theta}_1 = 2\bar{Y}_n\ $ and $\ \hat{\theta}_2 = (\frac{n+1}{n})Y(n)\ $ where $\ Y(n) = \underset{1\leq i\leq n}{max} Y_i\ $
\begin{enumerate}
\item[For $\mathbf{\hat{\theta}_1}$ :]\hfill
	\begin{enumerate}
	\item[$\mathbb{E}(\hat{\theta}_1)$] 
		$=\mathbb{E}(2\bar{Y}_n) = 2\mathbb{E}(\bar{Y}_n)$\\
		$=2 \mathbb{E}(\frac{1}{n}\sum_{i=1}^n Y_i) = \frac{2}{n}\sum_{i=1}^n \mathbb{E}(Y_i)$\\
		$=\frac{2}{n} \sum_{i=1}^n \frac{\theta}{2} = \frac{\cancel{2}}{\cancel{n}}\cdot \cancel{n}\frac{\theta}{\cancel{2}}$\\
		$=\theta$
	\item[$\ Var(\hat{\theta}_1)$]
		$=Var(2\bar{Y}_n) = 4 Var(\bar{Y}_n)$\\
		$=4 \cdot \frac{Var(Y)}{n} = 4 \cdot \frac{\sigma^2_Y}{n}$\\
		$\sigma^2_Y = Var(Y) = \mathbb{E}(Y^2) - [\mathbb{E}(Y)]^2$\\
		$\mathbb{E}(Y^2) = \int_{-\infty}^{+\infty} y^2 f_Y(y)dy = \int_{0}^\theta y^2 \cdot \frac{dy}{\theta}$\\
		$\tab = \frac{1}{3\theta} y^3 \big|_{0}^{\theta} = \frac{\theta^3}{3\theta} = \frac{\theta^2}{3}$\\
		$\sigma^2_Y = \mathbb{E}(Y^2) - [\mathbb{E}(Y)]^2 = \frac{\theta^2}{3} - [\frac{\theta}{2}]^2$\\
		$\tab = \frac{\theta^2}{3} - \frac{\theta^2}{4} = \frac{\theta^2}{12}$ ,\tab then:\\
		$Var(\hat{\theta}_1) = \frac{4\sigma^2_Y}{n} = \frac{4\frac{\theta^2}{12}}{n} = \frac{\theta^2}{3n}$
	\end{enumerate}
\item[For $\hat{\theta}_2$:]\hfill
	\begin{enumerate}
	\item[$F_{Y(n)}(t)$] 
		$= P(Y(n) \leq t) = P(Y_1\leq t , Y_2 \leq t, \cdots , Y_m \leq t)\tab \text{by } \coprod_{i=1}^n Y_i :$\\
		$\implies \prod_{i=1}^n P(Y_i \leq t) = \prod_{i=1}^n F_{Y_i}(t)$\\
		$=[F_Y(t)]^n\tab\therefore\ $ identically distributed.\\
		Thus $d_{Y(n)}(t) = \frac{d}{dt} F_{Y(n)}(t) = \frac{d}{dt} F_Y^n(t) = n f_Y(t)F_Y^{n-1}(t)$\\
		$f_{Y(n)}(t) = \left\{ \begin{array}{lr}
		n\frac{1}{\theta} (\frac{t}{\theta})^{n-1} &\tab 0<t<\theta \\
		0 & \tab \text{otherwise}
		\end{array}
		\right.$
	\item[$\mathbb{E}(\hat{\theta}_2)$]
		$= \mathbb{E}[(\frac{n+1}{n})Y_(n)] = (\frac{n+1}{n})\mathbb{E}(Y(n))$\\
		$=(\frac{n+1}{n})\int_0^\theta y \cdot n \cdot \frac{1}{\theta}(\frac{y}{\theta})^{n-1}dy$\\
		$=(\frac{n+1}{n})\cdot \frac{n}{\theta^n}\int_0^\theta y^n dy$\\
		$=(\frac{n+1}{n}) \frac{n}{\theta^n}\big[\frac{1}{n+1}y^{n+1}\big]_0^\theta$\\
		$=(\frac{\cancel{n+1}}{\cancel{n}}) \frac{\cancel{n}}{\cancel{\theta^n}} \frac{\theta^{\cancel{n+1}}}{\cancel{n+1}}$\\ 
		$=\theta$
	\item[$Var(\hat{\theta}_2)$]
		$=Var(\frac{n+1}{n} Y(n)) = \Big(\frac{n+1}{n})^2 Var(Y(n)\Big)$\\
		$Var(Y(n)) = \mathbb{E}(Y^2_{(2)}) - [\mathbb{E}(Y(n))]^2$\\
		$E(Y^2_{(n)}) = \int_0^\theta y^2 \cdot n \cdot \frac{1}{\theta}(\frac{y}{\theta})^{n-1} dy$\\
		$\tab =\frac{n}{\theta^n}\int_0^\theta y^{n+1} dy$\\
		$\tab =\frac{n}{\theta^n} \cdot \frac{1}{n+2} \cdot \theta^{n+2} = \frac{n \theta^2}{n+2}$\\
		$Var(Y(n)) = \frac{n\theta^2}{n+2} - [\frac{n}{n+1}\theta]^2$\\
		$\tab = n\theta^2 \Big(\frac{1}{n+2} - \frac{n}{(n+1)^2}\Big)$\\
		$\tab = \frac{n\theta^2}{(n+2)(n+1)^2}$\tab thus:\\
	$Var(\hat{\theta}_2) = (\frac{n+1}{n})^2 \cdot \frac{n\theta^2}{(n+2)(n+1)^2} = \frac{\theta^2}{n(n+2)}$
	\end{enumerate}
\end{enumerate}
Thus:
$$
	eff(\hat{\theta}_1 , \hat{\theta}_2) = \frac{Var(\hat{\theta}_2)}{Var(\hat{\theta}_1)} = \frac{\frac{\cancel{\theta^2}}{\cancel{n}(n+2)}}{\frac{\cancel{\theta^2}}{3\cancel{n}}} = \frac{3}{n+2} \to 0 \ \ as\ \ n\to\infty 
$$
Note that $\ eff(\hat{\theta}_1, \hat{\theta}_2) < 1\ $ for $\ n \geq 2\ $ . This means that $\hat{\theta}_2$ is more efficient than $\hat{\theta}_1$ for $n\geq 2$ .\\
We also notice that the efficiency gap increases as the sample size $n$ increases and for large values of $n$ , the efficiency tends to \underline{zero}.
\subsection{Consistency}
\begin{definition*}[Consistent Estimator]
We say $\hat{\theta}_n$ is a consistent estimator of $\theta$ if \mbox{$\hat{\theta}_n \xrightarrow{p} \theta$} as \mbox{$n \to \infty$} ; i.e:
$$
	\lim \limits_{n\to \infty}P\big(\vert \hat{\theta}_n - \theta\vert > \epsilon \big) = 0 \ \ \ , \ \ \forall\ \epsilon > 0\tab (\dagger)
$$
\end{definition*}
Consistency essentially means "being right-headed". It essentially says that if we have all the population, our procedure , $\hat{\theta}_n$ , sizes the target.\\
Note that $(\dagger)$ is equivalent to :
$$
\lim \limits_{n\to \infty} P\big(\vert \hat{\theta}_n - \theta \vert leq \epsilon\big) = 1\ \ \ , \ \ \ \forall \epsilon > 0
$$
Now this definition can be compared with the notion of the limit of a sequence of real numbers.
$$
	\lim\limits_{n\to\infty} a_n = a\ \ \ \ iff \ \ \ \forall\epsilon >0 \ \exists\ \ N(\epsilon) \ni \ \vert a_n - a\vert<\epsilon\ \ \ \ if\ \ n\geq N(\epsilon)
$$
Now since that $\hat{\theta}_n$ is a random variable no matter how large $n$ is, there is always a chance that \mbox{$\ \vert \hat{\theta}_n - \theta\vert > \epsilon\ $}. This chance, however, tends to zero as $\ n\to\infty\ $ .
\begin{example*}
	$X_i\distas{iid}\text{Bernoulli}(p) , $\\
	$\tab X_i = \left\{ \begin{array}{lr}1 &\tab p \\ 0 & \tab 1-p  \end{array}\right. \ \ \ ,\tab i=1,2,\ldots,n$
\end{example*}
$\hat{p}_n = \frac{1}{n}\sum_{i=1}^n X_i\ $. We want to show that:
$$
	\lim\limits_{n\to \infty} P\big(\vert \hat{p}_n - p\vert > \epsilon\big) = 0\ ,\ \ \forall \epsilon>0 \ .
$$
Compare $P\big(\vert \hat{p}_n - p\vert > \epsilon\big)$ with Tchbyshev's Inequality:
$$
	P(\vert X - \mathbb{E}(X) \vert > K\overbrace{\sigma}^{\sqrt{Var(X)}}) \leq \frac{1}{K^2}
$$
$X$ is replaced by $\hat{p}_n \ \ ,\ \ \mu_X = \mathbb{E}(X)$ by $p$ and $K\sigma$ by $\epsilon$.\\
Note that $\ \mathbb{E}(\hat{p}_n) = p\ $ so everything is in order for using Tchbyshev's Inequality. Now $\epsilon = K\sigma_X$ implies that \mbox{$K = (\frac{\sigma_X}{\epsilon})^{-1}$} and given that $X$ is replaced by $\hat{p}_n$ , we should have \mbox{$K=(\frac{\sigma_{\hat{p}_n}}{\epsilon})^{-1}$}. Thus:
\begin{equation*}
\begin{split}
	P\big(\vert \hat{p}_n - p\vert > \epsilon\big) &\leq \frac{1}{(\frac{\sigma_{\hat{p}_n}}{\epsilon})^{-2}} = \frac{\sigma_{\hat{p}_n}^2}{\epsilon^2}\\
	P\big(\vert \hat{p}_n - p\vert > \epsilon\big) &\leq \frac{Var(\hat{p}_n)}{\epsilon^2}\\
	\implies Var(\hat{p}_n) = Var(\frac{1}{n}\sum_{i=1}^n X_i) &= \frac{p(1-p)}{n}\\
\text{Therefore\tab} P\big(\vert \hat{p}_n - p\vert > \epsilon\big) &\leq \frac{p(1-p)}{n\epsilon^2} \leq \frac{1}{4 n \epsilon^2} \to 0\ \ \ as\ \ n\to\infty\ \ \ \because\ p(1-p)\leq \frac{1}{4}\tab[2cm] (\ddagger)\\
\text{Thus\tab} \lim\limits_{n\to\infty} P\big(\vert\hat{p}_n - p\vert > \epsilon\big) &= 0 \ \ , \ \ \forall \epsilon>0
\end{split}
\end{equation*}
Note further that we can let $\epsilon$ tend to zero as $\ n\to\infty\ $ , i.e. $\epsilon_n$ depend on $n$ and $\ \epsilon_n\to 0\ $ as $\ n\to\infty\ $. Using $(\ddagger)$ :
$$
	\lim\limits_{n\to\infty} P\big(\vert\hat{p}_n - p\vert > \epsilon_n\big) \leq \lim\limits_{n\to\infty} \frac{1}{4 n \epsilon^2_n}
$$
Let $\ \epsilon_n = \frac{\log(n)}{\sqrt{n}}\ $, then:
$$
	\lim\limits_{n\to\infty} P\big(\vert\hat{p}_n -p\vert > \frac{\log(n)}{\sqrt{n}}\big) \leq \lim\limits_{n\to\infty} \frac{1}{4 n (\frac{\log(n)}{\sqrt{n}})^2} = \lim\limits_{n\to\infty} \frac{1}{4(\log(n))^2} = 0
$$
This actually gives us an idea at what rate $\ \vert \hat{p}_n - p \vert \xrightarrow{P} p\ $ . Note $\epsilon_n = \frac{\alpha n}{\sqrt{n}}$ as long as $\alpha_n\to\infty$ , no matter how slow,  we still have the same result. This then suggests that perhaps $\vert \hat{p}_n - p\vert$ tends to zero in probability at the same rate as $\frac{1}{\sqrt{n}}$.\\
Suppose $\ X_1,\ldots,X_n\ $ have the same mean $\mu$ and variance $\sigma^2$ . Suppose further that \mbox{$Cov(X_i,X_j) = 0 \ \, \ i\neq j\ $} . Then \mbox{$\bar{X}_n \xrightarrow{P} \mu\ $} , i.e. $\bar{X}_n$ is a consistent estimator of $\mu\ $ , the population mean. Like the previous case:\\
$$
	P\big(\vert \bar{X}_n - \mu \vert > \epsilon\big) \leq \frac{1}{(\frac{\epsilon}{\sqrt{\frac{\sigma^2}{n}}})^2} = \frac{\sigma^2}{n\epsilon^2} \to 0 \ \ \ as\ n\to\infty\tab (1)
$$ 

Note that $\ \epsilon = K\sqrt{Var(\bar{X}_n)}=K\sqrt{\frac{\sigma^2}{n}}\ $ and hence $K=\frac{\epsilon}{\sqrt{\frac{\sigma^2}{n}}}\ $ . Then using Tchbushev's Inequality we obtain $(1)$ :
$$
	P\big(\vert \bar{X}_n - \mu\vert > \epsilon\big) \leq \frac{\sigma^2}{n \epsilon^2} \to 0 \ \ \ as\ m\to\infty\ \ \forall\ \epsilon > 0
$$
Thus:
$$
	\lim\limits_{n\to\infty} P\big(\vert \bar{X}_n - \mu\vert > \epsilon\big) = 0\ \ \ ,\ \ \forall\ \epsilon>0
$$
meaning that $\bar{X}_n\xrightarrow{P}\mu\ $ , \ i.e. $\bar{X}_n$ is a consistent estimator of $\mu$ .\\
The same approach cannot be used to show that $\delta^2_n\xrightarrow{P}\sigma^2\ $ ( We need the law of large numbers(Kolmogorov's result)).



\newpage
\section{Lecture 8}
\subsection{Consistency}
Consistency is the minimal property that an estimator is expected to possess. Consistency essentially means having right-headed ; in the sense that if "all" the population's information is available , the estimator produces the exact target. Recall once again:\\
\textbf{Definition: } Suppose $\hat{\theta}_n = \hat{\theta}(X_1,\ldots,X_n)$ is an estimator of $\theta$ . We say $\hat{\theta}_n$ is a consistent estimator of $\theta$ if \mbox{$\ \hat{\theta}_n\xrightarrow{P}\theta\ $} as $\ n\to\infty\ $ , i.e:
$$
	\lim\limits_{n\to\infty} P\big(\vert\hat{theta}_n - \theta\vert > \epsilon\big) = 0\ \ \ ,\ \ \ \forall\ \epsilon>0\tab .
$$
In lecture 7 we used Tchbyshev's inequality to establish consistency.\\
Markov's Inequality is an important tool in establishing consistency. In fact , Tchbyshev's inequality is a special case of Markov's inequality. It is often more straight forward to use Markov's inequality.\\
\subsection{Markov's Inequality}
Let $X$ be a random variable and $\ g \ $ a non-negative function. Then:
$$
	P(g(X) \geq \lambda) \leq \frac{\mathbb{E}[g(X)]}{\lambda} \ \ ,\ \ \forall\ \lambda>0\tab .
$$
Using Markov's inequality we have:
$$
	P\big(\vert\hat{\theta}_n - \theta\vert > \epsilon\big) \leq \frac{\mathbb{E}[\vert\hat{\theta}_n - \theta\vert]}{\epsilon}\tab (\dagger)
$$
To establish consistency it then suffices to show that the upper bound of the above inequality tends to zero as $n\to\infty$ .\\
Note that $(\dagger)$ follows from Markov's inequality if we define \mbox{$g(x)=\vert x-\theta\vert\ $} . Note also that our random variable is $\hat{\theta}_n$ .\\
To apply $\ (\dagger) \ $ , we need to find $\mathbb{E}[\vert\hat{\theta}_n - \theta\vert]$ which is not always easy. We however have:
$$
	P\big(\vert\hat{\theta}_n - \theta \vert > \epsilon\big) = P\big(\vert\hat{\theta}_n - \theta \vert^2 > \epsilon^2 \big) \overbrace{\leq}^{\text{Markov's Ineq.}} \frac{\mathbb{E}[\vert\hat{\theta}_n - \theta\vert^2]}{\epsilon^2}
$$
and thus:
$$
	P\big(\vert\hat{\theta}_n - \theta \vert > \epsilon \big) \leq \frac{MSE(\hat{\theta}_n}{\epsilon^2} = \frac{Var(\hat{\theta}_n) + Bias^2(\hat{\theta}_n)}{\epsilon^2}\tab (\ddagger)
$$
where
$$
	MSE(\hat{\theta}_n) = \mathbb{E}[(\hat{\theta}_n
-\theta)^2] = Var(\hat{\theta}_n) + [\overbrace{\mathbb{E}(\hat{\theta}_n) - \theta}^{\text{Bias}(\hat{\theta}_n)}]^2
$$
Now $(\ddagger)$ is often much easier to use since Variance and Bias of an estimator are often hard to find.
\begin{theorem*}[Slight Generalization of Theorem 9.1 , P450]
Suppose $\hat{\theta}_n$ is an estimator of $\ \theta\ $. Then $\ \hat{\theta}_n \xrightarrow{P}\theta\ $ if \mbox{$\ MSE(\hat{\theta}_n) \to 0\ $} as $n\to\infty$. In otherwords, $\ \hat{\theta}_n$ is a consistent estimator of $\theta\ $ if $\ MSE(\hat{\theta}_n)\to 0\ $ as $\ n\to\infty \ $ .
\end{theorem*} 
\begin{proof}
Using $(\ddagger)\ $ we have:
$$
	P\big(\vert \hat{\theta}_n - \theta \vert > \epsilon\big) \leq \frac{MSE(\hat{\theta}_n}{\epsilon^2}\to 0\tab as\ n\to\infty\ \ \forall \epsilon>0\ \ \because\ MSE(\hat{\theta}_n)\to 0\ \ \ as\ \ n\to\infty
$$
\end{proof}
\begin{corollary}
Let $\hat{\theta}_n$ be an unbiased estimator of $\theta$. Suppose $Var(\hat{\theta}_n)\to 0$ as $\ n\to\infty\ $. Then $\ \hat{\theta}_n\xrightarrow{P}\theta\ $ , i.e. $\hat{\theta}_n$ is a consistent estimator of $\theta$ .
\end{corollary}
\begin{proof}
$$
	MSE(\hat{\theta}_n) = Var(\hat{\theta}_n) + Bias^2(\hat{\theta}_n) = Var(\hat{\theta}_n)\to 0\ \ as\ \ n\to\infty
$$
Note that the $Bias(\hat{\theta}_n) = \mathbb{E}(\hat{\theta}_n) - \theta = 0\ $ if $\hat{\theta}_n$ is an unbiased estimator of $\theta$ , i.e. $\mathbb{E}(\hat{\theta}_n) = \theta $.
\end{proof}
\begin{example}
$X_i\distas{iid}Bernoulli(p)\ \ \ , \ \ i=1,2,\ldots,n$
\end{example}
\begin{equation*}
\begin{split}
	\hat{p}_n & = \frac{1}{n}\sum_{i=1}^n X_i \\
	\mathbb{E}(\hat{p}_n) &= \mathbb{E}[\frac{1}{n}\sum_{i=1}^n X_i] = \frac{1}{n}\sum_{i=1}^n \mathbb{E}(X_i) = \frac{1}{n} \sum_{i=1}^n p = p\\
	\text{Then }\ \mathbb{E}(\hat{p}_n) &= p\ .\ \text{i.e. $\hat{p}_n$ is an unbiased estimator of $p$ .}\\
	Var(\hat{p}_n) &= Var(\frac{1}{n}\sum_{i=1}^n X_i) = \frac{1}{n^2} \sum_{i=1}^n Var(\hat{X}_i) = \frac{1}{n^2}\sum_{i=1}^np(1-p) = \frac{p(1-p)}{n}\\
	\text{Now: } Var(\hat{p}_n) &= \frac{p(1-p)}{n}\to 0\ \ \ as \ n\to\infty
\end{split}
\end{equation*}
Thus using corollary 8.1 , $\hat{p}_n\xrightarrow{P}p\ $, i.e. $\hat{p}_n$ is a consistent estimator of $p$ .
\begin{example}
Suppose $X_1,\ldots,X_n$ are independent and identically distributed random variables with the common mean value $\mu$ and common variance $\sigma^2$.
\end{example}
Then:
$$
	\mathbb{E}(\bar{X}_n) = \mathbb{E}(\frac{1}{n}\sum_{i=1}^n X_i) = \frac{1}{n} \sum_{i=1}^n \mathbb{E}(X_i) = \frac{1}{n} \sum_{i=1}^n \mu = \mu
$$
$$
	Var(\bar{X}_n) = Var(\frac{1}{n}\sum_{i=1}^n X_i) = \frac{1}{n^2}\sum_{i=1}^n Var(X_i) = \frac{1}{n^2}\sum_{i=1}^n \sigma^2 = \frac{\sigma^2}{n}
$$
$$
\text{Thus:\tab} MSE(\bar{X}_n) = Var(\bar{X}_n) + \overbrace{Bias^2(\bar{X}_n)}^0 = \frac{\sigma^2}{n}\to 0 \ \ as \ n\to\infty
$$
and hence $\bar{X}_n\xrightarrow \mu $ using corollary 8.1 , i.e. $\bar{X}_n$ is a consistent estimator of $\mu$.
\begin{remark}
The conclusion of $2^{nd}$ example remains intact if the independence assumption is replaced by orthogonality , i.e. $Cov(X_i,X_j)\ \ , i\neq j$. 
\end{remark}
\begin{corollary}
Suppose $\hat{\theta}_n$ is an asymptotically unbiased estimator of $\theta$ , i.e. \mbox{$\lim\limits_{n\to\infty} \mathbb{E}(\hat{\theta}_n) = \theta\ $}. Suppose further that \mbox{$Var(\hat{\theta}_n)\to 0\ $} as $\ n\to\infty\ $. Then $\hat{\theta}_n \xrightarrow{P}\theta\ $ , i.e. $\hat{\theta}_n$ is a consistent estimator of $\theta$ 
\end{corollary}
\begin{proof}
$$
	\lim\limits_{n\to\infty} MSE(\hat{\theta}_n) = \lim\limits_{n\to\infty} Var(\hat{\theta}_n) + \lim\limits_{n\to\infty} Bias^2(\hat{\theta}_n) = 0+[\overbrace{\lim\limits_{n\to\infty}(\mathbb{E}(\hat{\theta}_n)-\theta)}^0]^2 = 0
$$
The desired result then follows form the above theorem.
\end{proof}
\begin{remark*}
The above result tells us that unbiasedness is NOT necessary for consistency.
\end{remark*}
\begin{example}
Suppose $X_1,\ldots,X_n$ from a random sample from a population with the mean $\mu$ and variance $\sigma^2$. We want to estimate $\sigma^2$.
\end{example}
We showed that $\mathbb{E}(S^2_n) = \sigma^2$ where :
$$
	S_n^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X}_n)^2
$$
We want to show that $S_n^2 \xrightarrow{P}\sigma^2$ , i.e. $S_n^2$ is a consistent estimator of $\sigma^2$ . Note that using Markov's inequality we have:
$$
	P\big(\vert S^2_n - \sigma^2\vert > \epsilon\big) \leq \frac{Var(S^2_n)}{\epsilon^2}
$$
We need to show that $Var(S^2_n)\to 0$ as $n\to\infty$ . To do this, we need to find $Var(S^2_n)$ in terms of the moments of the population. We therefore require conditions on the $4^{th}$ moment of the population from which the samples were taken. Below we give a different approach that is much easier to apply and require lesser assumptions, but much more base.
\subsection{Kolmogorov's Law of Large Numbers(LLN)}
Suppose ${X_n}^{\infty}_{n=1}$ is a sequence of $iid$ random variables with common mean $\mu$. Then:
$$
	\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i \xrightarrow{P} \mu = \mathbb{E}(X)
$$
\begin{remark*}
Kolgomorov's theorem is actually much stronger than this. It established a stronger notion of convergence. The complete form has two sides. It also shows that if $\bar{X}_n$ converges to a constant, since $C$ , in that stronger notion of convergence, then: \mbox{$\mathbb{E}(\vert X\vert)<\infty\ \ \ \And\ \ \ C=\mathbb{E}(X)$}.
\end{remark*}
\begin{corollary*}
Suppose $\big\{X_n \big\}^{\infty}_{n=1}$ is a sequence of $iid$ random variables with the common $K^{th}$-moment $\ \mu_K\ $ , i.e. $\mathbb{E}(X^K) = \mu_K\ $ , for some $K\in\mathbb{N}$ , then:
$$
	\frac{1}{n}\sum_{i=1}^nX_i^K \xrightarrow{P} \mu_K = \mathbb{E}(X^K)
$$
\end{corollary*}
This corollary follows from Kolmogorov's theorem immediately upon defining $Y_i = X_i^K$ .\\
Note that if $\mathbb{E}(X^K) < \infty$ , then $\mathbb{E}(X^n) < \infty \ \ \forall 0\leq r\leq K$ . This then means that if ${X_n}^{\infty}_{n=1}$ is a sequence $iid$ random variables with the common $K^{th}$-moment $\mu_K$ , then:
$$
	\frac{1}{n}\sum_{i=1}^n X_i^r \xrightarrow{P} \mu_K = \mathbb{E}(X^r) \ \ \forall\ 0\leq r\leq K
$$
We also need the following theorem which is essentially theorem 9.2 , page 451 of the textbook.
\begin{theorem}[Theorem 9.2, page 451]
	Suppose $\hat{\theta}_n = \hat{\theta}(X_1,\ldots,X_n)$ and \mbox{$\hat{\mathscr{C}}_n = 
\hat{\mathscr{C}}(X_1,\ldots,X_n)$} are consistent estimators of $\theta$ and $\mathscr{C}$ , respectively, i.e. $\hat{\theta}_n\xrightarrow{P}\theta$ and $\mathscr{\hat{C}}_n\xrightarrow{P}\mathscr{C}\ $ .
	\begin{enumerate}[a)]
	\item $\hat{\theta}_n\mathscr{\hat{C}}_n \xrightarrow{P} \theta\mathscr{C}$
	\item $\hat{\theta}_n \pm \mathscr{\hat{C}}_n \to \hat{\theta}_n \pm \hat{\mathscr{C}}_n$
	\item $\frac{\hat{\theta}_n}{\hat{\mathscr{C}}_n} \to \frac{\theta}{\mathscr{C}}\tab[4cm]$ provided that $\hat{\mathscr{C}}\neq 0\ , \ \mathscr{C}\neq 0$
	\item $g(\hat{\theta}_n)\to g(\theta)\tab[3cm]$ if $g(.)$ is a continuous function
	\end{enumerate}
\end{theorem}
Part(d) of the above theorem is called \textbf{Continuous Mapping Theorem} .\\
Now to establish consistency of $S^2_n$ , we fist establish \mbox{$\tab[0.5cm] S^2_{n,*} = \frac{1}{n}\sum_{i=1}^n (X_i - \bar{X}_n)^2 \xrightarrow{P} \sigma^2 .$}
	\begin{equation*}
	\begin{split}
		\text{\textbf{Step 1:}}\\
		S^2_{n,*} &= \frac{1}{n} \sum_{i=1}^n (X_i-\bar{X}_n)^2 = \frac{1}{n}[\sum_{i=1}^n X_i^2 -2\bar{X}_n\overbrace{\sum_{i=1}^nX_i}^{n\bar{X}_n} + n\bar{X}_n^2]\\
		&= \frac{1}{n} [\sum_{i=1}^n X_i^2 - 2\bar{X}_n(n\bar{X}_n)+n\bar{X}_n^2]\\
		&= \frac{1}{n}[\sum_{i=1}^n X_i^2 -2n\bar{X}^2_n + n\bar{X}^2_n]\\
		&= \frac{1}{n}[\sum_{i=1}^n X_i^2 - n\bar{X}_n^2]\\
		&= \frac{1}{n}\sum_{i=1}^n X_i^2 - \bar{X}^2_n
	\end{split}
	\end{equation*}
	\begin{equation*}
	\begin{split}
		\text{\textbf{Step 2:}}\\
		&\text{Using Kolmogorov's Theorem:}\\
		\text{a)\ } &\frac{1}{n} \sum_{i=1}^n X_i^2 \xrightarrow{P} \mathbb{E}(X^2)\\
		\text{b)\ } &\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i \xrightarrow{P} \mathbb{E}(X)
	\end{split}
	\end{equation*}			
	\begin{equation*}
	\begin{split}
		\text{\textbf{Step 3:}}\\
		&\text{Using Step 2 and continuous mapping theorem (Theorem 9.2 (d))}\\
		&\tab \bar{X}_n^2 \to [\mathbb{E}(X)]^2
	\end{split}
	\end{equation*}
	\begin{equation*}
	\begin{split}
		\text{\textbf{Step 4:}}\\
		&\text{Using Step 1,2,3 and Theorem 9.2 (b) we have:}\\
		S^2_n &= \frac{1}{n} \sum_{i=1}^n X_i^2 - \bar{X}_n^2 \tab[0.5cm]\xrightarrow{P}\tab[0.5cm]\mathbb{E}(X^2) - [\mathbb{E}(X)]^2 = Var(X) = \sigma^2
	\end{split}
	\end{equation*}
Thus $S^2_{n,*} \xrightarrow{P}\sigma^2\ $ , i.e. $S^2_{n,*}$ is a consistent estimator of $\sigma^2$ .\\
Next we note that:\\
$$
	S^2_n = \frac{1}{n-1}\sum_{i=1}^n (X_i-\bar{X}_n)^2 = (\frac{n}{n-1})\frac{1}{n}\sum_{i=1}^n (X_i - \bar{X}_n)^2 = (\frac{n}{n-1})S^2_{n,*}
$$ 
Using Theorem 9.2 (a):
$$
	\lim\limits_{n\to\infty} (\frac{n}{n-1}) = 1 \ \ \ \And\ \ \  S^2_{n,*} \xrightarrow{P} \sigma^2
$$
Then Theorem 9.2 (a) implies that:
$$
	S^2_n = (\frac{n}{n-1})S^2_{n,*}\ \ \ \xrightarrow{P}\ \ \ 1\cdot\sigma^2 = \sigma^2
$$
i.e. $S^2_n$ is a consistent estimator of $\sigma^2$ . 
\begin{remark*}
Suppose $P(X_n=C_n)=1\ \ ,\ n=1,2,\ldots$ where $\big\{C_n\big\}_{n=1}^{\infty}$ is a sequence of real numbers such that $\lim\limits_{n\to\infty} C_n = C$ . Then $X_n\xrightarrow{P}C$. The proof of this result is as follow:
\end{remark*}
\begin{proof}\hfill\\
	$\lim\limits_{n\to\infty} C_n = C\ $ i.e. $\ \forall\ \epsilon>0 \ \exists \ N(\epsilon) \in\ \mathbb{N} \ \ni\ \vert C_n - C\vert < \epsilon\ ,\ \forall\ n\geq\ N(\epsilon)$\\
	Now suppose $\epsilon > 0$ is given , then:\\
	$P(\vert X_n - C\vert > \epsilon) = P(\vert C_n - C\vert > \epsilon) = 0\tab \text{if $\ \ \ n\geq N(\epsilon)$}$\\
	Thus $\lim\limits_{n\to\infty} P(\vert X_n - C\vert > \epsilon)=0\ \ ,\ \ \forall\ \epsilon>0$
\end{proof}
\textbf{Question}: Why couldn't we use Kolmogorov's theorem directly to establish consistency of $S^2_{n,*}$ ? In other words, coudln't we define \mbox{$Z_i = (X_i - \bar{X}_n)^2$} and hence \mbox{$S^2_{n,*} = \frac{1}{n}\sum_{i=1}^n (X_i - \bar{X}_n)^2 = \frac{1}{n}\sum_{i=1}^n Z_i$} and then apply Kolgomorov's \mbox{theorem?}\\
The answer is that $Z_i$s are not independent . Note that \mbox{$\sum_{i=1}^n (X_i-\bar{X}_n)=0$} .
\subsection{Sufficiency}
Sufficiency is essentially comparison. Sufficiency is one of the main pillars of the \underline{likelihood Inference} .\\
As the following diagram shows the likelihood inference has three main components: the observable quantities, samples, the unobservable quantities, the unknown parameters to be estimated , and a parametric distribution that links the observables to unobservables .\\
\textbf{Example:\ \ } $f_{\theta}(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\ \ \ \ where \ \ \ \theta = (\mu, \sigma^2)
$\\
\#MISSING GRAPH Lecture 8 - Page 57



\newpage
\section{Lecture 9}
\subsection{Sufficiency}
Suppose $\ T:\mathbb{R}^n\to\mathbb{R}^m\ \ ,\ \ m<n\ $ is a map from $\mathbb{R}^n$ to $\ \mathbb{R}^m\ $ and $\ X=(X_1,X_2,\ldots,X_n)\ $ and $\ Y=(Y_1,Y_2,\ldots,Y_n)$ are two n-dimensional random vectors. Then \mbox{$\underset{\sim}{X}\ \ \And\ \ \underset{\sim}{Y}$} are called \underline{T-similar} if:
$$
	P_{_{\underset{\sim}{X}\vert{T=t}}}(\underset{\sim}{u}\vert t,\theta) = P_{_{\underset{\sim}{Y}\vert{T=t}}}(\underset{\sim}{u}\vert t,\theta) \tab \forall\ u\ \text{and\ }t
$$
\begin{definition}[T-similar]A realization of $\underset{\sim}{X}$ , say $\underset{\sim}{x}$ , and a realization of $\underset{\sim}{Y}$ say $\underset{\sim}{y}$ , are called T-similar if:
\begin{enumerate}
\item $\underset{\sim}{X}$ and $\underset{\sim}{Y}$ are T-similar .
\item $T(\underset{\sim}{x}) = T(\underset{\sim}{y})$
\end{enumerate}
\end{definition}
What do we expect from a \underline{good comparison}?\\
Suppose $\theta$ is the unknown parameter of interest. We want to estimate $\theta$ using \mbox{$\underset{\sim}{X} = (X_1,\ldots,X_n)$}. Now \mbox{$T_n = T(\underset{\sim}{X}) = T(X_1,\ldots,X_n)$} is a good comparison if:
\begin{enumerate}[a)]
\item	$T_n$ can preserve all the pertinent "information" in \mbox{$\underset{\sim}{X} = (X_1,\ldots,X_n) to \theta$}
\item if $\underset{\sim}{X}^{*} = (X_1^{*},\ldots,X_n^{*})$ is the original sample, for any given value of $T_n$ , say t, we can generate a $T_n$-similar sample of $x$s.
\end{enumerate}
In order to generate a $T_n$-similar sample \mbox{$P(X_1=x_1,X_2=x_2,\ldots,X_n=x_n \vert T_n=t)$} should be \underline{free} from any unknown parameter.\\
As per retaining pertinent information in the data to the unknown parameter $\underset{\sim}{\theta}$ , given that the link between the data and the unknown parameter(s) is the joint distribution:
$$
	P\big(X_1=x_1,X_2=x_2,\ldots,X_n=x_n ; \underset{\sim}{\theta}\big) = P_{_{\underset{\sim}{\theta}}}\big(X_1=x_1,X_2=x_2,\ldots,X_n=x_n\big)
$$
any possible information in the data about $\theta$ should be in \mbox{$P_{_{\underset{\sim}{\theta}}}\big(X_1=x_1,X_2=x_2,\ldots,X_n=x_n\big)$} .\\
As such $T_n = T(X_1,\ldots,X_n)$ can preserve all the pertinent information if \mbox{$P_{_{\underset{\sim}{\theta}}}\big(X_1=x_1,X_2=x_2,\ldots,X_n=x_n\big)\propto g(t ; \underset{\sim}{\theta})$}.\\
\emph{The Fisher-Neyman Factorization Theorem} shows that this proportional is indeed a characterization of sufficient statistics. Let's dig into this a bit more. Note that:
$$
P_{_{\underset{\sim}{\theta}}}\big(X_1=x_1,X_2=x_2,\ldots,X_n=x_n\big) = P_{_{\underset{\sim}{\theta}}}\big(X_1=x_1,X_2=x_2,\ldots,X_n=x_n\big| T_n = t\big)P_{\theta}(T_n = t)\ \ .
$$
If $P_{_{\underset{\sim}{\theta}}}\big(X_1=x_1,X_2=x_2,\ldots,X_n=x_n\big| T_n = t\big)$ is actually free from $\underset{\sim}{\theta}$ , then:
$$
	P_{_{\underset{\sim}{\theta}}}\big(X_1=x_1,X_2=x_2,\ldots,X_n=x_n\big) \ \ \propto\ \  P_{\theta}(T_n=t) \overset{def.}{=}g(t\ ; \theta)
$$
where the proportionality constant is a function of \mbox{$\underset{\sim}{x}=(x_1,x_2,\ldots,x_n)$} , the observed sample. A formal definition then emerges.
\begin{definition*}
Let $X_1,\ldots,X_n$ be a random sample from a distribution with an unknown parameter $\underset{\sim}{\theta}$ . A statistic \mbox{$T_n = T(X_1,\ldots,X_n)$} is called \textbf{sufficient} for $\theta$ if the conditional distribution of \emph{\mbox{$(X_1,\ldots,X_n)$}} given $T_n$ does not depend on $\underset{\sim}{\theta}$.
\end{definition*}
\begin{example*}
$X_i\distas{iid} Bernoulli(p)\ \ ,\ \ i=1,2,\ldots,n$.
\end{example*}
$$
	P_p(X=x) = p^x(1-p)^{1-x} \ \ ,\ \ x=0,1
$$
Consider $T_n = \sum \limits_{i=1}^n X_i\ $.\ Note that $\ T_n \sim Bin(n,p)\ $ Then:
\begin{equation*}
\begin{split}
	P\big(X_1=x_1,X_2=x_2,\ldots,X_n=x_n \big| T_n = t\big)&=\left\{
		\begin{array}{lr}
		\frac{P(X_1=x_1,X_2=x_2,\ldots,X_n=x_n , T_n = t)}{P(T_n = t)} &\tab[0.25cm] \text{if }\ \sum\limits_{i=1}^n x_i = t \\
		0 &\tab[0.25cm] \text{otherwise}
		\end{array}
		\right.\\
		&=\left\{
		\begin{array}{lr}
		\frac{P(X_1=x_1,X_2=x_2,\ldots,X_n=x_n)}{P(T_n = t)}
		 &\tab[0.6cm] \text{if }\ \sum\limits_{i=1}^n x_i = t \\
		 0 &\tab[0.6cm] \text{otherwise}
		\end{array}
		\right.\\
		&=\left\{
		\begin{array}{lr}
		\frac{\prod \limits_{i=1}^n p^{x_i}(1-p)^{1-x_i}}{\binom{n}{t}p^t (1-p)^{n-t}}
		&\tab[2cm] \text{if }\ \sum\limits_{i=1}^n x_i = t \\
		0 &\tab[2cm]\text{otherwise}
		\end{array}
		\right.\\
		&=\left\{
		\begin{array}{lr}
		\frac{\cancel{p^{\sum\limits_{i=1}^n x_i}}\cancel{(1-p)^{n-\sum\limits_{i=1}^n x_i}}}{\binom{n}{t} \cancel{p^t}\cancel{(1-p)^{n-t}}}
		&\tab[2cm] \text{if }\ \sum\limits_{i=1}^n x_i = t \\
		0&\tab[2cm]\text{otherwise}
		\end{array}
		\right.\\
	\text{thus\ }\ \ P\big(X_1=x_1,X_2=x_2,\ldots,X_n=x_n \big| T_n = t\big)&=
		\left\{
		\begin{array}{lr}
		\frac{1}{\binom{n}{t}}
		&\tab[3cm] \text{if }\ \sum\limits_{i=1}^n x_i = t \\
		0&\tab[3cm]\text{otherwise}
		\end{array}
		\right.\tab (1)
\end{split}
\end{equation*}
and hence $T_n$ is a sufficient statistic for $p\ $.
\begin{remark}
To generate a $T_n$-similar sample when $T_n$ is given, say $T_n = t$ , we define:
$$
	A_t = \big\{(x_1,x_2,\ldots,x_n)\colon \sum\limits_{i=1}^n x_i = t\big\}
$$
\end{remark}
Note that $card(A_t)=\binom{n}{t}$ . According to $(1)$ we give equal weight , i.e. problem mass, to each element of $A_t$ . We then choose one element of $A_t$ randomly.
\begin{example}
$X_i \distas{iid}P(\lambda)\ \ \ ,\ \ \ i=1,2,\ldots,n$
\end{example}
$$
	P_{\lambda}(X=x) = \frac{e^{-\lambda}\lambda^x}{x!}\ \ ,\ \ x=0,1,2,\ldots
$$
Consider $T_n = \sum\limits_{i=1}^n x_i$ . Note that $T_n\sim P_0(n\lambda)\ $ (Exercise)
\begin{equation*}
\begin{split}
P_{\lambda} (X_1=x_1,X_2=x_2,\ldots,X_n=x_n \big| T_n=t)
	&=\left\{ 
	\begin{array}{lr}
	\frac{P_{\lambda} (X_1=x_1,X_2=x_2,\ldots,X_n=x_n,T_n=t)}{P(T_n = t)}
	&\tab \text{if }\sum\limits_{i=1}^n x_i=t\\
	0 & \tab \text{otherwise}
	\end{array}
	\right.\\
	&=\left\{
	\begin{array}{lr}
	\frac{\frac{\prod\limits_{i=1}^n e^{-\lambda}\lambda^{x_i}}{x_i!}}{\frac{e^{-n\lambda}(n\lambda)^t}{t!}}
	&\tab[3cm] \text{if }\ \sum\limits_{i=1}^n x_i = t \\
	0 & \tab \text{otherwise}
	\end{array}
	\right.\\
	&=\left\{
	\begin{array}{lr}
	\frac{\frac{\cancel{e^{-n\lambda}}\cancel{\lambda^{\sum\limits_{i=1}^n x_i}}}{\prod\limits_{i=1}^n x_i !}}{\frac{\cancel{e^{n\lambda}\cancel{(n\lambda)^t}}}{t!}}
	&\tab[3cm] \text{if }\ \sum\limits_{i=1}^n x_i = t \\
	0&\tab[3cm]\text{otherwise}
	\end{array}
	\right.\\
	&=\left\{
	\begin{array}{lr}
	\frac{t!}{\prod\limits_{i=1}^n x_i!}\cdot\frac{1}{n^t}
	&\tab[3cm] \text{if }\ \sum\limits_{i=1}^n x_i = t \\
	0&\tab[3cm]\text{otherwise}
	\end{array}
	\right.\tab (2)
\end{split}
\end{equation*}
Thus:
$$
	\boxed{\underset{\sim}{X}\ |_{{_{T_n=t}}}\ \sim\ \text{Multinomial}(t,p_i=\frac{1}{n}, \ i=1,2,\ldots,n)}
$$
Recall that: \\
\mbox{$
	(Y_1,Y_2,\ldots,Y_k) \sim \text{Multinomial}(n,p_1,p_2.\ldots,p_k) \text{ \ if \ } P(X_1=x_1,\ldots,X_k=x_k) = \binom{n}{x_1,\ldots,x_k}\prod\limits_{i=1}^n p_i^{x_i}
$} where \mbox{$\ \sum\limits_{i=1}^n x_i = n \ \ \And\ \ \ \sum\limits_{i=1}^kp_i=1$} \ and \ $\binom{n}{x_1,x_2,\ldots,x_k} = \frac{n!}{x_1!,x_2!\ldots,x_k!}$\\
Again, to generate a $T_n$-similar sample we define:
$$
	A_t = \big\{(x_1,x_2,\ldots,x_n)\colon \sum\limits_{i=1}^n x_i=t\big\}
$$
The probability mass associated to elements of $A_t$ is given by $(2)$ . In other words, we choose an element of $A_t$ using a multinomial distribution with $n=t\ \ ,\ \ k=n\ \ and\ p_i=\frac{1}{n}\ \ ,\ i=1,2,\ldots,n$ .
\subsection{Likelihood}
\begin{definition*}
	Let $\underset{\sim}{Y} = (Y_1,Y_2,\ldots,Y_n)\ $ be a random vector whose joint pdf or pmf depends on $\underset{\sim}{\theta}\ $ , a vector of unknown parameters. \textbf{The Likelihood Function} . a function of $\underset{\sim}{\theta}$ , for a realization \mbox{$\underset{\sim}{y}=(y_1,y_2,\ldots,y_n)$} is defined to be:
\end{definition*}
$$
	\mathscr{L}(\underset{\sim}{\theta} ; \underset{\sim}{y}) = \left\{
		\begin{array}{lr}
		P_{_{\underset{\sim}{\theta}}}(Y_1=y_1,Y_2=y_2,\ldots,Y_n=y_n)
		&\tab\text{if $y_i$s are discrete random variables}\\
		f_{_{\underset{\sim}{\theta}}}(y_1,y_2,\ldots,y_n) 
		&\tab\text{if $y_i$s are continuous random variables}
		\end{array}
		\right.
$$
\begin{example*}
	$X_i \distas{iid}Bernoulli(p)\ \ ,\ \ i=1,2,\ldots,n$
\end{example*}
$P_p(X=x) = p^x(1-p)^{1-x}\ \ ,\ \ i=0,1$
\begin{equation*}
\begin{split}
	\mathscr{L}(p;x_1,x_2,\ldots,x_n) &= P_p(X_1=x_1,X_2=x_2,\ldots, X_n=x_n)\\
	\implies\ 
	&= P_p(X_1=x_1)\ldots P_p(X_n=x_n) \tab \text{Independenc and identically distributed}\\
	&= \prod\limits_{i=1}^n p^{x_i}(1-p)^{1-x_i} = p^{\sum\limits_{i=1}^nx_i}(1-p)^{n-\sum\limits_{i=1}^nx_i}\tab[2cm](B)
\end{split}
\end{equation*}
\begin{example*}
$X_i \distas{iid} P_0(\lambda)\ \ ,\ \ i=1,2,\ldots,n$
\end{example*}
$P_{\lambda}(X=x)=\frac{e^{-\lambda}\lambda^x}{x!}\ \ ,\ \ x=0,1,2,\ldots$
\begin{equation*}
\begin{split}
\mathscr{L}(\lambda ; x_1,x_2,\ldots,x_n) &= P_{\lambda}(X_1=x_1,\ldots,X_n=x_n)\\
	&= \prod\limits_{i=1}^n \frac{e^{-\lambda}\lambda^{x_i}}{x_i!}\tab[2cm](P)\tab[2cm]\text{Independence and identically distributed}\\
	&= \frac{e^{-n\lambda}\lambda^{\sum\limits_{i=1}^nx_i}}{\prod\limits_{i=1}^nx_i!}
\end{split}
\end{equation*}
\begin{example*}
$X_i \distas{iid} N(\mu,\sigma^2)\ \ , \ \ i=1,2,\ldots,n$
\end{example*}
$ f_{\mu,\sigma^2}(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\ \ \ \ -\infty<x<+\infty $
\begin{equation*}
\begin{split}
\mathscr{L}(\mu,\sigma^2; x_1,x_2,\ldots,x_n) &= f_{\mu,\sigma^2}(x_1,x_2,\ldots,x_n)\\
&=\prod\limits_{i=1}^n f_{\mu,\sigma^2}(x_i) \tab[2cm]\text{Independent and identically distributed}\\
&= \prod\limits_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-\mu)^2}{2\sigma^2}}\\
&=(\frac{1}{\sqrt{2\pi}\sigma^2})exp\Big\{-\frac{1}{2\sigma^2}\sum\limits_{i=1}^2(x_i-\mu)^2\Big\}\tab[2cm](N)
\end{split}
\end{equation*}
The examples we presented for sufficiency required first specifying a candidate statistic. The question then is how we come up with a sufficient statistic. The following theorem due to \emph{Fisher $\And$ Neyman} tells us how to find sufficient statistic.
\begin{theorem*}[Fisher-Neyman Factorization Theorem - Thm 9.4 , page 461]
A statistic $T=T(Y_1,T_2,\ldots,T_n)$ for $\underset{\sim}{\theta}$ the parameter of the distribution of \mbox{$Y_1,Y_2,\ldots,Y_n$} \underline{if and only if} :
\mbox{$	\mathscr{L}(\underset{\sim}{\theta};y_1,y_2,\ldots,y_n) = g\ (t ; \theta)\  h\ (y_1,y_2, \ldots,y_n)$}
For any realization $(y_1,y_2,\ldots,y_n)$ , where $t=T\ (y_1,y_2,\ldots,y_n)$ .
\end{theorem*}
\begin{example*}
$X_i\distas{iid}Bernoulli(p) \ \ ,\ \ i=1,2,\ldots,n$
\end{example*}
Using $(B)$ (the result above):
$$
	\mathscr{L}(p;x_1,x_2,\ldots,x_n) = p^{\sum\limits_{i=1}^nx_i}(1-p)^{n-\sum\limits_{i=1}^nx_i}
$$
Then define $T=\sum\limits_{i=1}^nx_i\ $\ , \ \mbox{$\ g(t;p) = p^{\sum\limits_{i=1}^nx_i}(1-p)^{n-\sum\limits_{i=1}^nx_i} = p^t(1-p)^{n-t} $} \ and \mbox{$\ h(x_1,x_2,\ldots,x_n) \equiv  1 \ $.} Using Fisher-Neyman theorem $T=\sum\limits_{i=1}^nX_i$ is a sufficient statistic for $p$ .
\begin{example*}
	$X_i \distas{iid} P_0(\lambda)\ \ ,\ \ i=1,2,\ldots,n$
\end{example*} 
Using $(P)$ (the result above):
$
	\mathscr{L}(\lambda;x_1,\ldots,x_n) = \frac{e^{-n\lambda}\lambda^{\sum\limits_{i=1}^n x_i}}{\prod\limits_{i=1}^n x_i!}
$ \ .\tab[0.5cm] Define\ $\ T=\sum\limits_{i=1}^n X_i$\ and \ \mbox{$g(t;\lambda) = e^{-n\lambda}\lambda^t = e^{-n\lambda}\lambda^{\sum\limits_{i=1}^nx_i}$} and $\ h(x_1,\ldots,x_n) = \frac{1}{\prod\limits_{i=1}^n x_i!}\ $ . Then using Fisher-Neyman Theorem, $T=\sum\limits{i=1}^n X_i\ $ is a sufficient statistic for $\lambda$ .
\begin{example*}
$X_i\distas{iid}N(\mu,\sigma^2)\ \ ,\ \ i=1,2,\ldots,n$ .
\end{example*}
Note that now we have \underline{two unknown parameters}, $\mu$ and $\sigma^2$ . Using $(N)$ :
\mbox{$
\mathscr{L}(\mu,\sigma^2 ; x_1,x_2,\ldots,x_n) = (\frac{1}{\sqrt{2\pi}\sigma})^n exp\Big\{-\frac{1}{2\sigma^2}\sum\limits_{i=1}^n(x_i-\mu)^2\Big\}
$}\\
Now note that: \ $\ \sum\limits_{i=1}^n (x_i-\mu)^2 = \sum\limits_{i=1}^n x_i^2 -2\mu\sum\limits_{i=1}^nx_i+n\mu^2$\\
Thus: \ $\ \mathscr{L}(\mu,\sigma^2;x_1,x_2,\ldots,x_n) = (\frac{1}{\sqrt{2\pi}\sigma})^n exp\Big\{-\frac{\sum\limits_{i=1}^nx_i^2}{2\sigma^2}+\frac{\mu\sum\limits_{i=1}^nx_i}{\sigma^2}-\frac{n\mu^2}{2\sigma^2}\Big\}$\\
Define: \ $\ T=(\sum\limits_{i=1}^nX_i , \sum\limits_{i=1}^n X_i^2)\ $ \ \ , \ \  \mbox{$g(\underset{\sim}{t} ; \underset{\sim}{\theta}) =  (\frac{1}{\sqrt{2\pi}\sigma})^n exp\Big\{-\frac{\sum\limits_{i=1}^nx_i^2}{2\sigma^2}+\frac{\mu\sum\limits_{i=1}^nx_i}{\sigma^2}-\frac{n\mu^2}{2\sigma^2}\Big\}$}  \ \ where $\underset{\sim}{\theta}=(\mu,\sigma^2)\ $\ and \ $\ h(x_1,x_2,\ldots,x_n)\equiv 1\ $. Then using Fisher-Neyman Theorem $\ T=(\sum\limits_{i=1}^nX_i , \sum\limits_{i=1}^n X_i^2)\ $ is a sufficient statistic for $\underset{\sim}{\theta} = (\mu,\sigma^2)$.
\begin{remark*}
Note that do identify the sufficient statistic using Fisher-Neyman Theorem,  you only need the part of the likelihood in which you cannot separate the unknown parameters from observations. This part is called \emph{\textbf{kernel}} . In other words, you can write a likelihood as the product of a function of observations alone, a function of parameters alone and the kernel. \emph{The sufficient statistic is in the kernel}.
\end{remark*}


\newpage
\section{Lecture 10}
\subsection{The Rao-Blackwell Theorem}[Theorem 9.5 , page 464]
An interesting and important application of sufficiency is in variance reduction. This application is formalized in a theorem due to Rao(C.R) and Blackwell (David) .\\
We first need to recall Theorem 5.14 (Page 286) and Theorem 5.15 (page 287):
\tab Theorem 5.14 (page 286):
$$
	\mathbb{E}(X) = \mathbb{E}\ \big\{\mathbb{E}(X | Y)\big\}
$$
\tab Theorem 5.15 (page 287):
$$
	Var(X) = Var\ \big\{\mathbb{E}(X | Y)\big\} + \mathbb{E}\ \big\{Var(X|Y)\big\}
$$
\begin{theorem*}[The Rao-Blackwell Theorem - Thm 9.5 , page 464]
Let $\hat{\theta}$ be an unbiased estimator for $\theta$ such that $V(\hat{\theta})<\infty$ . Suppose $T$ is a sufficient statistic for $\theta$ . Define \mbox{$\hat{\theta}^{*}=\mathbb{E}(\hat{\theta}\big| T)$}. Then, for all $\theta$ :\\
\tab $(a) \ \ \mathbb{E}(\hat{\theta}^{*}) = \theta$\\
\tab $(b) \ \ Var(\hat{\theta}^{*}) \leq Var(\hat{\theta})$ .
\end{theorem*}
\begin{proof}
	First note that $T$ is a sufficient statistic for $\theta$ , thus the distribution of $\hat{\theta}$ given $T$ does not depend on $\theta$ . Therefore $\mathbb{E}(\hat{\theta} | T)$ is a statistic . This is when sufficiency plays its role.\\
	To prove part $(a)$ of formula, we use Theorem 5.14, page 286:
	$$
		\mathbb{E}(\hat{\theta}^{*}) = \mathbb{E}\big[\mathbb{E}(\hat{\theta} | T) \big] = \mathbb{E}(\hat{\theta}) = \theta\tab \forall \ \theta .
	$$
To prove part $(b)$, we use Theorem 5.15, page 287:
$$
	Var(\hat{\theta}^{*}) = Var\big\{\mathbb{E}(\hat{\theta} | T)\big\} \leq Var\big\{\mathbb{E}(\hat{\theta} | T)\big\} + 
	\overbrace{\mathbb{E}\{\underbrace{Var(\hat{\theta}| T)}_{\geq 0}\}}^{\geq 0}
$$
\end{proof}
\begin{remark*}[Completeness] A statistic $T$ or its family of distribution \mbox{$\{F_{\theta} \colon \theta\in\Theta\}$} where $\Theta$ is the set of all admissible values of $\theta$, is called \textbf{\underline{complete}}  if for any \underline{reasonable} $g$:
$$
	\mathbb{E}_{\theta}[g(T)] = 0 \ \ \ , \ \ \ \forall\ \theta\in\Theta
$$
\end{remark*}
implies that $g(t) = 0$ for all possible values of t. If a sufficient statistic $T$ is also complete, then \mbox{$\hat{\theta}^{*} = \mathbb{E}(\hat{\theta} | T)$} will be the \underline{M}inimum \underline{V}ariance \underline{U}nbiased \underline{E}stimator (MVUE) . This often means that within the class of unbiased estimator $\hat{\theta}^{*}$ is the least; i.e. the closest in the $MSE$  sense,  to the unknown parameter $\theta$. Recall that:
$$
	MSE_{\theta}(\hat{\theta}) = Var_{\theta}(\hat{\theta})+Bias_{\theta}^2(\hat{\theta})=Var_{\theta}(\hat{\theta})
$$
if $Bias_{\theta}(\hat{\theta}) = 0 $ ; i.e. if $\hat{\theta}$ is an unbiased estimator of $\theta$.\\
The notion of Completeness is due to \emph{\textbf{Lehmann(Eric Leo) and Scheffe'(Henry)}}.

Then using the Rao-Blackwell and Lehmann-Schaffe' theorems we have an easy recipe for finding the $MVUE$.\\
\tab\textbf{Step 1:\ } Using Fisher-Neyman theorem, find a sufficient statistic, say T, for $\theta$ .\\
\tab\textbf{Step 2:\ } Find an unbiased estimator $\theta$ , say $\hat{\theta}$ .\\
\tab\textbf{Step 3:\ } Find $\hat{\theta}^{*} = \mathbb{E}(\hat{\theta} | T)$ .
\begin{remark*}
For the examples and exercises in the course , the sufficient statistic you find in \textbf{Step 1} using Fisher-Neyman Theorem is also complete.
\end{remark*}
\begin{example*}[Ex. 9.6 , page 466]
	$X_i\distas{iid}\text{Bernoulli}(p)\ \ , \ \ i=1,2,\ldots,n$
\end{example*}
\textbf{Step 1:} 
\begin{equation*}
\begin{split}
	\mathscr{L}(p;x_1,\ldots,x_n) &= P_p (X_1=x_1,\ldots,X_n=x_n)\\
	\text{Independence}\tab &= \prod\limits_{i=1}^n P_p(X_i=x_i)\\
	\text{Identically distributed}\tab &= \prod\limits_{i=1}^n p^{x_i}(1-p)^{1-x_i}\\
	&= p^{\sum\limits_{i=1}^n x_i}(1-p)^{n-\sum\limits_{i=1}^n x_i}\\
	\text{Then $\ T=\sum\limits_{i=1}^n X_i\ $ is a sufficient statistic.}
\end{split}
\end{equation*}
\tab\textbf{Step 2:}
$$
\hat{p}_n = \frac{1}{n}\sum\limits_{i=1}^n X_i \text{ is an unbiased estimator of } p
$$
$$
	\mathbb{E}(\hat{p}_n) = \mathbb{E}(\frac{1}{n}\sum\limits_{i=1}^n X_i) = \frac{1}{n}\sum\limits_{i=1}^n \underbrace{\mathbb{E}(X_i)}_{p} = \frac{1}{n}\cdot np = p
$$
\tab\textbf{Step 3:}
$$
	\text{Note that \ } \hat{p}_n = \frac{1}{n}\sum\limits_{i=1}^n X_i = \frac{T}{n}\tab\text{thus}\tab\mathbb{E}(\hat{p}_n | T) = \frac{T}{n} = \hat{p}_n \ \ .
$$
\begin{remark*}\emph{
	What we observed in \emph{step 3} of the above example tells us that \emph{step 3} of our recipe is redundant when $\theta$ found in \emph{step 2} is a function of the sufficient statistic found in \emph{step 1}.}
\end{remark*}
\begin{example*}[Ex. 9.8 , page 467]
$X_i\distas{iid} N(\mu,\sigma^2)$
\end{example*}
\textbf{Step 1}
\begin{equation*}
\begin{split}
	\mathscr{L} (\mu,\sigma^2 ; x_1,\ldots,x_n) &= f_{\mu,\sigma^2}(x_1,\ldots,x_n)\\
	&= \prod\limits_{i=1}^n f_{\mu,\sigma^2} (x_i)\\
	&= \prod\limits_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-\mu)^2}{2\sigma^2}}\\
	\text{hence: } \mathscr{L}(\mu,\sigma^2 ; x_1,\ldots,x_n) &= (\frac{1}{\sqrt{2\pi}\sigma})^n\ exp\Big\{-\frac{1}{2\sigma^2}\big[\sum\limits_{i=1}^n x_i^2 - 2\mu\sum\limits_{i=1}^nx_i + n\mu^2\big]\ \Big\}\\
	\text{Thus:\tab[2.7cm]} T &= \big(\sum\limits_{i=1}^n X_i \ , \ \sum\limits_{i=1}^n X_i^2\big)
\end{split}
\end{equation*}
\tab\textbf{Step 2:\ } \ $\bar{X}_n = \frac{1}{n}\sum\limits_{i=1}^nX_i\ $ is an unbiased estimator of $\ \mu\ $ and \mbox{$\ S_n^2 = \frac{1}{n-1}\sum\limits_{i=1}^n (X_i-\bar{X})^2\ $} \tab is an unbiased estimator of $\ \sigma^2\ $. \\
\tab\textbf{Step 3:}
\begin{equation*}
\begin{split}
\tab & \mathbb{E}(\bar{X}_n \big| T) = \bar{X}_n\tab[4cm] \text{since $\ \bar{X}_n\ $ is a function of $T$}\\
& S^2_n = \frac{1}{n-1}\big[\sum\limits_{i=1}^n X_i^2 - n\bar{X}\big]\tab[2.3cm] \text{is also a function of $T$}\\
\text{Thus \ } &\mathbb{E}(S^2_n | T)= S^2_n\\
\text{Thus $\ \bar{X}_n\ $ is the $\ MVUE\ $ of $\ \mu\ $ and $\ S^2_n\ $ is the $\ MVUE\ $ of $\ \sigma^2\ $ .}
\end{split}
\end{equation*}
\begin{example*}[Ex. 9.7 , page 466-467]\ 
$\ Y_i \distas{iid}\text{Weibull}(m=2,\theta) \ \ ,\ \ i=1,2,\ldots,n$
\end{example*}
$$
f_{\theta}(y) = \left\{
	\begin{array}{lr}
	(\frac{2y}{\theta}) e^{-\frac{y^n}{\theta}}&\tab y>0 \\
	0&\tab \text{otherwise}
	\end{array}
	\right.
$$
\tab\textbf{Step 1:}
\begin{equation*}
\begin{split}
	\mathscr{L}(\theta ; y_1,y_2,\ldots,y_n) &= \prod\limits_{i=1}^n (\frac{2y_i}{\theta})e^{-\frac{y_i^2}{\theta}}\\
	& = (\frac{2}{\theta})^n\ e^{-\frac{1}{\theta}\sum\limits_{i=1}^n y_i^2}\prod\limits_{i=1}^n y_i\\
\end{split}
\end{equation*}
Thus $\ T=\sum\limits_{i=1}^nY_i^2 \ $ is a sufficient statistic for $\theta\ $ .\\
Note that the Kernel is $ \ exp\Big\{-\frac{1}{\theta}\sum\limits_{i=1}^nY_i^2\Big\}\ $ . We can also see this through Fisher-Neyman by choosing:
$$
	g(t;\theta) = (\frac{2}{\theta})^n\ e^{-\frac{t}{\theta}}\ \ \ \text{and}\ \ \ h(y_1,\ldots,y_n)=\prod\limits_{i=1}^n y_i\ \ \ \text{where\ } t=\sum\limits_{i=1}^ny_i^2
$$
\tab\textbf{Step 2:\ } Define $\ W_i = Y_i^2\ \ , \ \ i=1,2,\ldots,n\ $. Note that:
\begin{equation*}
\begin{split}
	\tab f_{_W}(w) &= f_{_Y}(\sqrt{w})\ \big|\frac{d\sqrt{w}}{dw} \big|\tab\text{using \emph{Transformation Method}}\\
	&=\left\{
	\begin{array}{lr}
	(\frac{2\sqrt{w}}{\theta})\ e^{-\frac{(\sqrt{w})^2}{\theta}}\cdot\frac{1}{2\sqrt{w}} &\tab \text{if }\ w>0 \\
	0&\tab \text{otherwise}
	\end{array}
	\right. \\
	&= \left\{
	\begin{array}{lr}
	\frac{1}{\theta}\ e^{-\frac{w}{\theta}} &\tab\text{if\ }\ w>0 \\
	0 &\tab \text{otherwise}
	\end{array}
	\right.
\end{split}
\end{equation*}
\tab Thus $\ W\sim Exp(\theta)\ $ and therefore 
$$
	\mathbb{E}(T) = \mathbb{E}(\sum\limits_{i=1}^n Y_i^2) = \sum\limits_{i=1}^n \mathbb{E}(Y_i^2) = \sum\limits_{i=1}^n \mathbb{E}(W_i) = n\theta\implies \mathbb{E}(\frac{T}{n})=\theta
$$
\tab\textbf{Step 3}
$$
	\mathbb{E}(\frac{T}{n}\ \big|\ T) = \frac{T}{n}\ \ \ \text{therefore} \ \ \ \ \frac{T}{n}=\frac{1}{n}\sum\limits_{i=1}^n Y_i^2\ \text{\ \ \ is the $\ MVUE\ $ of $\ \theta\ $ .}
$$
\begin{remark*}\emph{
Sufficient statistics can often be used to make a pivotal quantity , in the above example for instance,
$$
	\frac{2}{\theta} W\ \sim\ \chi^2_{(2)} \tab\text{(Exercise)}
$$
and hence 
$$\ \frac{2}{\theta}\sum\limits_{i=1}^n W_i = \frac{2}{\theta}\sum\limits_{i=1}^n Y_i^2 \sim \chi^2_{(2n)}\tab\text{i.e.\tab} \frac{2T}{\theta}\sim\chi^2_{2n}$$
This pivotal quantity can therefore be used to make \emph{exact confidence interval} for $\ \theta\ $. See example 9.10 , page 468, confidence interval made using sufficient statistic based on pivotal quantities often have the shortest possible length for a given confidence level .
}\end{remark*}


\newpage
\section{Lecture 11}
\textbf{Methods of Estimation:}
\tab A)\ Method of Maximum Likelihood (ML)\\
\tab B)\ Method of Moments

\subsection{Method of Maximum Likelihood (ML)}
\begin{definition*}
The \textbf{Maximum Likelihood Estimation (MLE)} of a parameter $\theta$ based on the realized values \mbox{$(y_1,y_2,\ldots,y_n)$} of a sample \mbox{$Y_1,Y_2,\ldots,Y_n$} is:
$$
	\hat{\theta}_{_{ML}} = \text{argmax\ }\mathscr{L}(\theta ; y_2,\ldots,y_n)
$$
\end{definition*}
Then we have a two step procedure for finding the $\hat{\theta}_{ML}$:\\
\textbf{Step 1:\ } Set up the likelihood $\ \mathscr{L}(\theta ; y_1,\ldots, y_n)\ $\\
\textbf{Step 2:\ } Find the maximizer of the likelihood when we have a random sample, which is the case in this course:
$$
	\mathscr{L}(\theta ; y_1,\ldots,y_n) = \prod\limits_{i=1}^n f_{\theta}(x_i)
$$
To find $\ \hat{\theta}_{ML}\ $ is often easier to work with \mbox{\ $\mathscr{L}(\theta ; y_1,\ldots,y_n) = \log(\mathscr{L}(\theta;y_1,\ldots,y_n))=\sum\limits_{i=1}^n \log(f_{\theta}(x_i))$}.\\
Note that $\log$ is a monotone increasing function, Thus:
$$
	\text{argmax } l\ (\theta ; y_1,\ldots,y_n) = \text{argmax }\mathscr{L}(\theta ; y_1,\ldots, y_n)
$$
\begin{example*}
$X_i\distas{iid}\text{Bernoulli}(p)\ \ ,\ \ i=1,2,\ldots,n$
\end{example*}
\tab$
 P(X=x) = p^x(1-p)^{1-x}\ \ ,\ \ x=0,1$\\
\textbf{Step 1:\tab[0.5cm]}$\ \mathscr{L}(p ; x_1,\ldots,x_n) = p^{\sum\limits_{i=1}^n x_i}(1-p)^{n-\sum\limits_{i=1}^n x_i}$\\
\textbf{Step 2:}
\begin{equation*}
\begin{split}
l(p ; x_1,\ldots,x_n) &= \log\ (\ \mathscr{L}(p ; x_1,\ldots, x_n)\ )\\
&= (\sum\limits_{i=1}^n x_i) \log p + (n-\sum\limits_{i=1}^n)\ \log(1-p)\\
\frac{\partial}{\partial p} l(p ; x_1,\ldots, x_n) &= \frac{\sum\limits_{i=1}^n x_i}{p} - \frac{n-\sum\limits_{i=1}^nx_i}{1-p}
\end{split}
\end{equation*}
Let $\ t=\sum\limits_{i=1}^n x_i\ $ . The $\ \hat{p}_{ML}\ $ is then the solution to 
$$
	\frac{t}{\hat{p}_{ML}}-\frac{n-t}{1-\hat{p}_{ML}} = 0 \iff \frac{t}{n-t} = \frac{\hat{p}_{ML}}{1-\hat{p}_{ML}} \iff \hat{p}_{ML} = \frac{t}{n} = \frac{1}{n} \sum\limits_{i=1}^n X_i = \hat{p}_n
$$
Note that
$$
	\frac{\partial^2 l}{\partial p^2} = -\frac{\sum\limits_{i=1}^n x_i}{p^2} - \frac{n-\sum\limits_{i=1}^nx_i}{(1-p)^2} < 0
$$
Thus $\ \hat{p}_{_{ML}}\ $ is the maximizer of $\ l\ (p ; x_1,\ldots,x_n)$ . 
\begin{example*}
$X_i\distas{iid}N(\mu,\sigma^2)\ \ , \ \ i=1,2,\ldots,n$
\end{example*}
\textbf{Step 1:}
$$
	\mathscr{L}(\mu,\sigma^2 ; x_1,\ldots,x_n) = (\frac{1}{\sqrt{2\pi}})^n\ exp\Big\{-\frac{1}{2\sigma^2}\sum\limits_{i=1}^n(x_i-\mu)^2\Big\}
$$
$$
\text{hence\ \ \ }l\ (\mu,\sigma^2,x_1,\ldots,x_n) = -\frac{n}{2}\log \sqrt{2\pi} - \frac{n}{2}\log \sigma^2 - \frac{1}{2\sigma^2} \sum\limits_{i=1}^n (x_i-\mu)^2
$$

\textbf{Step 2:}
\begin{equation*}
\begin{split}
\frac{\partial\ l(\mu,\sigma^2)}{\partial \mu} &= -\frac{1}{2\sigma^2}\sum\limits_{i=1}^n -2(x_i-\mu)\\
\frac{\partial\ l(\mu,\sigma^2)}{\partial\sigma^2} &= -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4}\sum\limits_{i=1}^n(x_i-\mu)^2\\
\text{the MLE , }\ & \hat{\mu}_{ML}\And\hat{\sigma^2}_{ML}\ \text{are therefore solutions to:}\\
&\left\{
	\begin{array}{lr}
	-\frac{1}{2\ \hat{\sigma}^2_{ML}} \ \sum\limits_{i=1}^n -2(x_i-\hat{\mu}_{ML}) = 0\\
	-\frac{n}{2\ \hat{\sigma}^2_{ML}} + \frac{1}{2\ \hat{\sigma}^4_{ML}}\sum\limits_{i=1}^n (x_i - \hat{\mu}_{ML}) = 0
	\end{array}
	\right.\\
	\text{From the }1^{st}& equation we find:\\
	&\sum\limits_{i=1}^n (x_i-\hat{\mu}_{ML}) = 0 \implies \sum x_i = n\hat{\mu}_{ML} \implies \boxed{\hat{\mu}_{{_ML}} = \bar{x}_n}\\
	\text{If we plug in for } &\hat{\mu}_{_{ML}}\text{ in the second equation, we find}\\ 
	 -\frac{n}{\cancel{2}\cancel{\hat{\sigma}^2_{ML}}} + \frac{1}{\cancel{2}\hat{\sigma}^{\cancel{4}}_{ML}} &\sum\limits_{i=1}^n (x_i - \bar{x}_n)^2 = 0\\
	 \implies\tab
	 \frac{1}{\hat{\sigma}^2_{ML}}&\sum\limits_{i=1}^n (x_i - \bar{x}_n)^2 = n\\
	 \implies \tab[1.7cm] &\boxed{\hat{\sigma}^2_{_{ML}} = \frac{1}{n}\sum\limits_{i=1}^n (x_i - \bar{x}_n)^2}
\end{split}
\end{equation*}
To show that this is a maximizer we should check that:
$$
\left[
	\begin{array}{lr}
	\frac{\partial^2 l}{\partial\mu^2} & \frac{\partial^2 l}{\partial\mu \partial \sigma^2}\\
	\frac{\partial^2 l}{\partial \sigma^2 \partial\ \mu} & \frac{\partial^2 l}{\partial (\sigma^2)^2}
	\end{array}
	\right] \ \ \text{is a negative-definite matrix .\tab[3cm]}
$$
This is not a hard task, but it is not required in this course. We only check the $2^{nd}$ derive for cases that there is only one unknown parameter.
\begin{example*}
$X_i \distas{iid} Unif(0,\theta)\ \ ,\ \ i=1,2,\ldots,n$
\end{example*}
$$
f_{_{\theta}}(x) = \left\{ \begin{array}{lr}
	\frac{1}{\theta} &\tab 0<x<\theta\\
	0 &\tab \text{otherwise}
\end{array}
	\right.
$$
In other words, $\ f_{_{\theta}}(x) = \frac{1}{\theta}\cdot I_{[0,\theta]}(x)\ $ where \mbox{$I_{_A}(x) = \left\{ \begin{array}{lr}
		1 &\tab \text{if } x\in\ A\\
		0 &\tab \text{if } x\notin\ A
\end{array}\right.$}
\textbf{Step 1:}
$$
	\mathscr{L}(\theta ; x_11,\ldots,x_n) = \prod\limits_{i=1}^n f_{_{\theta}}(x_i) = \frac{1}{\theta^n}\prod\limits_{i=1}^n I_{_{[0,\theta]}}(x_i)
$$
Now note that
$$
	\prod\limits_{i=1}^n I_{[0,\theta]}(x_i) = I_{_{[0,\theta]}}\underset{1\leq i \leq n}{(\text{max }x_i)}
$$
Since $\ 0\leq x_i\leq \theta\ \ , \ \ i=1,2,\ldots,n\ $ if and only if $\ 0\leq\underset{1\leq i\leq n}{\text{max }x_i}\leq \theta$ . Therefore:
$$
	\mathscr{L}(\theta ; x_1,\ldots,x_n) = \frac{1}{\theta^n}\ I_{_{[0,\theta]}}(\underset{1\leq i \leq n}{\text{max }x_i})
$$
\textbf{Step 2:\ } We note that the likelihood is a monotone decreasing function of $\theta$. That is why, the max of $\mathscr{L}(\theta ; x_1,\ldots,x_n)$ happens when $\theta$ takes its smallest possible value. Since that $0\leq \underset{1\leq i \leq n}{\text{max }x_i}\leq \theta$ , the smallest value for $\theta$ is $\underset{1\leq i\leq n}{max\ } x_i$ , thus \mbox{$\hat{\theta}_{ML} = \underset{1\leq i \leq n}{max\ } x_i$}\ .\\
The method of $ML$ has both the intuitive and theoretical appeal.\\
\textbf{Intuitive Appeal:\ }
The $ML$ method is based on the idea that \emph{"What I have observed is what should have expected to observe"}. In other words, we observe the most likely scenario. Now given a sample, we choose the unknown parameter such that what we have observed has its maximum possible chance.\\
\textbf{Theoretical Appeal:\ }
\begin{enumerate}[*]
\item \textbf{Consistency:} $MLE$ is a consistent estimator under rather mild conditions.
\item \textbf{Asymptotic Normality:} $\ \sqrt{n} (\hat{\theta}_{ML} - \theta)\distas{app} N(0, I^{-1}(\theta))$ for large n where \mbox{$I(\theta) = \mathbb{E}\big\{ [\frac{\partial}{\partial \theta} \log f_{\theta}(X)]^2 \big\}$} , the Fisher information.\\
We can therefore make confidence interval for $\theta$ easily if we use $\hat{\theta}_n = \hat{\theta}_{ML}$ as the estimator.
\item \textbf{Asymptotic Efficiency:} $MLE$ is the most concentrated estimator about its estimand among a considerably large class of reasonable estimators called \textbf{\emph{"regular estimators"}}.
\item \textbf{Invariance: } If $\hat{\theta}_{ML}$ is the $MLE$ of $\theta$ , then $g(\hat{\theta}_{ML})$ is the $MLE$ of $g(\theta)$.\\
This property simplifies life a bit. Since if we find the $MLE$ of $\theta$ , then we have found $MLE$ of any function of $\theta$. 
\end{enumerate}
























































\newpage
\section{Lecture 12}



\end{document}
