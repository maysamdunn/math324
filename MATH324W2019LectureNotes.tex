\documentclass[14pt,twoside,a4paper,fleqn]{article}
\usepackage{amsmath,mathtools,latexsym,semantic,amsthm,cancel,stackengine,graphicx}
\theoremstyle{plain}
%theorems and lemmas with numbering
\newtheorem{theorem}{Theorem}[section]
\newtheorem{example}{Example}[section]
\newtheorem{lemma}{Lemma}[theorem]
\newtheorem{remark}{Remark}[section]
\newtheorem{definition}{Definition}[section]


%theorem and lemmas without numbering
\newtheorem*{theorem*}{Theorem}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{remark*}{Remark}
\newtheorem*{definition*}{Definition}
\newtheorem*{example*}{Example}

%commands:
\newcommand\myeq{\mathrel{\stackrel{\makebox[0pt]{\mbox{\normalfont\tiny def}}}{=}}}

%for distribution
\makeatletter
\newcommand{\distas}[1]{\mathbin{\overset{#1}{\kern\z@\sim}}}%

\usepackage{parskip}
\setlength{\parindent}{14pt}

\usepackage{setspace}
\doublespacing
% \onehalfspacing





\title{MATH324 (Statistics) -- Lecture Notes\\McGill University}
\date{Winter 2019}
\author{Masoud Asgharian}


\begin{document}

\maketitle

%newcommands:
%       newcommand{\COMMAND}{REPLACEDCOMMAND}
\newcommand{\nexists}{\not\exists}
%	tab command
\newcommand\tab[1][1cm]{\hspace*{#1}}

\tableofcontents
\newpage
\section{Lecture 0}
\section{Lecture 1}

\newpage
%Lecture 3
\section{Lecture 2}
%Markov's Inequality
\subsection{Markov's Inequality}
Let $X$ be a random variable and $h$ be a \textbf{non-negative} function; ie:
	\begin{equation*}
	h:R->R^{+}\ \cup\ \{0\}\tab[0.5cm]=\tab[0.5cm] [0,\infty)
	\end{equation*}
Suppose $E(h(X))\ < \infty$ ,then for some $\lambda > 0$ , we have:
	\begin{equation}\label{markov}
	P(h(X) \geq \lambda)\ \leq\ \frac{E[h(X)]}{\lambda}
	\end{equation}
\begin{proof}
Suppose $X$ is a continuous random variable:
	\begin{equation*}
	\begin{split}
	E[h(x)] &= \int_{x} h(x)f_{_X}(x)dx \\
		&=\Big(\int_{x:h(x)\geq\lambda}h(x)f_{_X}(x)dx + \int_{x:h(x) < \lambda}h(x)f_{_X}(x)dx\Big) \\
		&\geq \int_{x:h(x)\geq\lambda}h(x)f_{_X}(x)dx\tab[4cm] \underline{since}\ h\geq 0\\
		&\geq\lambda \int_{x:h(x\geq\lambda}f_{_X}(x)dx = \lambda\ P(h(X)\geq \lambda)\\
		\implies P(h(X)\geq \lambda) \leq \frac{E(h(X))}{\lambda}
	\end{split}
	\end{equation*}
\emph{The proof for the discrete case is similar.}
\end{proof} 

%Tchebushev's Inequality
\subsection{Tchebyshev's Inequality}
\emph{Tchebyshev's Inequality} is a special case of Markov's Inequality. \mbox{Consider $h(x) = (x-\mu)^2$}, then:
	\begin{equation*}
	\begin{split}
	P(\vert X-\mu \vert \geq \lambda) &= P((X-\mu)^2 \geq \lambda^2)\\
		&\leq \frac{E[(X-\mu)^2]}{\lambda^2}\tab[2cm]if\ E[(X-\mu)^2]<\infty
	\end{split}
	\end{equation*}
Let $\mu = E(X)$, then $E[(X-\mu)^2] = Var(X)$ denoted by $\sigma_{_X}^2$. We therefore have:
	\begin{equation}\label{tchev_sigma}
		P(\vert X-\mu_{_X} \vert \geq \lambda) \leq \frac{\sigma_{_X}^2}{\lambda^2}\tab[2cm] where\ \mu_{_X} = E(X)
	\end{equation}
Now consider $\lambda= K\sigma_x$ where $K$ is a known number. Then:
	\begin{equation}\label{tchev}
		P(\vert X-\mu_{_X}\vert \geq K\sigma_{_X}) \geq \frac{\sigma_{_X}^2}{K^2\sigma_{_X}^2}=\frac{1}{K^2}
	\end{equation}

This is called \textbf{Tchbyshev's Inequality}. 
\begin{example}
Suppose $K=3$.\hfill\newline
$$P(\vert X-\mu_x\vert \geq 3\ \sigma_{_X}) \leq \frac{1}{9}$$
In other words, at least $88\%$ of the observations are within $3$ standard deviation from the population mean.
\end{example}

Going back to the our example:
$$X_i \sim (\mu, 1)\tab[0.5cm],\tab[0.5cm] \bar X_n = \frac{1}{n}\sum_{i=1}^{n} X_i$$
We want to study $P(\epsilon \geq \delta) = P(\vert \bar{X}_n - \mu\vert \geq \delta)$, first we note that: $$E(X_i)=\mu\ \ \ ,\ i=1,2,\ldots,n$$
Then:
	\begin{equation*}
	\begin{split}
		E(\bar{X}_n) &= E\big(\frac{1}{n}\sum_{i=1}^{n}X_i \big)=\frac{1}{n}\sum_{i=1}^{n}E(X_i)\\
		&= \frac{1}{n}\sum_{i=1}^{n} \mu =\frac{1}{n}(n\mu)= \frac{1}{\cancel{n}}.(\cancel{n}\mu)\\
		&= \mu\tab[13cm] (*)
	\end{split}
	\end{equation*}
Thus, using (\ref{tchev_sigma}) we have:
	$$P(\vert\bar{X}_n - \mu\vert \geq \delta ) \leq \frac{Var(\bar{X}_n)}{\delta^2}$$
Now:
	\begin{equation*}
	\begin{split}
	Var(\bar{X}_n) &= Var\big(\frac{1}{n} \sum_{i=1}^{n} X_i \big) = \frac{1}{n^2}Var\big(\sum_{i=1}^{n}X_i \big)\\
	&=\frac{1}{n^2}\Big[\sum_{i=1}^{n} Var(X_i) +\sum_{1\leq i<j\leq n}\ \sum_{1\leq i<j\leq n} Cov(X_i, X_j) \Big]\ \ \ using\ Thm\ 5.12(b)-page\ 271\\
	&=\frac{1}{n^2}\sum_{i=1}^{n}Var(X_i)\tab[2cm]since\ \coprod_{1}^{n}X_i\\
	&=\frac{1}{n^{\cancel{2}}} \cancel{n} Var(X) = \frac{Var(X)}{n} \tab since\ x_{i}s\ are\ identically\ distributed\\
	&=\frac{\delta_X^2}{n}\tab[12cm](**)
	\end{split}
	\end{equation*}
	In our case $X\sim N(\mu, 1)$ so $Var(X) = \delta_X^2 = 1$. Thus $Var(\bar{X}_n) = \frac{1}{n}$
	
\begin{remark*}
$X\coprod Y \implies Cov(X,Y)=0$. Note that:
$$X \coprod Y \implies E\big[g_1(X)g_2(Y)\big] = E[g_1(X)].E[g_2(Y)]$$
in particular:
$$X \coprod Y \implies E\big[XY\big] = E[X].E[Y]$$
on the other hand:
$$Cov(X,Y) = E[XY] - E(X)E(Y)$$
thus: 
$$X\coprod Y\implies Cov(X,Y)=0.$$
\end{remark*}
recall that $X\coprod Y$ means X and Y are independent, i.e. \mbox{$f_{_{X,Y}}(x,y)=f_{_X}(x)f_{_Y}(y)$} where $f_{_{X,Y}},f_{_X} and f_{_Y}$ represent respectively the %%missed words%%%/////TODO\\




We therefore have:
\begin{equation}\
P(\vert\bar{X}_n - \mu\vert \geq \delta) \leq \frac{1}{n\delta^2}
\end{equation}
Using $(4)$ and  the sample size, $n$, we can find an upper bound for the proportion of deviations which are greater than a given threshold $\delta$.\hfill\newline
We can also use $(4)$ for \underline{Sample Size Deterministic}:\\
Suppose $\delta$ is given and we want $P(\vert\bar{X}_n - \mu\vert \geq \delta) \leq \beta$ where $\beta$ is also given. Then setting $\frac{1}{n\delta^2} = \beta$, we can estimate $n\approx\frac{1}{\beta\delta^2}$.\hfill\newline
%Application to voting
\subsection{Application to Voting}
Define $X_i = \begin{cases}
		1 & \text{NDP}\\
		0 & \text{otherwise}
		\end{cases}$ . Associated to each eligible voter in Canada we have a binary variable X. Let $p=P(X=1)$. So $p$ represents the proportion of eligible voters who favor $NDP$. Of interest is often estimation of $p$. Suppose we have a sample of size $n$, $X_1,X_2,\ldots,X_n$.\\
		$\bar{X}_n = \frac{1}{n}\sum_{i=1}^{n}X_i$ is the sample proportion; The counterpart of $p$ which nat be denoted by $\hat{p}$. Note that:
		$$\mu_{_X} = E(X) = 1 \times P(X=1) + 0\times P(X=0) = 1-p + 0\times (1-p)=p$$
		and:
		$$E(X^2) = 1^2 \times P(X=1) + 0^2 \times P(X=0) = 1-p + 0\times (1-p) = p$$
From (*) and (**) we find that :
	$$E(\hat{p}_n) = E(\bar{X}_n) \mu_{_X} = p$$
and:
	$$Var(\hat{p}_n) = E(\bar{X}_n) = \frac{Var(X)}{n} = \frac{\sigma_X^2}{n} = \frac{p(1-p)}{n}$$
Thus using (\ref{tchev_sigma}), we have:
$$P(\vert\hat{p}_n - p\vert \geq \delta) \leq \frac{Var(\hat{p}_n)}{\delta^2} = \frac{p(1-p)}{n\delta^2}$$
Note that the above bound on the probability of derivation depends on $p$ which is \emph{unknown}. We however notice that $p(1-p) \leq \frac{1}{4}$ .\\
Define $\zeta(x) = x(1-x)\ for\ 0 < x < 1$. Then:
	\begin{equation*}
	\begin{split}
	&\zeta^{'}(x) = 1-2x \implies \zeta^{'}(x)= 0 \implies x = \frac{1}{2} \\
	&\zeta^{"}(\frac{1}{2}) = -2 \implies x = \frac{1}{2}\tab \text{which is a \textbf{maximizer}}\\
	&\zeta(\frac{1}{2}) = \frac{1}{2}(1-\frac{1}{2})=\frac{1}{4}
	\end{split}
	\end{equation*}
	(Note that $\zeta^{"}(x) = -2$ for all $0 < x < 1$)
	
	
	%The graph is missing
	
	
We therefore find:
	\begin{equation}\label{voter}
	P(\vert\hat{p}_n - p\vert \geq \delta) \leq \frac{1}{4n\delta^2}
	\end{equation}
Using (\ref{voter}) and a given sample size $n$ we can find an upper bound for the probability of derivation by $\delta$ and the amount for any given $\delta$.\newline
We can also use (\ref{voter}) for \underline{sample size deterministic} for a size bound $
\beta$ and derivative $\delta$ as follows:
$$\frac{1}{4n\delta^2}= \beta\tab \implies \tab n \geq \frac{1}{4\beta\delta^2}$$
This is of course conservative since $p(1-p)\leq \frac{1}{4}$.



\newpage
\section{Lecture 3}
\subsection{MSE} 
\textbf{MSE}: To study estimation error we started by studying $P(\vert \hat{\Theta}_n - \Theta\vert > \delta)$ , deviation above a given threshold $\delta$, by bounding this probability. One may take a different approach by studying average Euclidean distance, i.e. $E[\ \vert\hat{\Theta}_n - \Theta\vert ^2]$, which denoted by \textbf{MSE($\hat{\Theta}_n$)}.\newline
We note that if $\Theta = E(\hat{\Theta}_n)$, i.e. $\hat{\Theta}_n$ is an unbiased estimation of $\Theta$, then: 
$$MSE(\hat{\Theta}_n) = E[\vert\hat{\Theta}_n - \Theta\vert^2] = E[(\hat{\Theta}_n - \mu _{n_{\Theta _n}})^2] = Var(\hat{\Theta}_n)$$
Now recall that $Var(X) = 0\  \implies \ P(X = \text{constant}) = 1$ which essentially means random variable X is a constant.\\
The same comment applies to MSE($\hat{\Theta}_n$). We want to find the closest estimator $\hat{\Theta}_n$ to $\Theta$ which means that we want to minimize $E[(\hat{\Theta}_n - \Theta)^2]$ over all possible estimators, ideally at least the above comment tells us that in real applications we cannot expect to find an estimator whose MSE is equal to zero. Let's try to understand the MSE a bit more:
\begin{equation*}
\begin{split}
\text{MSE}(\hat{\Theta}_n) &= E[(\hat{\Theta}_n - \Theta)^2]\\
	&= E\Big[\Big(\big(\hat{\Theta}_n - E(\hat{\Theta}_n)\big) + \big(E(\hat{\Theta}_n) - \Theta\big) \Big)^2 \Big]\\
	&= E\big[\big(\hat{\Theta}_n - E(\hat{\Theta}_n)\big)^2\big] + \big(E(\hat{\Theta}_n) - \Theta\big)^2 + 2 \cdot E\big[\big(\hat{\Theta}_n - E(\hat{\Theta}_n)\big)\big]\cdot \big(E(\hat{\Theta}_n) - \Theta\big)\\
	&= E\big[(\hat{\Theta}_n - E(\hat{\Theta}_n))^2\big] + E\big[\overbrace{(E(\hat{\Theta}_n) - \Theta)^2}^{\text{not a r.v.}}\big]
		+2\cdot E\Big[\overbrace{\big(E(\hat{\Theta}_n) - \Theta)}^{\text{not a r.v.}}\big)\cdot \big(\hat{\Theta}_n - E(\hat{\Theta}_n)\big) \Big]\\
		&= Var(\hat{\Theta}_n) + \big[\overbrace{E(\hat{\Theta}_n) - \Theta}^{\text{Bias$(\hat{\Theta}_n)$}}\big]^2 + 2\cdot Bias(\hat{\Theta}_n)\cdot \overbrace{E[(\hat{\Theta}_n - E(\hat{\Theta}_n))]}^{E(\hat{\Theta}_n) - E(\hat{\Theta}_n) = 0}\\
		&= Var(\hat{\Theta}_n) + Bias^2(\hat{\Theta}_n)
\end{split}
\end{equation*}
Roughly speaking, \textbf{bias} measures how far off the target we hit on the average while \textbf{variance} measures how much fluctuation our estimator may show from one sample to another.

\subsection{Unbiased Estimators}
In almost all real applications, the class of possible estimators for an \textbf{ESTIMANAL} is huge and the best estimator, i.e. the one that minimizes MSE no matter what the value of the \textbf{ESTIMANAL} is, almost never exists. Thus we try to reduce the class of potential estimators by improving a plausible restriction, for example Bias$(\hat{\Theta}_n) = 0$.

\begin{definition*}An estimator  $\hat{\Theta}_n$ of an \textbf{ESTIMANAL} $\Theta$ is said to be \textbf{unbiased} if $E(\hat{\Theta}_n) = \Theta$ , for all possible values of $\Theta$.
\end{definition*}
\begin{example}
$X_i\distas{iid}  N(\mu, \sigma^2)\tab i=1,2,\ldots,n$\\
Suppose both $\mu$ and $\sigma^2$ are unknown. Consider $\bar{X}_n = \frac{1}{n}\sum_{i=1}^{n} X_i$.
$$E(\bar{X_n}) = E(\frac{1}{n}\sum_{i=1}^{n} X_i) = \frac{1}{n}\sum_{i=1}^n \overbrace{E(X_i)}^{\mu} = \frac{1}{\cancel{n}}\cdot \cancel{n}\mu = \mu$$
\end{example}
Thus $\bar{X_n}$ is an unbiased estimator of $\mu$. As for the MSE$(\bar{X_n})$, we need to find Var$(\bar{X_n})$.
\begin{equation*}
\begin{split}
Var(\bar{X_n})&= Var\big(\frac{1}{n}\sum_{i=1}^{n} X_i\big) = \frac{1}{n^2} Var(\sum_{i=1}^n X_i)\\
	&= \frac{1}{n^2}\Big[\sum_{i=1}^n Var(X_i) + 2\cdot\sum\sum_{1\leq i < j \leq n} \overbrace{Cov(X_i,X_j)}^{0} \Big]\tab[2cm] \text{Theorem 5.12(b) - page 271}\\
	&= \frac{1}{n^2}\sum_{i=1}^n Var(X_i)\tab[8cm] \coprod_{i=1}^n X_i\\
	& = \frac{1}{n^2}\sum_{i=1}^{n} \sigma^2 = \frac{1}{n^{\cancel{2}}}\cdot\cancel{n}\sigma^2 = \frac{\sigma^2}{n}\tab[5cm]\ \ \text{identically distributed}\\
&\implies MSE(\bar{X_n}) = Var(\bar{X_n}) + \overbrace{Bias^2(\bar{X_n})}^{0}=Var(\bar{X_n}) = \frac{\sigma^2}{n}
\end{split}
\end{equation*}
An inspection of the above calculation shows that for unbiased $\mu$ we only require a common mean $\mu$ while for calculating the variance we would only require a common variance $\sigma^2$ and orthogonality, i.e:
$$Cov(X_i,X_j) = 0\tab \text{where}\ i\neq j$$
Suppose $X_1,\ldots,X_n$ have the same mean value $\mu$. Then:
$$E(\bar{X_n}) = E(\frac{1}{n}\sum_{i=1}^n X_i) = \frac{1}{n}\sum_{i=1}^n E(X_i) = \frac{1}{\cancel{n}}\cancel{n}\mu=\mu$$
Suppose further that $ X_1,\ldots ,X_n$ have the same variance $\sigma^2$ and \mbox{$Cov(X_i,X_j) = 0,\ i\neq j$}. Then:
\begin{equation*}
\begin{split}
Var(\bar{X_n}) &= Var(\frac{1}{n}\sum_{i=1}^n X_i) = \frac{1}{n^2} Var(\sum_{i=1}^n X_i)\\
	&= \frac{1}{n^2}\Big[\ \sum_{i=1}^nVar(X_i) + 2\sum\sum_{1\leq i < j \leq n} Cov(X_i, X_j)\ \Big]\tab[3cm] \text{Theorem 5.12(b) - Page 271}\\
	&= \frac{1}{n^2}\sum_{i=1}^n Var(X_i)\tab[5cm] \text{Orthogonality:\ i.e. $Cov(X_i,X_j) = 0\ \ if\ i\neq j$}\\
	&=\frac{1}{n^2}\sum_{i=1}^n \sigma^2 = \frac{1}{n^{\cancel{2}}}\cancel{n}\sigma^2 = \frac{\sigma^2}{n} \tab[4cm] \text{having the same variance}\\
	&\implies MSE(\bar{X_n}) = Var(\bar{X_n}) = \frac{\sigma^2}{n}
\end{split}
\end{equation*}
If $X_1,\ldots,X_n$ have the same mean value and variance and they are orthogonal.
\subsection{Stein's Paradox}
We will learn later that if $X_i\distas{iid} N(\mu,\sigma^2)$ then \mbox{$\bar{X_n} = \frac{1}{n}\sum_{i=1}^n X_i$} has many optimal properties. A paradox due to Charles Stein, however, shows that such a nice optimal properties are not preserved in higher dimensions. In fact if:
$$X_i\distas{iid}N(\mu_{_X},1),\ \ Y_i\distas{iid}N(\mu_{_Y},1)\ \text{and  }Z_i\distas{iid}(\mu_{_Z},1)$$
then, we can find the biased estimators of 
$\begin{pmatrix} \mu_{_X}\\\mu_{_Y}\\ \mu_{_Z} \end{pmatrix}$
which are closer to 
$\begin{pmatrix} \mu_{_X}\\\mu_{_Y}\\ \mu_{_Z} \end{pmatrix}$ 
than 
$\begin{pmatrix}\bar{X_n}\\\bar{Y_n}\\\bar{Z_n} \end{pmatrix}$ 
for any 
$\begin{pmatrix} \mu_{_X}\\\mu_{_Y}\\ \mu_{_Z} \end{pmatrix}$.
We may then say that 
$\begin{pmatrix}\bar{X_n}\\\bar{Y_n}\\\bar{Z_n} \end{pmatrix}$
 is an \mbox{\textbf{\underline{inadmissible estimator}} of 
 $\begin{pmatrix} \mu_{_X}\\\mu_{_Y}\\ \mu_{_Z} \end{pmatrix}$.}
 \subsection{Admissibility}
 An estimator $\hat{\Theta}$ is called admissible if there is no estimator $\tilde{\Theta}$ such that:
 $$MSE(\tilde{\Theta}) \leq MSE(\hat{\Theta})\tab[3cm]\text{for all possible values of $\Theta$}$$
and this inequality is strict for some values of $\Theta$.\newline
What this example tells us is that by allowing MISSING of bias we may be able to reduce variance considerably and hence find an estimator which is closer to the target than the most natural unbiased estimator. Note that this phenomena happens only when the dimension is at least 3.














\newpage
\section{Lecture 4}












\newpage
\section{Lecture 5}
\section{Lecture 6}
\section{Lecture 7}
\section{Lecture 8}
\section{Lecture 9}
\section{Lecture 10}
\section{Lecture 11}
\section{Lecture 12}



\end{document}
