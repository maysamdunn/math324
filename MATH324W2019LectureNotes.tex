\documentclass[14pt,twoside,a4paper,fleqn]{article}
\usepackage{amsmath,mathtools,latexsym,forest,semantic,amsthm,cancel,stackengine,dcolumn,pxfonts,graphicx,mathrsfs}
\usepackage{enumerate,tikz}
\usetikzlibrary{trees}
\theoremstyle{plain}
%theorems and lemmas with numbering
\newtheorem{theorem}{Theorem}[section]
\newtheorem{example}{Example}[section]
\newtheorem{lemma}{Lemma}[theorem]
\newtheorem{remark}{Remark}[section]
\newtheorem{definition}{Definition}[section]


%theorem and lemmas without numbering
\newtheorem*{theorem*}{Theorem}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{remark*}{Remark}
\newtheorem*{definition*}{Definition}
\newtheorem*{example*}{Example}

%commands:
\newcommand\myeq{\mathrel{\stackrel{\makebox[0pt]{\mbox{\normalfont\tiny def}}}{=}}}

\newcolumntype{d}[1]{D{.}{\cdot}{#1} }

%for distribution
\makeatletter
\newcommand{\distas}[1]{\mathbin{\overset{#1}{\kern\z@\sim}}}%

\usepackage{parskip}
\setlength{\parindent}{14pt}

\usepackage{setspace}
\doublespacing
% \onehalfspacing


%for the tree:
\forestset{
  L1/.style={draw=black,},
  L2/.style={,edge={,line width=0.8pt}},
}

%	\begin{tikzpicture}
%	\node {root}[edge from parent fork right,grow=east]
%	child {node {left}}
%	child {node {right}
%	child {node {child}}
%	child {node {child}}
%	};
%	\end{tikzpicture}



\title{MATH324 (Statistics) -- Lecture Notes\\McGill University\\Prof. Masoud Asgharian}
\date{Winter 2019}
\author{Sam K.H.Targhi-Dunn\\sam.targhi@mail.mcgill.ca}


\begin{document}

\maketitle

%newcommands:
%       newcommand{\COMMAND}{REPLACEDCOMMAND}
%\newcommand{\nexists}{\not\exists}
%	tab command
\newcommand\tab[1][1cm]{\hspace*{#1}}

\tableofcontents
\newpage
\section{Lecture 1}

\newpage
%Lecture 3
\section{Lecture 2}
%Markov's Inequality
\subsection{Markov's Inequality}
Let $X$ be a random variable and $h$ be a \textbf{non-negative} function; ie:
	\begin{equation*}
	h:R->R^{+}\ \cup\ \{0\}\tab[0.5cm]=\tab[0.5cm] [0,\infty)
	\end{equation*}
Suppose $E(h(X))\ < \infty$ ,then for some $\lambda > 0$ , we have:
	\begin{equation}\label{markov}
	P(h(X) \geq \lambda)\ \leq\ \frac{E[h(X)]}{\lambda}
	\end{equation}
\begin{proof}
Suppose $X$ is a continuous random variable:
	\begin{equation*}
	\begin{split}
	E[h(x)] &= \int_{x} h(x)f_{_X}(x)dx \\
		&=\Big(\int_{x:h(x)\geq\lambda}h(x)f_{_X}(x)dx + \int_{x:h(x) < \lambda}h(x)f_{_X}(x)dx\Big) \\
		&\geq \int_{x:h(x)\geq\lambda}h(x)f_{_X}(x)dx\tab[4cm] \underline{since}\ h\geq 0\\
		&\geq\lambda \int_{x:h(x\geq\lambda}f_{_X}(x)dx = \lambda\ P(h(X)\geq \lambda)\\
		\implies P(h(X)\geq \lambda) \leq \frac{E(h(X))}{\lambda}
	\end{split}
	\end{equation*}
\emph{The proof for the discrete case is similar.}
\end{proof} 

%Tchebushev's Inequality
\subsection{Tchebyshev's Inequality}
\emph{Tchebyshev's Inequality} is a special case of Markov's Inequality. \mbox{Consider $h(x) = (x-\mu)^2$}, then:
	\begin{equation*}
	\begin{split}
	P(\vert X-\mu \vert \geq \lambda) &= P((X-\mu)^2 \geq \lambda^2)\\
		&\leq \frac{E[(X-\mu)^2]}{\lambda^2}\tab[2cm]if\ E[(X-\mu)^2]<\infty
	\end{split}
	\end{equation*}
Let $\mu = E(X)$, then $E[(X-\mu)^2] = Var(X)$ denoted by $\sigma_{_X}^2$. We therefore have:
	\begin{equation}\label{tchev_sigma}
		P(\vert X-\mu_{_X} \vert \geq \lambda) \leq \frac{\sigma_{_X}^2}{\lambda^2}\tab[2cm] where\ \mu_{_X} = E(X)
	\end{equation}
Now consider $\lambda= K\sigma_x$ where $K$ is a known number. Then:
	\begin{equation}\label{tchev}
		P(\vert X-\mu_{_X}\vert \geq K\sigma_{_X}) \geq \frac{\sigma_{_X}^2}{K^2\sigma_{_X}^2}=\frac{1}{K^2}
	\end{equation}

This is called \textbf{Tchbyshev's Inequality}. 
\begin{example}
Suppose $K=3$.\hfill\newline
$$P(\vert X-\mu_x\vert \geq 3\ \sigma_{_X}) \leq \frac{1}{9}$$
In other words, at least $88\%$ of the observations are within $3$ standard deviation from the population mean.
\end{example}

Going back to the our example:
$$X_i \sim (\mu, 1)\tab[0.5cm],\tab[0.5cm] \bar X_n = \frac{1}{n}\sum_{i=1}^{n} X_i$$
We want to study $P(\epsilon \geq \delta) = P(\vert \bar{X}_n - \mu\vert \geq \delta)$, first we note that: $$E(X_i)=\mu\ \ \ ,\ i=1,2,\ldots,n$$
Then:
	\begin{equation*}
	\begin{split}
		E(\bar{X}_n) &= E\big(\frac{1}{n}\sum_{i=1}^{n}X_i \big)=\frac{1}{n}\sum_{i=1}^{n}E(X_i)\\
		&= \frac{1}{n}\sum_{i=1}^{n} \mu =\frac{1}{n}(n\mu)= \frac{1}{\cancel{n}}.(\cancel{n}\mu)\\
		&= \mu\tab[13cm] (*)
	\end{split}
	\end{equation*}
Thus, using (\ref{tchev_sigma}) we have:
	$$P(\vert\bar{X}_n - \mu\vert \geq \delta ) \leq \frac{Var(\bar{X}_n)}{\delta^2}$$
Now:
	\begin{equation*}
	\begin{split}
	Var(\bar{X}_n) &= Var\big(\frac{1}{n} \sum_{i=1}^{n} X_i \big) = \frac{1}{n^2}Var\big(\sum_{i=1}^{n}X_i \big)\\
	&=\frac{1}{n^2}\Big[\sum_{i=1}^{n} Var(X_i) +\sum_{1\leq i<j\leq n}\ \sum_{1\leq i<j\leq n} Cov(X_i, X_j) \Big]\ \ \ using\ Thm\ 5.12(b)-page\ 271\\
	&=\frac{1}{n^2}\sum_{i=1}^{n}Var(X_i)\tab[2cm]since\ \coprod_{1}^{n}X_i\\
	&=\frac{1}{n^{\cancel{2}}} \cancel{n} Var(X) = \frac{Var(X)}{n} \tab since\ x_{i}s\ are\ identically\ distributed\\
	&=\frac{\delta_X^2}{n}\tab[12cm](**)
	\end{split}
	\end{equation*}
	In our case $X\sim N(\mu, 1)$ so $Var(X) = \delta_X^2 = 1$. Thus $Var(\bar{X}_n) = \frac{1}{n}$
	
\begin{remark*}
$X\coprod Y \implies Cov(X,Y)=0$. Note that:
$$X \coprod Y \implies E\big[g_1(X)g_2(Y)\big] = E[g_1(X)].E[g_2(Y)]$$
in particular:
$$X \coprod Y \implies E\big[XY\big] = E[X].E[Y]$$
on the other hand:
$$Cov(X,Y) = E[XY] - E(X)E(Y)$$
thus: 
$$X\coprod Y\implies Cov(X,Y)=0.$$
\end{remark*}
recall that $X\coprod Y$ means X and Y are independent, i.e. \mbox{$f_{_{X,Y}}(x,y)=f_{_X}(x)f_{_Y}(y)$} where $f_{_{X,Y}},f_{_X} and f_{_Y}$ represent respectively the %%missed words%%%/////TODO\\




We therefore have:
\begin{equation}\
P(\vert\bar{X}_n - \mu\vert \geq \delta) \leq \frac{1}{n\delta^2}
\end{equation}
Using $(4)$ and  the sample size, $n$, we can find an upper bound for the proportion of deviations which are greater than a given threshold $\delta$.\hfill\newline
We can also use $(4)$ for \underline{Sample Size Deterministic}:\\
Suppose $\delta$ is given and we want $P(\vert\bar{X}_n - \mu\vert \geq \delta) \leq \beta$ where $\beta$ is also given. Then setting $\frac{1}{n\delta^2} = \beta$, we can estimate $n\approx\frac{1}{\beta\delta^2}$.\hfill\newline
%Application to voting
\subsection{Application to Voting}
Define $X_i = \begin{cases}
		1 & \text{NDP}\\
		0 & \text{otherwise}
		\end{cases}$ . Associated to each eligible voter in Canada we have a binary variable X. Let $p=P(X=1)$. So $p$ represents the proportion of eligible voters who favor $NDP$. Of interest is often estimation of $p$. Suppose we have a sample of size $n$, $X_1,X_2,\ldots,X_n$.\\
		$\bar{X}_n = \frac{1}{n}\sum_{i=1}^{n}X_i$ is the sample proportion; The counterpart of $p$ which nat be denoted by $\hat{p}$. Note that:
		$$\mu_{_X} = E(X) = 1 \times P(X=1) + 0\times P(X=0) = 1-p + 0\times (1-p)=p$$
		and:
		$$E(X^2) = 1^2 \times P(X=1) + 0^2 \times P(X=0) = 1-p + 0\times (1-p) = p$$
From (*) and (**) we find that :
	$$E(\hat{p}_n) = E(\bar{X}_n) \mu_{_X} = p$$
and:
	$$Var(\hat{p}_n) = E(\bar{X}_n) = \frac{Var(X)}{n} = \frac{\sigma_X^2}{n} = \frac{p(1-p)}{n}$$
Thus using (\ref{tchev_sigma}), we have:
$$P(\vert\hat{p}_n - p\vert \geq \delta) \leq \frac{Var(\hat{p}_n)}{\delta^2} = \frac{p(1-p)}{n\delta^2}$$
Note that the above bound on the probability of derivation depends on $p$ which is \emph{unknown}. We however notice that $p(1-p) \leq \frac{1}{4}$ .\\
Define $\mathscr{C}(x) = x(1-x)\ for\ 0 < x < 1$. Then:
	\begin{equation*}
	\begin{split}
	&\mathscr{C}^{'}(x) = 1-2x \implies \mathscr{C}^{'}(x)= 0 \implies x = \frac{1}{2} \\
	&\mathscr{C}^{"}(\frac{1}{2}) = -2 \implies x = \frac{1}{2}\tab \text{which is a \textbf{maximizer}}\\
	&\mathscr{C}(\frac{1}{2}) = \frac{1}{2}(1-\frac{1}{2})=\frac{1}{4}
	\end{split}
	\end{equation*}
	(Note that $\mathscr{C}^{"}(x) = -2$ for all $0 < x < 1$)
	
	
	%The graph is missing
	
	
We therefore find:
	\begin{equation}\label{voter}
	P(\vert\hat{p}_n - p\vert \geq \delta) \leq \frac{1}{4n\delta^2}
	\end{equation}
Using (\ref{voter}) and a given sample size $n$ we can find an upper bound for the probability of derivation by $\delta$ and the amount for any given $\delta$.\newline
We can also use (\ref{voter}) for \underline{sample size deterministic} for a size bound $
\beta$ and derivative $\delta$ as follows:
$$\frac{1}{4n\delta^2}= \beta\tab \implies \tab n \geq \frac{1}{4\beta\delta^2}$$
This is of course conservative since $p(1-p)\leq \frac{1}{4}$.



\newpage
\section{Lecture 3}
\subsection{MSE} 
\textbf{MSE}: To study estimation error we started by studying $P(\vert \hat{\Theta}_n - \Theta\vert > \delta)$ , deviation above a given threshold $\delta$, by bounding this probability. One may take a different approach by studying average Euclidean distance, i.e. $E[\ \vert\hat{\Theta}_n - \Theta\vert ^2]$, which denoted by \textbf{MSE($\hat{\Theta}_n$)}.\newline
We note that if $\Theta = E(\hat{\Theta}_n)$, i.e. $\hat{\Theta}_n$ is an unbiased estimation of $\Theta$, then: 
$$MSE(\hat{\Theta}_n) = E[\vert\hat{\Theta}_n - \Theta\vert^2] = E[(\hat{\Theta}_n - \mu _{n_{\Theta _n}})^2] = Var(\hat{\Theta}_n)$$
Now recall that $Var(X) = 0\  \implies \ P(X = \text{constant}) = 1$ which essentially means random variable X is a constant.\\
The same comment applies to MSE($\hat{\Theta}_n$). We want to find the closest estimator $\hat{\Theta}_n$ to $\Theta$ which means that we want to minimize $E[(\hat{\Theta}_n - \Theta)^2]$ over all possible estimators, ideally at least the above comment tells us that in real applications we cannot expect to find an estimator whose MSE is equal to zero. Let's try to understand the MSE a bit more:
\begin{equation*}
\begin{split}
\text{MSE}(\hat{\Theta}_n) &= E[(\hat{\Theta}_n - \Theta)^2]\\
	&= E\Big[\Big(\big(\hat{\Theta}_n - E(\hat{\Theta}_n)\big) + \big(E(\hat{\Theta}_n) - \Theta\big) \Big)^2 \Big]\\
	&= E\big[\big(\hat{\Theta}_n - E(\hat{\Theta}_n)\big)^2\big] + \big(E(\hat{\Theta}_n) - \Theta\big)^2 + 2 \cdot E\big[\big(\hat{\Theta}_n - E(\hat{\Theta}_n)\big)\big]\cdot \big(E(\hat{\Theta}_n) - \Theta\big)\\
	&= E\big[(\hat{\Theta}_n - E(\hat{\Theta}_n))^2\big] + E\big[\overbrace{(E(\hat{\Theta}_n) - \Theta)^2}^{\text{not a r.v.}}\big]
		+2\cdot E\Big[\overbrace{\big(E(\hat{\Theta}_n) - \Theta)}^{\text{not a r.v.}}\big)\cdot \big(\hat{\Theta}_n - E(\hat{\Theta}_n)\big) \Big]\\
		&= Var(\hat{\Theta}_n) + \big[\overbrace{E(\hat{\Theta}_n) - \Theta}^{\text{Bias$(\hat{\Theta}_n)$}}\big]^2 + 2\cdot Bias(\hat{\Theta}_n)\cdot \overbrace{E[(\hat{\Theta}_n - E(\hat{\Theta}_n))]}^{E(\hat{\Theta}_n) - E(\hat{\Theta}_n) = 0}\\
		&= Var(\hat{\Theta}_n) + Bias^2(\hat{\Theta}_n)
\end{split}
\end{equation*}
Roughly speaking, \textbf{bias} measures how far off the target we hit on the average while \textbf{variance} measures how much fluctuation our estimator may show from one sample to another.

\subsection{Unbiased Estimators}
In almost all real applications, the class of possible estimators for an \textbf{ESTIMANAL} is huge and the best estimator, i.e. the one that minimizes MSE no matter what the value of the \textbf{ESTIMANAL} is, almost never exists. Thus we try to reduce the class of potential estimators by improving a plausible restriction, for example Bias$(\hat{\Theta}_n) = 0$.

\begin{definition*}An estimator  $\hat{\Theta}_n$ of an \textbf{ESTIMANAL} $\Theta$ is said to be \textbf{unbiased} if $E(\hat{\Theta}_n) = \Theta$ , for all possible values of $\Theta$.
\end{definition*}
\begin{example}
$X_i\distas{iid}  N(\mu, \sigma^2)\tab i=1,2,\ldots,n$\\
Suppose both $\mu$ and $\sigma^2$ are unknown. Consider $\bar{X}_n = \frac{1}{n}\sum_{i=1}^{n} X_i$.
$$E(\bar{X_n}) = E(\frac{1}{n}\sum_{i=1}^{n} X_i) = \frac{1}{n}\sum_{i=1}^n \overbrace{E(X_i)}^{\mu} = \frac{1}{\cancel{n}}\cdot \cancel{n}\mu = \mu$$
\end{example}
Thus $\bar{X_n}$ is an unbiased estimator of $\mu$. As for the MSE$(\bar{X_n})$, we need to find Var$(\bar{X_n})$.
\begin{equation*}
\begin{split}
Var(\bar{X_n})&= Var\big(\frac{1}{n}\sum_{i=1}^{n} X_i\big) = \frac{1}{n^2} Var(\sum_{i=1}^n X_i)\\
	&= \frac{1}{n^2}\Big[\sum_{i=1}^n Var(X_i) + 2\cdot\sum\sum_{1\leq i < j \leq n} \overbrace{Cov(X_i,X_j)}^{0} \Big]\tab[2cm] \text{Theorem 5.12(b) - page 271}\\
	&= \frac{1}{n^2}\sum_{i=1}^n Var(X_i)\tab[8cm] \coprod_{i=1}^n X_i\\
	& = \frac{1}{n^2}\sum_{i=1}^{n} \sigma^2 = \frac{1}{n^{\cancel{2}}}\cdot\cancel{n}\sigma^2 = \frac{\sigma^2}{n}\tab[5cm]\ \ \text{identically distributed}\\
&\implies MSE(\bar{X_n}) = Var(\bar{X_n}) + \overbrace{Bias^2(\bar{X_n})}^{0}=Var(\bar{X_n}) = \frac{\sigma^2}{n}
\end{split}
\end{equation*}
An inspection of the above calculation shows that for unbiased $\mu$ we only require a common mean $\mu$ while for calculating the variance we would only require a common variance $\sigma^2$ and orthogonality, i.e:
$$Cov(X_i,X_j) = 0\tab \text{where}\ i\neq j$$
Suppose $X_1,\ldots,X_n$ have the same mean value $\mu$. Then:
$$E(\bar{X_n}) = E(\frac{1}{n}\sum_{i=1}^n X_i) = \frac{1}{n}\sum_{i=1}^n E(X_i) = \frac{1}{\cancel{n}}\cancel{n}\mu=\mu$$
Suppose further that $ X_1,\ldots ,X_n$ have the same variance $\sigma^2$ and \mbox{$Cov(X_i,X_j) = 0,\ i\neq j$}. Then:
\begin{equation*}
\begin{split}
Var(\bar{X_n}) &= Var(\frac{1}{n}\sum_{i=1}^n X_i) = \frac{1}{n^2} Var(\sum_{i=1}^n X_i)\\
	&= \frac{1}{n^2}\Big[\ \sum_{i=1}^nVar(X_i) + 2\sum\sum_{1\leq i < j \leq n} Cov(X_i, X_j)\ \Big]\tab[3cm] \text{Theorem 5.12(b) - Page 271}\\
	&= \frac{1}{n^2}\sum_{i=1}^n Var(X_i)\tab[5cm] \text{Orthogonality:\ i.e. $Cov(X_i,X_j) = 0\ \ if\ i\neq j$}\\
	&=\frac{1}{n^2}\sum_{i=1}^n \sigma^2 = \frac{1}{n^{\cancel{2}}}\cancel{n}\sigma^2 = \frac{\sigma^2}{n} \tab[4cm] \text{having the same variance}\\
	&\implies MSE(\bar{X_n}) = Var(\bar{X_n}) = \frac{\sigma^2}{n}
\end{split}
\end{equation*}
If $X_1,\ldots,X_n$ have the same mean value and variance and they are orthogonal.
\subsection{Stein's Paradox}
We will learn later that if $X_i\distas{iid} N(\mu,\sigma^2)$ then \mbox{$\bar{X_n} = \frac{1}{n}\sum_{i=1}^n X_i$} has many optimal properties. A paradox due to Charles Stein, however, shows that such a nice optimal properties are not preserved in higher dimensions. In fact if:
$$X_i\distas{iid}N(\mu_{_X},1),\ \ Y_i\distas{iid}N(\mu_{_Y},1)\ \text{and  }Z_i\distas{iid}(\mu_{_Z},1)$$
then, we can find the biased estimators of 
$\begin{pmatrix} \mu_{_X}\\\mu_{_Y}\\ \mu_{_Z} \end{pmatrix}$
which are closer to 
$\begin{pmatrix} \mu_{_X}\\\mu_{_Y}\\ \mu_{_Z} \end{pmatrix}$ 
than 
$\begin{pmatrix}\bar{X_n}\\\bar{Y_n}\\\bar{Z_n} \end{pmatrix}$ 
for any 
$\begin{pmatrix} \mu_{_X}\\\mu_{_Y}\\ \mu_{_Z} \end{pmatrix}$.
We may then say that 
$\begin{pmatrix}\bar{X_n}\\\bar{Y_n}\\\bar{Z_n} \end{pmatrix}$
 is an \mbox{\textbf{\underline{inadmissible estimator}} of 
 $\begin{pmatrix} \mu_{_X}\\\mu_{_Y}\\ \mu_{_Z} \end{pmatrix}$.}
 \subsection{Admissibility}
 An estimator $\hat{\Theta}$ is called admissible if there is no estimator $\tilde{\Theta}$ such that:
 $$MSE(\tilde{\Theta}) \leq MSE(\hat{\Theta})\tab[3cm]\text{for all possible values of $\Theta$}$$
and this inequality is strict for some values of $\Theta$.\newline
What this example tells us is that by allowing a bit of bias we may be able to reduce variance considerably and hence find an estimator which is closer to the target than the most natural unbiased estimator. Note that this phenomena happens only when the dimension is at least 3.





\newpage
\section{Lecture 4}
We now want to restrict the class of estimators even further. Suppose $X_1,\ldots,X_n$ have the same mean $\mu$ and variance $\sigma^2$ and they are orthogonal; i.e. \mbox{$Cov(X_i,X_j) = 0\ ,\ i\neq j$}. Consider $\tilde{X}_{n,\underset{\sim}{C}} = \sum_{i=1}^n C_i\ X_i$ and
$$\mathscr{C} = \Big\{\tilde{X}_{n,\underset{\sim}{C}}\ \colon \underset{\sim}{C} = 
	(C_1,\ldots,C_n)\in \mathbf{R}^n ,\ \sum_{i=1}^n C_i = 1 \Big\} $$
Note that
\begin{equation*}
\begin{split}
E(\tilde{X}_{n,\underset{\sim}{C}}) &= E(\sum_{i=1}^n C_i\ X_i) = \sum_{i=1}^n C_i\ E(X_i)\\
	&= \sum_{i=1}^n C_i\ \mu = \mu\overbrace{\sum_{i=1}^n C_i}^1 = 1\cdot\mu\\
	&=\mu
\end{split}
\end{equation*}
Thus $\tilde{X}_{n,\underset{\sim}{C}}$ is an unbiased estimator of $\mu$ fir any $\underset{\sim}{C}\in\mathbf{R}^n$ as long as $\sum_{i=1}^n C_i = 1$.
Then $\mathscr{C}$ is the class of all unbiased linear estimators of $\mu$. We want to find the best estimator with $\mathscr{C}$; i.e.:
$$\underset{\underset{\sim}{C}\in\mathbf{R}^n}{\text{Min}}\ MSE(\tilde{X}_{n,\underset{\sim}{C}})\tab[0.5cm] s.t\ \sum_{i=1}^n C_i = 1\tab[2cm] (*)$$
First we note that $MSE(\tilde{X}_{n,\underset{\sim}{C}}) = Var(\tilde{X}_{n,\underset{\sim}{C}})$ since $\tilde{X}_{n,\underset{\sim}{C}}$ is an unbiased estimator of $\mu$ when $\sum_{i=1}^n C_i = 1$. On the other hand:
\begin{equation*}
\begin{split}
Var(\tilde{X}_{n,\underset{\sim}{C}}) &= Var(\sum_{i=1}^n C_i\ X_i)\\
	&= \sum_{i=1}^n {C_i}^2\ Var(X_i) + 2\sum\sum_{1\leq i<j\leq n} Cov(C_iX_i,C_j,X_j)\tab \text{Theorem 5.12 page 271}\\
	&= \sum_{i=1}^n {C_i}^2 \sigma^2 + 2 \cancel{\sum\sum_{1\leq i<j\leq n} C_iC_j\overbrace{Cov(X_i,X_j)}^0}\\
	&= \sigma^2\sum_{i=1}^n {C_i}^2 \\
\text{Thus (*) is equivalent to}:\\
	&\underset{\underset{\sim}{C}\in\mathbf{R}^n}{\text{Min}}\ \sigma^2\sum_{i=1}^n {C_i}^2\tab[2cm] (**)
\end{split}
\end{equation*}
Using the \emph{Lagrange Theorem}, (**) is equivalent to:
$$
\underset{\sim}{C} = \overset{Min}{(C_1,...,C_n)} \in\mathbb{R}^n\ \ \big\{\overbrace{\sigma^2\sum_{i=1}^n C_i + \lambda(\sum_{i=1}^n C_i -1)}^{\mathscr{C_{_\lambda} (\underset{\sim}{\text{C}})}}\big\}\ .
$$
Note that: 
$
\tab\frac{\partial\ \mathscr{C}_{\lambda}(\underset{\sim}{C})}{\partial C_i}
	= 2\ \sigma^2\ C_i + \lambda\tab[0.5cm] ,\ i=1,2,3,\ldots\\
$\\
$\frac{\partial}{\partial \lambda} \mathscr{C}_{\lambda}(\underset{\sim}{C})
	= \sum_{i=1}^n C_i - 1$\newline\\
$\begin{cases}
		\frac{\partial}{\partial C_i}\mathscr{C}_{\lambda}(\underset{\sim}{C})
			= 2\ \sigma^2\ C_i + \lambda = 0\ \ ,\ \ i=1,2,3,\ldots\\
		\frac{\partial}{\partial \lambda} \mathscr{C}_{\lambda} = 0\ \ \implies \ \ \sum_{i=1}^n C_i =1
		\end{cases}$\hfill\newline\\
Thus $C_i = -\frac{\lambda}{2\ \sigma^2}\ \ , \ \ i=1,2,3,\ldots,n\ $  and using the last equation:
$$
\sum_{i=1}^n -\frac{\lambda}{2\ \sigma^2} = 1 \implies \lambda = -\frac{2\ \sigma^2}{n}
$$
and therefore:
$$
C_i = -\frac{\lambda}{2\ \sigma^2} = -\frac{-\frac{2\ \sigma^2}{n}}{2\ \sigma^2} = \frac{1}{n}\tab ,\ \ i=1,2,3,\ldots,n
$$
We can further find:
$$\ \mathcal{H} = [\frac{\partial^2}{\partial C_i \partial C_j} \mathscr{C}_{\lambda}(\underset{\sim}{C})]\tab,\ \ i,j=1,2,\ldots,n$$
and show that:
\begin{equation*}
\begin{split}
\underset{\sim}{x}^T\ \mathcal{H}\underset{\sim}{x}\ &\geq\ 0\tab \forall \underset{\sim}{x}\in\mathbb{R}^n\\
&=0\tab \text{if and only if \ } \underset{\sim}{x} = 0
\end{split}
\end{equation*}
This then guarantees that ${\underset{\sim}{C}}^{*} = (\frac{1}{n},\frac{1}{n},\ldots,\frac{1}{n})$ is indeed a minimizer; in fact, the \mbox{\emph{unique minimizer}}. To summarize:
$$
\tilde{X}_{n,{\underset{\sim}{C}}^{*}} = \sum{i=1}^n \frac{1}{n} X_i = \frac{1}{n}\sum_{i=1}^n X_i = \bar{X_n}
$$
Thus $\bar{X_n}$ is the best unbiased linear estimator.
\subsection{Estimating Variance}
So far we confirmed ourselves to estimation of th population mean.\newline
Now suppose we are interested in estimating variance from $X_1,\ldots,X_n$ where $X_is$ have the same mean value $\mu$\ , the same variance $\sigma^2$ and they are orthogonal, i.e. \mbox{$Cov(X_i,X_j)=0\ ,\ i\neq j$}, then a \mbox{\emph{natural estimator}} of:
$$
\sigma^2 = Var(X) = \mathbb{E}[(x-\mu)^2]
$$
is \underline{its sample counterpart}, i.e.
$$
S_{n,*}^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X_n})^2
$$
Now the first question is if $S_{n,*}^2$ is an unbiased estimator of $\sigma^2$ , i.e. $\mathbb{E}(S_{n,*}^2) = \sigma^2$


\begin{equation*}
\begin{split}
(X_i - \mu)^2 &= \big[(X_i - \bar{X_n})+(\bar{X_n}-\mu)\big]^2\\
	&= (X_i - \bar{X_n})^2 + (\bar{X_n}-\mu)^2 + 2\cdot (X_i - \bar{X_n}) (\bar{X_n}-\mu)\\
\sum_{i=1}^n(X_i -\mu)^2 &= \sum_{i=1}^n (X_i - \bar{X_n})^2 + n(\bar{X_n}-\mu)^2 + 2\cdot (\bar{X_n} - \mu)\overbrace{\sum_{i=1}^{n}(X_i-\bar{X_n})}^{0}\\
	&= \sum_{i=1}^n (X_i - \bar{X_n})^2 + n(\bar{X_n}-\mu)^2\tab[5cm] (I)
\end{split}
\end{equation*}

Taking estimation we find:
\begin{equation*}
\begin{split}
\mathbb{E}\big[\sum_{i=1}^n(X_i - \mu)^2\big] &= \mathbb{E}[n\cdot S_{n,*}^2]+\mathbb{E}[n(\bar{X_n}-\mu)^2]\tab[5cm] (II)\\
	RHS &= \sum_{i=1}^n\overbrace{\mathbb{E}(X_i - \mu)^2}^{\sigma^2} = n\cdot\sigma^2
\end{split}
\end{equation*}
Note that $\mathbb{E}(\bar{X_n}-\mu)=0$ ,\ i.e. $\mathbb{E}(\bar{X_n})=\mu$. Thus:
$$
	\mathbb{E}[n(\bar{X_n}-\mu)^2] = n\ \mathbb{E}[(\bar{X_n}-\mu)^2] = n\ Var(\bar{X_n})\ .
$$
On the other hand $Var(\bar{X_n})=\frac{\sigma^2}{n}$. We therefore have:
$$
	\mathbb{E}[n(\bar{X_n}-\mu)^2] = n\cdot Var(\bar{X_n}) = n\cdot \frac{\sigma^2}{n} = \sigma^2
$$
and hence from $(II)$:
$$
	n\sigma^2 = \mathbb{E}(n\ S_{n,*}^2) + \sigma^2
$$
which implies:
$$
	\implies \mathbb{E}(S_{n,*}^2 = (\frac{n-1}{n})\sigma^2 = (1-\frac{1}{n})\sigma^2
$$
meaning that $S_{n,*}^2$ is \textbf{NOT} an unbiased estimator of $\sigma^2$.\\
Multiplying both sides of the last equation by the reciprocal of $(1-\frac{1}{n})$ we find \mbox{$\mathbb{E}(\frac{n}{n-1}S_{n,*}^2) = \sigma^2$} . Note however that:
$$
\frac{n}{n-1}S_{n,*}^2 = \frac{\cancel{n}}{n-1}\cdot\frac{1}{\cancel{n}}\sum_{i=1}^n (X_i - \bar{X_n})^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X_n})^2
$$
Thus $\boxed{S_n^2 = \frac{1}{n-1}\sum_{i=1}^n(X_i - \bar{X_n})^2}$ is an unbiased estimator.\\ \\
\textbf{Question: Why $(n-1)$?}\\
$"n-1"$ is the dimension of $span\overbrace{\{X_i - \bar{X_n} \colon i=1,2,\ldots,n\}}^{\mathbb{V}}$\ .\newline $n-1 = dim(span\ V)$.\ \ Note however $\tab[0.3cm] dim(span\ W)=n$ where \mbox{$W={X_i-\mu\ ,\ i=1,2,\ldots,n}$}.\newline
We discuss these issues further in Chapter $11$ where we learn about the \mbox{regression}.\par
So far we only considered sampling from one population. We may have samples from two or more populations and may want to make inference about differences between the populations. 
\begin{example}
\end{example}
Suppose we want to study the differences between the average salaries of men and women:

\begin{tabular}{l r c d{1} }
Men&Women\\
$X_1$&$Y_1$\\
$\vdots$&$\vdots$\\
$X_m$&$Y_n$\\
\end{tabular}

where $X_is$ have the common mean $\mu_{_X}$ and $Y_js$ have the command mean $\mu_{_\mu}$. We want to estimate $\mu_{_X} - \mu_{_Y}$. The natural estimator is $\bar{X_m} = \bar{Y_n}$. Show that:
$$\mathbb{E}[\bar{X_m}-\bar{Y_n}] = \mu_{_X} - \mu_{_Y}$$
Hence $\bar{X_m}-\bar{Y_n}$ is an unbiased estimator of $\mu_{_X} - \mu_{_Y}$.\newline
Assume further that $X$s and $Y$s are independent and $X$s have common variance $\sigma_{_X}^2$ and $Y$s have common variance $\sigma_{_Y}^2$ and \mbox{$Cov(X_i,X_j)=0\ ,\ \ \ i\neq j$} and \mbox{$Cov(Y_i,Y_j)=0\ ,\ \ \ i\neq j$}.\newline
Find $Var(\bar{X_m} - \bar{Y_n})$. Hint: use $Thm\ 5.12$.\\
The difference between two proportions can be treated similarly. Note that proportions are essentially means of binary variables.

\newpage
\section{Lecture 5 : Confidence Intervals}
\begin{definition*}{Random Interval}
An interval whose endpoint(s) are random variables is called a \mbox{\textbf{Random Interval}}.
\end{definition*}
\subsection{Confidence Intervals}
A $100(1-\alpha)\%\ $ confidence interval for a parameter $\theta$ is a \emph{random interval} \mbox{$\big(\hat{\Theta}_L(X),\hat{\Theta}_V(X)\big)$} such that:
$$
	P(\hat{\Theta}_L(X) < \Theta < \hat{\Theta}_V(X))
$$
\textbf{Pivotal Quantity} : A function of the observation $X_1,\ldots,X_n$ and some unknown parameters, ideally just the parameter(s) of interest, whose distribution DOES NOT depend on any unknown parameter is called \textbf{Pivotal Quantity}.\\
Pivotal Quantities play a central role in theory confidence intervals.
\begin{example*}\emph{
Let $X_i \distas{iid} N(\mu, \sigma^2)\ ,\ i=1,2,\ldots,n\ $ where $\sigma^2$ is \underline{known}, but $\mu\ $ is \underline{unknown}. We show that 
$$
\bar{X}_n \sim N(\mu,\frac{\sigma^2}{n})\tab where\tab \bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i
$$}
\end{example*}
Recall that there are three  methods for finding the distribution of a function of random variables:
\begin{enumerate}
\item\textbf{Method of Transformation:} This si essentially theorem of change of variables in calculus.

\item\textbf{Method of Distribution:} On this method we connect the \emph{cdf} of the new variable to the \emph{cdf} of the original variables.
	\begin{example*}
	Suppose $X_i\distas{iid} f\ ,\ i=1,2,\ldots,n\ $ are continuous random variables with \emph{pdf} $\ f$ and \emph{cdf} F. Define $X_{(n)} = \underset{1 \leq i \leq n}{max}$ .
	\end{example*}
	\begin{equation*}
	\begin{split}
	F_{x(n)}(t)= P(X_{(n)} \leq t) &= P(X_1 \leq t, X_2\leq t,\ldots,X_n\leq t)\\
	&=\prod_{i=1}^nP(X_i\leq t)	\tab[2cm] (\text{by }\coprod_{i=1}^n X_i)\\
	&=\prod_{i=1}^n F_{X_i}(t)\\
	&=\prod_{i=1}^n F(t) = F^n(t)\tab[2cm] \text{identically distributed} \\
	\text{Thus}\\
	f_{X(n)}(t) &= \frac{d}{dt} F_{X(n)}(t) = \frac{d}{dt}F^n(t)\\
		&=n\ f(t)\ F^{n-1}(t)
	\end{split}
	\end{equation*}

\item\textbf{Method of Moment Generating Function(mgf): } This method is essentially based on the \emph{mgf} of the new variable of the \emph{mgf} if the original variables.
	\begin{example*}
		Suppose $X_i\sim N(\mu_{i}, \sigma_i^2)\ ,\ i=1,2,\ldots,n$ and $X_is$ are independent. Define $S=\sigma_{i=1}^n X_i$.
	\end{example*}
	\begin{equation*}
	\begin{split}	
		m_s(t) &=\mathbb{E}[e^{tS}] = \mathbb{E}[e^{t \sum_{i=1}^n X_i}]\\
		&= \mathbb{E}\Big[\prod_{i=1}^n e^{t X_i}\Big]\tab[3cm]\text{using independence: }(\coprod)\\
		&=\prod_{i=1}^n m_{X_i}(t)\\
		&=\prod_{i=1}^n e^{\mu_i t + \frac{\sigma_i^2 t^2}{2}}			\\
		&=exp\Big\{t\sum_{i=1}^n \mu_i + \frac{t^2}{2}\sum_{i=1}^n \sigma_i^2 \Big\}\\
\implies &S\sim N(\sum_{i=1}^n \mu_i, \sum_{i=1}^n \sigma_i^2)
	\end{split}
	\end{equation*}
	If we further assume that $X_is$ are identically distributed, then: $$\mu_i = \mu \tab\text{and}\tab\sigma_i^2 = \sigma\tab\forall i=1,2,\ldots,n$$
	Therefore we have:
	$$
	m_S(t) = exp\Big\{n\mu t + \frac{n\sigma^2 t^2}{2}  \Big\}
	$$
	and hence:
	\boxed{$$S\sim N(n\mu, n\sigma^2)$$}
	Then:
	\begin{equation*}
	\begin{split}	
		m_{\bar{X}_n}(t) &= \mathbb{E}[e^{t\bar{X}_n}] = \mathbb{E}[e^{t \frac{1}{n}\sum_{i=1}^n X_i}]\\
		\text{by }t^{*}=\frac{t}{n}\implies\tab&=E[e^{t^{*}S}]\\
		&= m_S(t^{*}) = e^{n\mu t^{*} + \frac{n\sigma^2 {t^{*}}^2}{2}}\\
		&=exp\Big\{n\mu t^{*} + \frac{n\sigma^2 {t^{*}}^2}{2}\Big\}\\
		\implies \text{\boxed{$$\bar{X}_n \sim N(\mu,\frac{\sigma^2}{n})$$}}\tab[3cm](1)
	\end{split}
	\end{equation*}
\end{enumerate}
Note further that if $X\sim N(\mu,\sigma^2)$ , then $Z=\frac{X-\mu}{\sigma} \sim N(0,1)$ . We prove a general form of this. Let $X\sim N(\mu,\sigma^2)$ ; then:
$$
	aX+b \sim N(a\mu +b\ ,\ a^2\sigma^2)\tab \text{for any constant $a,b$}
$$
	Let $V=ax+b$ , then:
	\begin{equation*}
	\begin{split}
	m_v(t) &= \mathbb{E}[e^{tV}] = \mathbb{E}[e^{t(ax+b)}]\\
		&= \mathbb{E}[e^{taX+tb}] = \mathbb{E}[\underbrace{e^{tb}}_{constant}\cdot e^{\overbrace{taX}^{t^{*}}}]\\
		&=e^{tb}\cdot \mathbb{E}[e^{t^{*}X}]\\
		&=e^{tb}\cdot e^{\mu t a+\frac{\sigma^2 t^2 a^2}{2}}\\
		&=exp\ \Big\{t(a\mu+b) + \frac{t^2(a^2\sigma^2)}{2}	\Big\}\\
	Thus:\\
	(ax&+b)\sim N(a\mu +b\ ,\ a^2\sigma^2)\\
	Now:\\
	Z&=\frac{X-\mu}{\sigma} = \frac{X}{\sigma} - \frac{\mu}{\sigma}\\
	&=\frac{1}{\sigma}X - \frac{\mu}{\sigma}\\
	&=aX+b\tab[3cm]\text{where $\ a=\frac{1}{\sigma}\ $ and $\ b=-\frac{\mu}{\sigma}$}\\
Hence:\\
	Z\sim&\ N(\overbrace{\frac{1}{\sigma}\mu +(-\frac{\mu}{\sigma})}^0 \ ,\ \overbrace{(\frac{1}{\sigma})^2\sigma^2}^1)\\
Thus:\\ 
	&\text{\boxed{$$Z\sim N(0,1)$$}}\tab[3cm](2)\\
\text{Using (1) and (2) :}\\
	\frac{\bar{X}_n - \mu}{\sqrt{\frac{\sigma^2}{n}}} &= \frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}} \sim N(0,1).\\
	\text{This means that :}\\
	&\text{\boxed{$$\frac{\bar{X}_n - \mu}{\sqrt{h}}$$\text{ is a Pivotal Quantity}}}
	\end{split}
	\end{equation*}
To summarize:
$$
	X_i\distas{iid} N(\mu, \sigma^2)\tab\implies\tab \frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}}\tab\text{is a \textbf{pivotal quantitity}}
$$
Notice that using the table for the normal distribution:
$$
	P\Big(\big\vert \frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}}\big\vert \leq 1.96	\Big) = 0.95
$$
Equivalently:
$$
	P\Big(\bar{X}_n - 1.96\frac{\sigma}{\sqrt{n}}\ \ \leq\ \mu\ \leq \bar{X}_n + 1.96\frac{\sigma}{\sqrt{n}} \Big) = 0.95
$$
This means that:
$$
	(\bar{X}_n - 1.96\frac{\sigma}{\sqrt{n}}\ ,\ \bar{X}_n + 1.96\frac{\sigma}{\sqrt{n}})
$$
covers the true $\mu$ with $\ 95 \%\ $ probability.\\
Thus a $\ 100\ (1-\alpha)\%$ confidence interval for $\mu$ where $X_i\distas{iid} N(\mu,\sigma^2)$ and $\sigma^2$ is known:\\
$$
	\bar{X}_n \pm \zeta_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}}
$$
where:
$$
	P(Z > \zeta_{\frac{\alpha}{2}}) = \frac{\alpha}{2}\tab,\tab Z\sim N(0,1)
$$
\#MISSING GRAPH - (LECTURE 5 - P5)


\begin{remark*}
\emph{
In real applications we compute $\bar{X}_n$ and obtain an interval, say \mbox{$(125, 135)$} . Now either this interval covers the true $\mu$ or it does not. Then the question is what do we mean by a $95\%$ \#MISSING ?\\
Note that the $\ 100(1-\alpha)\%\ $ confidence is the property of the procedure. It means that out of the all possible intervals of the form \mbox{$(\bar{X}_n \pm 1.96 \frac{\sigma}{\sqrt{n}})$} that we can make by taking samples of size $n$ from $N(\mu,\sigma^2)$, $\ 95\%$ of them cover the true $\mu$ . Now this is a real application when we make one of the such intervals by taking a random sample of size $n$ from $N(\mu,\sigma^2)$ , it is like taking one of those intervals randomly. Since that $\ 95\%$ of them cover $\mu$ , my chance of selecting an interval that covers $\mu$ is $\ 95\%\ $. Thus I can take a bet $19$ to $1$ that the interval I select covers $\mu$. 
}
\end{remark*}

\subsection{Large Sample Confidence Interval}
The derivation of the pivotal quantity in the above example totally hinges over the normality assumption, i.e. \mbox{$X_i \sim N(\mu,\sigma^2)$}.\\
What happens if we do not know the parametric for the population distribution?
\begin{theorem*}[\textbf{General Limit Theorem - GLT} (baby version)]
\emph{
Suppose $X_1,\ldots,X_n$ are independent random variables with common $\mu$ and variance $\sigma^2$ . Then:
$$\frac{\bar{X}_n - \mu}{(\frac{\sigma}{\sqrt{n}})}\ \ \distas{app}\ \ N(0,1)\tab\text{when \ $n$\ \ is large enough}$$
}
\end{theorem*}
This is a powerful theorem that implies that $\frac{\bar{X}_n - \mu}{(\frac{\sigma}{\sqrt{n}})}$ is approximately a pivotal quantity distributed according to $N(0,1)$ for large enough $n$ regardless of population distribution provided that the condition of the \textbf{GLT} are met.\\

\begin{forest}
    for tree={
        grow=0,reversed, % tree direction
        parent anchor=east,child anchor=west, % edge anchors
        edge={line cap=round},outer sep=+1pt, % edge/node connection
        rounded corners,minimum width=15mm,minimum height=8mm, % node shape
        l sep=10mm % level distance
    }
  [$\sigma^2$,L1
    [$\sigma^2$ is known:
    	$\implies\ (\bar{X}_n \pm \zeta_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}})$ is a $\ 100(1-\alpha)\%\ $ \#MISSING for $\mu$
    ,L2]
    [$\sigma^2$ is unknown:
    	$\implies\ (\bar{X}_n \pm \zeta_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}})$ is not \underline{not useful}
    ,L2]
  ]
\end{forest}



Note: if $\sigma^2$ is unknown, $(\bar{X}_n \pm \zeta_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}})$ is still a $\ 100(1-\alpha)\%\ $ \#MISSING for $\mu$ but not useful.\\
We need to somehow get rid of the \#MISSING parameter $\sigma$ . We can replace $\sigma$ by $S_n$ where:
$$
	S_n^2 = \frac{1}{n-1} \sum_{i=1}^{n}(X_i - \bar{X}_n)^2
$$ 

\begin{forest}
    for tree={
        grow=0,reversed, % tree direction
        parent anchor=east,child anchor=west, % edge anchors
        edge={line cap=round},outer sep=+1pt, % edge/node connection
        rounded corners,minimum width=15mm,minimum height=8mm, % node shape
        l sep=10mm % level distance
    }
  [Justification,L1
    [Intuitive, L2]
    [Formal, L2]
  ]
\end{forest}
\begin{enumerate}
\item[\textbf{Intuitive}] : 
	$S_n^2$ is the sample counterpart, almost, for $\sigma^2$ . Thus as $n$ increases, greater portion of the population and hence our sample sets closer to the population.
\item[\textbf{Formal}] : 
	The formal proof comprises three steps:
	\begin{enumerate}[1.]
	\item \textbf{GLT} of $$\frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}}$$
	\item Consistency of $S_n^2$ for $\sigma^2$, i.e. $S_n^2 \overset{P}{\rightarrow} \sigma^2$ which we learn in \emph{Chapter 9}. We then use a theorem called \textbf{Continuous Mapping Theorem} which says that if \mbox{$S_n^2 \overset{P}{\rightarrow} \sigma^2$}, then:
	$$
	g(S_n^2) \overset{P}{\rightarrow} g(\sigma^2)\tab[2cm]\text{for any continuous function}
	$$
	Considering $g(x) = \sqrt{X}$ , we obtain $S_n\overset{P}{\rightarrow}\sigma$ and hence $\frac{\sigma}{S}\overset{P}{\rightarrow} 1$.
	\item \textbf{Cramer's Theorem :}
			
	 This result says that if $V_n\overset{D}{\rightarrow} X$ and $Y_n\overset{P}{\rightarrow} 1$ , then $Y_n\cdot V_n \overset{D}{\rightarrow} X$ :\\
		$$\frac{\bar{X}_n - \mu}{\frac{S}{\sqrt{n}}} = \underbrace{\frac{\sigma}{S_n}}_{Y_n} \cdot \underbrace{\frac{\bar{X_n} - \mu}{\frac{\sigma}{\sqrt{n}}}}_{V_n}$$
	\end{enumerate}
\end{enumerate}
Note that GLT implies that $V_n\overset{D}{\rightarrow}2$ , i.e:
$$
\overbrace{F_{V_n}(t)}^{\text{cdf of $V_n$}} \rightarrow F_{_Z}(t) =\overbrace{\int_{-\infty}^t\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx}^{\Phi(t)\text{ cdf of }N(0,1)}
$$
Using step (2), $Y_n = \frac{\sigma}{S_n}\overset{P}{\rightarrow} 1$ and application of Cramer's Theorem computes the proof.
To summarize:

\begin{forest}
    for tree={
        grow=0,reversed, % tree direction
        parent anchor=east,child anchor=west, % edge anchors
        edge={line cap=round},outer sep=+1pt, % edge/node connection
        rounded corners,minimum width=15mm,minimum height=8mm, % node shape
        l sep=10mm % level distance
    }
  [$\sigma^2$,L1
    [$\sigma^2$ is known $\implies\ \ \ (\bar{X}_n \pm \zeta_{\frac{\alpha}{2}})$ is a $100(1-\alpha)\%$
    , L2]
    [$\sigma^2$ is unknown $\implies\ \ \ (\bar{X}_n \pm \zeta_{\frac{\alpha}{2}})$ is a $100(1-\alpha)\%$
    , L2]
  ]
\end{forest}
To be more precise, these confidence intervals are \underline{approximate} $\ 100(1-\alpha)\%\ $ confidence intervals for $\mu$ when $n$ is large enough.\\
So far we focused on $C.I$ for population mean. How can we make $C.I$ for other estimates?\\
A common, perhaps the most common, method of estimation that we will learn about in Chapter 9 is \underline{the method of maximum likelihood}. Suppose $\Theta$ is a parameter of interest. Suppose $\hat{\Theta}_n = \hat{\Theta}(X_1,\ldots,X_n)$ is the maximum likelihood estimate $(MLE)$ of $\Theta$ based on $X_1,\ldots,X_n$. Then relatively several condition we have:
$$
	\frac{\hat{\Theta}_n - \Theta}{\sqrt{Var(\hat{\Theta}_n)}} \distas{app} N(0,1)\ \ \text{when $n$ is large enough
}\tab(*)
$$
We therefore have a several recipe for confidence interval when the sample size $n$ is large enough, namely:
$$
	\hat{\Theta}_n \pm \zeta_{\frac{\alpha}{2}}\sqrt{Var(\hat{\Theta}_n)}\tab[2cm] (\dagger)
$$ 
that is a $\ 100(1-\alpha)\%\ C.I$ for $Q$.
\begin{example}
$X_i \distas{iid} N(\mu, \overbrace{\sigma^2}^{known})\ ,\ i=1,2\ldots,n$ .
\end{example}
We show in chapter 9 that $\bar{X}_n$ in the $MLE$ of $\mu$. Note that \mbox{$Var(\bar{X}_n) = \frac{\sigma^2}{n}$}. Then using $(\dagger)$ :
$$
	\bar{X}_n \pm \zeta_{\frac{\alpha}{2}} \sqrt{\frac{\sigma^2}{n}}\ \ \ \ \text{is a $100(1-\alpha)\%$ C.I for $\mu$}
$$

\begin{example}
$X_i \distas{iid} Bernoulli(p)\ \ \forall\ i=1,2,\ldots,n$ , i.e: 

$X_i = \left\{
  \begin{array}{lr}
    1 & \tab p\\
    0 & \tab 1-p
  \end{array}
\right.$
\end{example}
Then $\hat{p}_n = \frac{1}{n}\sum_{i=1}^n x_i$ is the $MLE$ of $p$. Thus using $(\dagger)$ :
$$
	\hat{p}_n \pm \zeta_{\frac{\alpha}{2}\sqrt{Var(\hat{p}_n)}}\ \ \text{ is a $100(1-\alpha)\%\ C.I$ for $p$ .}
$$

Note that $Var(\hat{p}_n) = \frac{p(1-p)}{n}$ . We have two choices:

\begin{enumerate}
\item replace $p$ by $\hat{p}_n$ in $Var(\hat{p}_n)$ :
	$$\hat{p}_n \zeta_{\frac{\alpha}{2}} \frac{\sqrt{\hat{p}_n(1-\hat{p}_n)}}{\sqrt{n}}$$
\item replace $p(1-p)$ in $Var(\hat{p}_n)$ by $\frac{1}{4}$ to find a conservatively large $C.I$ for $p$ :
	$$	\hat{p}_n \pm \zeta_{\frac{\alpha}{2}} \frac{1}{2\sqrt{n}}	$$
\end{enumerate}

\begin{example}
Suppose $X_i\distas{iid} Ber(p)\ ,\ i=1,2\ldots,n$ and we are interested in $\Theta = p(1-p)$ , the variance.
\end{example}
An interesting property of $MLE$ is the invariance , i.e. if $\hat{\Theta}_n$ if the $MLE$ of $\Theta$ , then $h(\hat{\Theta}_n)$ is the $MLE$ of $h(\Theta)$. The invariance property then implies that: \mbox{$\hat{\Theta}_n = \hat{p}_n(1-\hat{p}_n)$} is the $MLE$ of $p(1-p)=\Theta$ .\\
The $ 100(1-\alpha)\%\ $ $C.I\ $ for $\Theta=p(1-p)$\ \ \ \ is \ \ 
\mbox{$\ \hat{\Theta}_n \pm \zeta_{\frac{\alpha}{2}} \sqrt{Var(\hat{\Theta}_n)}$}
\subsection{Small Sample Confidence Intervals}
Unlike the large sample case, there is no general recipe like $({*})$ using which we can find an approximate pivotal quantity. In fact, there is on the paper, but only gives \#MISSING in special cases.\\
To summarize , small sample probabilities are solved mostly case by case. A case of particular importance is the \emph{normal case} . We will learn about the importance of this case when we discuss regression and ANOVA (Analysis of Variance) .\\ \\
\textbf{Normal Case:} \\
Suppose $X_i \distas N(\overbrace{\mu}^{\text{of interest}}, \underbrace{\sigma^2}_{nuisance})\ ,\ i=1,2,\ldots,n$ where n, the sample size is \emph{NOT} large.\\
We learned that when $X_i \distas{iid} N(\mu,\sigma^2)\ ,\ i=1,2,\ldots,n$ :
$$\frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}} \distas{Exact} N(0,1)\tab[2cm] (\ddagger)$$ 
This by itself is not useful since $\sigma$ is \underline{not} known. We discussed in previous section at length why we can replace $\sigma$ by $S$ when n is large enough. The formal justification is \textbf{not} applicable now since $n$ is small, the intuitive justification still stands though.\\
Replacing $\sigma$ with $(\ddagger)$ changes the picture a bit. Given that $S$ has the same spirit as $\sigma$ , though in a small \#MISSING the distribution of \mbox{$T = \frac{\bar{X}_n - \mu}{\frac{s}{\sqrt{n}}}$} still has a bell curve shape. The tails of the distribution, however, die out much more slowly than those of normal distribution. Heavier tails mean much more \underline{variability} and this should perhaps be expected since by replacing $\sigma$ by $S$ which can be crude estimate when $n$ is small, can add suite a hit to the variability. This is, of course, a intuitive argument. Following we present the sketch of a formal argument:
\begin{enumerate}
\item[step 1)] $X_i \distas{iid} N(\mu, \sigma^2) \implies \bar{X}_n\distas{} N(\mu,\frac{\sigma^2}{n})$\\
	$\implies \frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}}\distas{} 		N(0,1)$
\item[step 2)]
	$X_i \distas{iid} N(\mu,\sigma^2) \implies \frac{(n-1)S^2}{\sigma^2} \sim \mathcal{X}^2_{(n-1)}$\\
	\textbf{proof}
	\begin{equation*}
	\begin{split}
	\sum_{i=1}^n(X_i - \mu)^2 &= \sum_{i=1}^n \big[(X_i - \bar{X}_n)+(\bar{X}_n - \mu)\big]^2\\
		&= \sum_{i=1}^n (X_i - \bar{X}_n)^2 + n(\bar{X}_n - \mu)^2 + 2(\bar{X}_n - \mu)\overbrace{\sum_{i=1}^n (X_i - \bar{X}_n)}^0 \\
		& = (n-1)S^2 + n(\bar{X}_n - \mu)^2 \\
		\text{by dividing both sides by $\sigma^2$ we obtain:}\\
		\underbrace{\sum_{i=1}^n (\frac{X_i - \mu}{\sigma})^2}_W &= \underbrace{\frac{(n-1)S^2}{\sigma^2}}_{U} + \underbrace{(\frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}})^2}_{V}\\
		Now note that: \\
		X_i \distas{iid} N(\mu,\sigma^2) &\implies \frac{X_i - \mu}{\sigma} \sim N(0,1)\\
		&\implies (\frac{X_i - \mu}{\sigma})^2 \sim \mathcal{X}^2_1\\
		&\implies \sum_{i=1}^n (\frac{X_i - \mu}{\sigma})^2 \sim \mathcal{X}^2_n\\
		\text{(Exercise: Theorem 7.2 , page 356)}
	\end{split}
	\end{equation*}
	Thus $W \sim \mathcal{X}^2_n$ . On the other hand, using step 1:
	$$(\frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}})^2 \sim \mathcal{X}^2_1$$
\item[step 3)]
	\tab[0.5cm]If \tab[0.5cm]$X_i \distas{iid} N(\mu, \sigma^2)$			\tab[0.5cm]then\tab[0.5cm]$\bar{X}_n \coprod S^2$
\item[step 4)]
	\begin{equation*}
	\begin{split}
		m_{_W}(t) & = \mathbb{E}{e^{tW}}\\
			&= \mathbb{E}[e^{t(U+V)}]\\
			&= \mathbb{E}[e^{tU}\cdot e^{tV}]\\
			&= \mathbb{E}[e^{tU}]\cdot \mathbb{E}[e^{tV}]\\
			&= m_{_U}(t) + m_{_V}(t)\tab[2cm] U\coprod V\ \text{ using step 3}\\
			\text{Thus\tab  $m_{_U}(t)$} &= \frac{m_{_W}(t)}{m_{_V}(t)}\\
			&=\frac{(1-2t)^{-\frac{n}{2}}}{(1-2t)^{-\frac{1}{2}}}\\
			&=(1-2t)^{-\frac{n-1}{2}}\\
	\text{which implies that\tab$U\sim\mathcal{X}^2_{(n-1)}$}
	\end{split}
	\end{equation*}
\item[step 5)]
	If $Z\sim N(0,1)\ ,\ U\sim\mathcal{X}^2_{V}$ and $Z\coprod U$ \, then:
	$$	\frac{Z}{\sqrt{\frac{U}{V}}} \sim T_{n-1} \tab[4cm] \text{(Exercise 7.30 , page 367)}	$$
\item[step 6)]
	$$
	T_{n-1} = \frac{\bar{X}_n - \mu}{\frac{S}{\sqrt{n}}} = \frac{(\frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}})}{\sqrt{\frac{(\frac{(n-1)S^2}{\sigma^2})}{(n-1)}}}	= \frac{Z}{\sqrt{\frac{U}{V}}}
	$$
	The pdf of $T_{v}$ is :
	\begin{equation*}
	\begin{split}
		f_{_{T_v}}(t) = \frac{\Gamma{\frac{v+1}{2}}}{\Gamma(\frac{v}{2})\sqrt{v\pi}}(1+\frac{t^2}{v})^{-\frac{v+1}{2}}\tab -\infty<t<+\infty 
	\end{split}
	\end{equation*}
	\#MISSING GRAPH Lecture 5 - page 13\\
	
	$\mathbb{E}[T_{v}^r] = \left\{
  \begin{array}{lr}
    0 & \tab \text{if $r<v$ and $r$ is odd}\\
    v^{\frac{r}{2}}\cdot\frac{\Gamma(\frac{1+r}{2})\Gamma(\frac{v-r}{2})}{\Gamma(\frac{1}{2})\Gamma(\frac{v}{2})} & \tab \text{if $r<v$ and $r$ is even}
  \end{array}
\right.$\\ \\
	Thus, if $X_i\distas{iid}N(\mu,\sigma^2)\ ,\ i=1,2,\ldots,n\ $ and $\mu$ and $\sigma^2$ are both \underline{unknown} :
	$$\bar{X}_n \pm t_{(n-1),\frac{\alpha}{2}} \frac{S}{\sqrt{n}}$$ 
	provides a $ 100(1-\alpha)\%\ $ $C.I$ for $\mu$ where $P(T_{(n-1)} > t_{(n-1), \frac{\alpha}{2}}) = \frac{\alpha}{2}$ 
	
\subsection{Pivotal Quantity and Probability Integral Transform}
Suppose $X$ is a continuous random variable with $p.d.f$ $f$ and $cdf$ $F$. Then $F(X)\sim Uniform(0,1) \tab \text{(Exercise)}$\\
This result is referred to as the \textbf{Probability Integral Transform}. Now suppose $X_i \distas{iid} F$ . Then :
\begin{equation*}
\begin{split}
	F(X_i) \sim Unif(0,1) &\implies -2\ln{F(X_i) \sim \mathcal{X}^2_2}\\
	&\implies -2\sum_{i=1}^n \ln{F(X_i)}\sim \mathcal{X}^2_{2n}\\
	&\implies -2\sum_{i=1}^n \ln{[1-F(X_i)]} \sim \mathcal{X}^2_{2n}
\end{split}
\end{equation*}

There is hence a general recipe for finding a pivotal quantity when we have samples from continuous random variables. The usefulness of this pivotal quantity \emph{depends} on the form of $F$ , the $cdf$ of $X$ .\\
Suppose $X_i \distas{iid}exp(\lambda)\ ,\ i=1,2,\ldots,n\ ,\ $ i.e:

$$
	f_{_X}(x) = \left\{
  	\begin{array}{lr}
   	 	\lambda e^{-\lambda x} & \tab x>0\\
   	 	0 & \tab o/w
  	\end{array}
	\right.
$$

Then:
$$
	F(x) = \int_{0}^x f(t)dt = 1-e^{-\lambda x}\tab[0.5cm] ,\tab[0.5cm] x>0
$$
and:
$$
	F(x) = \left\{
  	\begin{array}{lr}
   	 	1-e^{-\lambda x} & \tab x>0\\
   	 	0 & \tab o/w
  	\end{array}
	\right.
$$
Using the above discussion:
$$
	-2\sum_{i=1}^n \ln{F(X_i)} \sim \mathcal{X}^2_{2n}\tab\text{and}\tab -2\sum_{i=1}^n \ln{[1-F(X_i)]} \sim \mathcal{X}^2_{2n}
$$
for this example it is easier to work with the latter, i.e. :
\begin{equation*}
\begin{split}
\-2\sum_{i=1}^n \ln{[1-F(X_i)]} &= -2\sum_{i=1}^n \ln{(e^{-\lambda X_i})}\\
	&= 2\lambda\sum_{i=1}^n X_i = 2 n \lambda \bar{X}_n \\
	so \implies &2 n \lambda \bar{X}_n \sim \mathcal{X}^2_{2n}
\end{split}
\end{equation*}
Using the $\mathcal{X}^2$ table (Application 3, page 850-851) , we can find \mbox{$\mathcal{X}^2_{(2n),0.025}$} and \mbox{$\mathcal{X}^2_{(2n),0.975}$} such that:
$$
P(\mathcal{X}^2_{(2n),0.975} < 2 n \lambda \bar{X}_n < \mathcal{X}^2_{(2n),0.025}) = 0.95
$$
Thus:
$$
	\big(\frac{\mathcal{X}^2_{(2n),0.975}}{2 n \bar{X}_n} , \frac{\mathcal{X}^2_{(2n),0.025}}{2 n \bar{X}_n}	\big)
$$
provides that a $95\%$ $C.I$ for $\lambda$ . Note that $\mathcal{X}^2_{(2n),\alpha}$ is such that \mbox{$P(\mathcal{X}^2_{(2n)} > \mathcal{X}^2_{(2n),\alpha}) = \alpha$}

\#MISSING GRAPH LECTURE 5 - PAGE 15
		
\end{enumerate} 






\newpage
\section{Lecture 6}
\subsection{Small Sample Confidence Interval(general case):}
We learned in the last lecture how to find $C.I.$ fir th population mean when the population distribution is normal. The two main pivotal quantities are:
$$
	(a)\ \ \ \ \frac{(n-1)S^2_n}{\sigma^2}\sim \chi^2_{(n-1)}\tab \&\tab (b)\ \ \ \ \frac{\bar{X}_n = \mu}{\frac{S}{\sqrt{n}}}\sim T_{(n-1)}
$$ 
when $X_i\distas{iid} N(\mu, \sigma^2)$ , $\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$ and $S^2_n = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X}_n)^2$ . \\
The first result can be used to make a $C.I$ for $\sigma^2$ and $\sigma$ which the latter is used for making a $C.I$ for $\mu$ .\\
We now consider the general case. 
\subsection{Probability Integral Transform(PIT)}Suppose $X_i \distas{iid} F_{_X}$ and $f$ is the pdf of $X_i$s:
\begin{equation*}
\begin{split}
X\sim F_X\ , Y = F_X(X) \\
F_Y(t) &= P(Y\leq t) = P(F_X(x)\leq t)\\
&=P(X\leq F_X^{-1}(t))\\
&=F_X(F_X^{-1}(t)) = t\tab \text{for } 0\leq 1\leq 1\\
\text{Thus \ \ } F_Y(t)= &\left\{
  	\begin{array}{lr}
   	 	0 & \tab \text{if } t<0 \\
   	 	t & \tab \text{if } 0\leq t <1 \\
   	 	1 & \tab \text{if } 1\leq t
  	\end{array}
	\right.
\end{split}
\end{equation*}
and hence $Y \sim Unif(0.1)$. This is called \textbf{Probability Integral Transform(PIT)}.
\begin{example}
$X\sim Exp(\lambda)$ , 
$f_X(x) = \left\{ \begin{array}{lr} \lambda e^{- \lambda x} &\tab x > 0\\ 0 &\tab x\leq 0
\end{array}
\right.$ , $\lambda > 0$.
\end{example}
\begin{equation*}
\begin{split}
F_{_X}(x) &= P(X\leq x) = \int_{-\infty}^x f_{_X}(t) dt = \int_0^x \lambda e^{-\lambda t} dt\\
	&= -e^{-\lambda t}\Big|_0^x\\
	&= 1-e^{-\lambda x}\tab[4cm] (1)\\
\text{Now consider \ \ } Y &= F_{_X}(x) = 1-e^{-\lambda x} :\\
F_{_Y}(t) &= P( Y \leq t) = P(1-e^{-\lambda x} \leq t)\\
	&=P(e^{-\lambda x} \geq 1-t) = P(X \leq -\frac{ln(1-t)}{\lambda})\\
	&=F_{_X}(-\frac{ln(1-t)}{\lambda})\\
	&=1-e^{-\lambda(-\frac{ln(1-t)}{\lambda})} \tab[3cm] \text{using } (1)\\
	&=1-e^{ln(1-t)} = 1-(1-t)\\
	&=t
\end{split}
\end{equation*}
Thus $Y \sim Unif(0,1)$.\\
\begin{remark*}\emph{
Using $PIT$ we can essentially generate random numbers from any continuous distributions. In fact, suppose we want samples from cdf F. Then:\\
\tab \textbf{Step 1:} Generate $U_i \distas{iid} \text{Unif}(0,1)\ \ ,\ \ i=1,2,\ldots,n$.\\
\tab \textbf{Step 2:} $X_i = F^{-1}(U_i)\distas{iid} F \ \ ,\ \ i=1,2,\ldots,n$
}
\end{remark*}
This algorithm then works as long as we can generate uniform random numbers and $F^{-1}$ can be explicitly found or well approximated.\\
	\begin{example*}
		$f_{_X}(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \ \ ,\ \ -\infty < x < +\infty \ \ \ (X\sim N(0,1))$ 
	\end{example*}
$$
\text{Then: \ \ } F_{_X}(x) = \int_{-\infty}^x f_{_X}(t)dt = \int_{-\infty}^x \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}dt\ \ \ .
$$
In this case, $F^{-1}$ does not have an explicit nice form, but it can be well approximated.\\
\textbf{Remark. } A simple and useful transformation:\\
$$
	X\sim F \implies \overbrace{Y = F(X) \sim Unif(0,1)}^{P.I.T} \implies -2\cdot log(Y) \sim \chi^2_2
$$
\subsection{Pivotal Quantity}
Now suppose 





















\newpage
\section{Lecture 7}
\section{Lecture 8}
\section{Lecture 9}
\section{Lecture 10}
\section{Lecture 11}
\section{Lecture 12}



\end{document}
